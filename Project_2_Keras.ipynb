{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raymec/keras/blob/main/Project_2_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmSTcGw2lmqB"
      },
      "source": [
        "# Spring 2022\n",
        "# CPSC 585 Project 2\n",
        "## Raymond Carpio\n",
        "## Yu Pan\n",
        "## Sijie Shang\n",
        "## John Tu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwUguxm0px6t",
        "outputId": "83527786-0f65-4f7e-a26f-9bcbbfbbec2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oB4W5pNn319",
        "outputId": "147f097b-2ff5-4ae7-9f87-5206997b94b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING_SET:  ('A', [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "TEST_SET:  ('A', [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "MESSAGE:  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "from dataset import * # Import the entire dataset\n",
        "import random # Needed to generate random numbers\n",
        "import numpy as np # Needed to do NumPy functions\n",
        "from matplotlib import pyplot as plt # Needed to do matplotlib graphs\n",
        "\n",
        "print(\"TRAINING_SET: \", TRAINING_SET[0])\n",
        "print(\"TEST_SET: \", TEST_SET[0])\n",
        "print(\"MESSAGE: \", MESSAGE[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dth6n0lcl37G"
      },
      "source": [
        "# 1. As with Project 1, convert the images in TRAINING_SET, TEST_SET, and MESSAGE into two-dimensional NumPy arrays of size (# examples Ã— # features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awAfKGbfpiYl"
      },
      "outputs": [],
      "source": [
        "def show(image):\n",
        "    letter_len = len(image)\n",
        "    counter = 0\n",
        "    for x in range(letter_len):\n",
        "        if image[x] == 1:\n",
        "            print('#', end='')\n",
        "        else:\n",
        "            print(' ', end='')\n",
        "        counter += 1\n",
        "        if counter == 16:\n",
        "            counter = 0\n",
        "            print('\\n', end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEtinAfAkx_-",
        "outputId": "c78c06cd-1c7f-4397-d282-537f4df467d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(52, 256)\n",
            "(26, 256)\n",
            "(31, 256)\n",
            "\n",
            "The image of the first letter: \n",
            "\n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "The letter list:  <function letter_list at 0x7f2255967ef0>\n"
          ]
        }
      ],
      "source": [
        "def convert_2d_array(input_data):\n",
        "    if len(input_data) == 0:\n",
        "        return None\n",
        "    output_data = []\n",
        "    if len(input_data[0]) == 2:\n",
        "        for x, y in input_data:\n",
        "            output_data.append(np.array(y))\n",
        "    else:\n",
        "        for x in input_data:\n",
        "            output_data.append(np.array(x))\n",
        "    return np.array(output_data)\n",
        "\n",
        "def letter_list(input_data):\n",
        "    if len(input_data) == 0:\n",
        "        return None\n",
        "    output_data = []\n",
        "    if len(input_data[0]) == 2:\n",
        "        for x, y in input_data:\n",
        "            output_data.append(np.array(x))\n",
        "    return np.array(output_data)\n",
        "\n",
        "TRAINING_SET_2D = convert_2d_array(TRAINING_SET)\n",
        "TEST_SET_2D = convert_2d_array(TEST_SET)\n",
        "MESSAGE_2D = convert_2d_array(MESSAGE)\n",
        "\n",
        "# Verify that each 2-dimensional NumPy array contains the same number of elements from the original arrays.\n",
        "print(TRAINING_SET_2D.shape)\n",
        "print(TEST_SET_2D.shape)\n",
        "print(MESSAGE_2D.shape)\n",
        "\n",
        "print(\"\\nThe image of the first letter: \\n\")\n",
        "show(TRAINING_SET_2D[0])\n",
        "\n",
        "letter_list_train=letter_list(TRAINING_SET)\n",
        "letter_list_test=letter_list(TEST_SET)\n",
        "print('The letter list: ',letter_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieAOi1CcmZXT"
      },
      "source": [
        "# 2. Rather than training 26 different perceptrons as you did in Project 1, this time you will use a single network with 26 possible outputs.\n",
        "# In order to use the character labels in TRAINING_SET and TEST_SET, convert them into integer class vectors using ord(), then into 26 one-hot encoded categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpwNzPB9uj9b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT8OYTrerfqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1e9852-a755-4e5d-eebd-8a34fbaab272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "unicode_letters_train, unicode_letters_test = [], []\n",
        "for i in letter_list_train:\n",
        "  unicode_char=ord(i)-ord('A')\n",
        "  unicode_letters_train.append(unicode_char)\n",
        "\n",
        "print(unicode_letters_train)\n",
        "\n",
        "unicode_cat_train = tf.keras.utils.to_categorical(unicode_letters_train, num_classes=None, dtype=\"float32\")\n",
        "\n",
        "unicode_cat_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7SRbOcSnOpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8170dea0-6cef-4a13-9b3d-6a7ddf251114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "for i in letter_list_test:\n",
        "  unicode_char=ord(i)-ord('A')\n",
        "  unicode_letters_test.append(unicode_char)\n",
        "\n",
        "print(unicode_letters_test)\n",
        "\n",
        "unicode_cat_test = tf.keras.utils.to_categorical(unicode_letters_test, num_classes=None, dtype=\"float32\")\n",
        "\n",
        "unicode_cat_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMjg_gFmmjpN"
      },
      "source": [
        "# 3. Create a Sequential Keras model with a Dense hidden layer and a Dense output layer with softmax activation and categorical cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWI6X0D7NK0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560a9dab-7efc-4c87-bda9-c641ddd82ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Hidden_layer (Dense)        (None, 32)                8224      \n",
            "                                                                 \n",
            " Output_layer (Dense)        (None, 26)                858       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,082\n",
            "Trainable params: 9,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create a sequential model with two Dense layers:\n",
        "# One as a hidden layer and the other as an output layer\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(256,)))\n",
        "model.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer\"))\n",
        "model.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZoyNBQygS1B"
      },
      "source": [
        "# 4. compile and fit the model to the training set. Train the model until the accuracy is as high as possible. You may wish to use an EarlyStopping callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn4Aze_nhvdK"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "             metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3FRJkvMiNv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b3c20e-72cb-41d4-9a88-437c66947029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9999\n",
            "2/2 - 1s - loss: 3.4528 - categorical_accuracy: 0.0385 - 578ms/epoch - 289ms/step\n",
            "Epoch 2/9999\n",
            "2/2 - 0s - loss: 3.3588 - categorical_accuracy: 0.0385 - 13ms/epoch - 6ms/step\n",
            "Epoch 3/9999\n",
            "2/2 - 0s - loss: 3.2932 - categorical_accuracy: 0.0385 - 10ms/epoch - 5ms/step\n",
            "Epoch 4/9999\n",
            "2/2 - 0s - loss: 3.2432 - categorical_accuracy: 0.0192 - 9ms/epoch - 4ms/step\n",
            "Epoch 5/9999\n",
            "2/2 - 0s - loss: 3.1978 - categorical_accuracy: 0.0385 - 8ms/epoch - 4ms/step\n",
            "Epoch 6/9999\n",
            "2/2 - 0s - loss: 3.1630 - categorical_accuracy: 0.0385 - 7ms/epoch - 4ms/step\n",
            "Epoch 7/9999\n",
            "2/2 - 0s - loss: 3.1278 - categorical_accuracy: 0.0769 - 8ms/epoch - 4ms/step\n",
            "Epoch 8/9999\n",
            "2/2 - 0s - loss: 3.0921 - categorical_accuracy: 0.0962 - 7ms/epoch - 4ms/step\n",
            "Epoch 9/9999\n",
            "2/2 - 0s - loss: 3.0600 - categorical_accuracy: 0.0769 - 8ms/epoch - 4ms/step\n",
            "Epoch 10/9999\n",
            "2/2 - 0s - loss: 3.0291 - categorical_accuracy: 0.1346 - 7ms/epoch - 4ms/step\n",
            "Epoch 11/9999\n",
            "2/2 - 0s - loss: 2.9985 - categorical_accuracy: 0.1538 - 8ms/epoch - 4ms/step\n",
            "Epoch 12/9999\n",
            "2/2 - 0s - loss: 2.9670 - categorical_accuracy: 0.1923 - 8ms/epoch - 4ms/step\n",
            "Epoch 13/9999\n",
            "2/2 - 0s - loss: 2.9381 - categorical_accuracy: 0.1923 - 10ms/epoch - 5ms/step\n",
            "Epoch 14/9999\n",
            "2/2 - 0s - loss: 2.9077 - categorical_accuracy: 0.2115 - 6ms/epoch - 3ms/step\n",
            "Epoch 15/9999\n",
            "2/2 - 0s - loss: 2.8784 - categorical_accuracy: 0.2308 - 7ms/epoch - 3ms/step\n",
            "Epoch 16/9999\n",
            "2/2 - 0s - loss: 2.8506 - categorical_accuracy: 0.2500 - 6ms/epoch - 3ms/step\n",
            "Epoch 17/9999\n",
            "2/2 - 0s - loss: 2.8207 - categorical_accuracy: 0.2115 - 7ms/epoch - 3ms/step\n",
            "Epoch 18/9999\n",
            "2/2 - 0s - loss: 2.7909 - categorical_accuracy: 0.2308 - 8ms/epoch - 4ms/step\n",
            "Epoch 19/9999\n",
            "2/2 - 0s - loss: 2.7625 - categorical_accuracy: 0.2500 - 8ms/epoch - 4ms/step\n",
            "Epoch 20/9999\n",
            "2/2 - 0s - loss: 2.7317 - categorical_accuracy: 0.2885 - 7ms/epoch - 4ms/step\n",
            "Epoch 21/9999\n",
            "2/2 - 0s - loss: 2.7026 - categorical_accuracy: 0.3269 - 7ms/epoch - 3ms/step\n",
            "Epoch 22/9999\n",
            "2/2 - 0s - loss: 2.6715 - categorical_accuracy: 0.3654 - 8ms/epoch - 4ms/step\n",
            "Epoch 23/9999\n",
            "2/2 - 0s - loss: 2.6446 - categorical_accuracy: 0.3462 - 6ms/epoch - 3ms/step\n",
            "Epoch 24/9999\n",
            "2/2 - 0s - loss: 2.6152 - categorical_accuracy: 0.3462 - 7ms/epoch - 3ms/step\n",
            "Epoch 25/9999\n",
            "2/2 - 0s - loss: 2.5848 - categorical_accuracy: 0.3654 - 8ms/epoch - 4ms/step\n",
            "Epoch 26/9999\n",
            "2/2 - 0s - loss: 2.5537 - categorical_accuracy: 0.4231 - 8ms/epoch - 4ms/step\n",
            "Epoch 27/9999\n",
            "2/2 - 0s - loss: 2.5236 - categorical_accuracy: 0.4423 - 8ms/epoch - 4ms/step\n",
            "Epoch 28/9999\n",
            "2/2 - 0s - loss: 2.4927 - categorical_accuracy: 0.4615 - 7ms/epoch - 3ms/step\n",
            "Epoch 29/9999\n",
            "2/2 - 0s - loss: 2.4619 - categorical_accuracy: 0.4615 - 7ms/epoch - 4ms/step\n",
            "Epoch 30/9999\n",
            "2/2 - 0s - loss: 2.4304 - categorical_accuracy: 0.4615 - 7ms/epoch - 4ms/step\n",
            "Epoch 31/9999\n",
            "2/2 - 0s - loss: 2.3982 - categorical_accuracy: 0.4808 - 11ms/epoch - 6ms/step\n",
            "Epoch 32/9999\n",
            "2/2 - 0s - loss: 2.3685 - categorical_accuracy: 0.4808 - 7ms/epoch - 3ms/step\n",
            "Epoch 33/9999\n",
            "2/2 - 0s - loss: 2.3343 - categorical_accuracy: 0.5000 - 7ms/epoch - 4ms/step\n",
            "Epoch 34/9999\n",
            "2/2 - 0s - loss: 2.3005 - categorical_accuracy: 0.5192 - 7ms/epoch - 3ms/step\n",
            "Epoch 35/9999\n",
            "2/2 - 0s - loss: 2.2675 - categorical_accuracy: 0.5385 - 7ms/epoch - 3ms/step\n",
            "Epoch 36/9999\n",
            "2/2 - 0s - loss: 2.2349 - categorical_accuracy: 0.5577 - 6ms/epoch - 3ms/step\n",
            "Epoch 37/9999\n",
            "2/2 - 0s - loss: 2.2006 - categorical_accuracy: 0.5577 - 7ms/epoch - 4ms/step\n",
            "Epoch 38/9999\n",
            "2/2 - 0s - loss: 2.1663 - categorical_accuracy: 0.5577 - 10ms/epoch - 5ms/step\n",
            "Epoch 39/9999\n",
            "2/2 - 0s - loss: 2.1319 - categorical_accuracy: 0.5769 - 7ms/epoch - 4ms/step\n",
            "Epoch 40/9999\n",
            "2/2 - 0s - loss: 2.0980 - categorical_accuracy: 0.5769 - 9ms/epoch - 4ms/step\n",
            "Epoch 41/9999\n",
            "2/2 - 0s - loss: 2.0629 - categorical_accuracy: 0.5769 - 9ms/epoch - 5ms/step\n",
            "Epoch 42/9999\n",
            "2/2 - 0s - loss: 2.0274 - categorical_accuracy: 0.5769 - 9ms/epoch - 5ms/step\n",
            "Epoch 43/9999\n",
            "2/2 - 0s - loss: 1.9925 - categorical_accuracy: 0.5962 - 9ms/epoch - 4ms/step\n",
            "Epoch 44/9999\n",
            "2/2 - 0s - loss: 1.9559 - categorical_accuracy: 0.5962 - 7ms/epoch - 4ms/step\n",
            "Epoch 45/9999\n",
            "2/2 - 0s - loss: 1.9188 - categorical_accuracy: 0.5962 - 8ms/epoch - 4ms/step\n",
            "Epoch 46/9999\n",
            "2/2 - 0s - loss: 1.8819 - categorical_accuracy: 0.5962 - 11ms/epoch - 5ms/step\n",
            "Epoch 47/9999\n",
            "2/2 - 0s - loss: 1.8452 - categorical_accuracy: 0.5962 - 10ms/epoch - 5ms/step\n",
            "Epoch 48/9999\n",
            "2/2 - 0s - loss: 1.8073 - categorical_accuracy: 0.6154 - 6ms/epoch - 3ms/step\n",
            "Epoch 49/9999\n",
            "2/2 - 0s - loss: 1.7707 - categorical_accuracy: 0.6538 - 6ms/epoch - 3ms/step\n",
            "Epoch 50/9999\n",
            "2/2 - 0s - loss: 1.7331 - categorical_accuracy: 0.6538 - 6ms/epoch - 3ms/step\n",
            "Epoch 51/9999\n",
            "2/2 - 0s - loss: 1.6950 - categorical_accuracy: 0.6923 - 7ms/epoch - 3ms/step\n",
            "Epoch 52/9999\n",
            "2/2 - 0s - loss: 1.6577 - categorical_accuracy: 0.7115 - 9ms/epoch - 4ms/step\n",
            "Epoch 53/9999\n",
            "2/2 - 0s - loss: 1.6209 - categorical_accuracy: 0.7308 - 8ms/epoch - 4ms/step\n",
            "Epoch 54/9999\n",
            "2/2 - 0s - loss: 1.5849 - categorical_accuracy: 0.7308 - 8ms/epoch - 4ms/step\n",
            "Epoch 55/9999\n",
            "2/2 - 0s - loss: 1.5475 - categorical_accuracy: 0.7500 - 6ms/epoch - 3ms/step\n",
            "Epoch 56/9999\n",
            "2/2 - 0s - loss: 1.5087 - categorical_accuracy: 0.7500 - 6ms/epoch - 3ms/step\n",
            "Epoch 57/9999\n",
            "2/2 - 0s - loss: 1.4742 - categorical_accuracy: 0.8077 - 6ms/epoch - 3ms/step\n",
            "Epoch 58/9999\n",
            "2/2 - 0s - loss: 1.4378 - categorical_accuracy: 0.8269 - 7ms/epoch - 3ms/step\n",
            "Epoch 59/9999\n",
            "2/2 - 0s - loss: 1.4011 - categorical_accuracy: 0.8269 - 6ms/epoch - 3ms/step\n",
            "Epoch 60/9999\n",
            "2/2 - 0s - loss: 1.3661 - categorical_accuracy: 0.8462 - 10ms/epoch - 5ms/step\n",
            "Epoch 61/9999\n",
            "2/2 - 0s - loss: 1.3316 - categorical_accuracy: 0.8654 - 10ms/epoch - 5ms/step\n",
            "Epoch 62/9999\n",
            "2/2 - 0s - loss: 1.2969 - categorical_accuracy: 0.8462 - 8ms/epoch - 4ms/step\n",
            "Epoch 63/9999\n",
            "2/2 - 0s - loss: 1.2649 - categorical_accuracy: 0.8462 - 10ms/epoch - 5ms/step\n",
            "Epoch 64/9999\n",
            "2/2 - 0s - loss: 1.2305 - categorical_accuracy: 0.8846 - 8ms/epoch - 4ms/step\n",
            "Epoch 65/9999\n",
            "2/2 - 0s - loss: 1.1994 - categorical_accuracy: 0.8654 - 7ms/epoch - 4ms/step\n",
            "Epoch 66/9999\n",
            "2/2 - 0s - loss: 1.1685 - categorical_accuracy: 0.9038 - 9ms/epoch - 4ms/step\n",
            "Epoch 67/9999\n",
            "2/2 - 0s - loss: 1.1392 - categorical_accuracy: 0.9038 - 9ms/epoch - 4ms/step\n",
            "Epoch 68/9999\n",
            "2/2 - 0s - loss: 1.1080 - categorical_accuracy: 0.9231 - 6ms/epoch - 3ms/step\n",
            "Epoch 69/9999\n",
            "2/2 - 0s - loss: 1.0795 - categorical_accuracy: 0.9231 - 7ms/epoch - 3ms/step\n",
            "Epoch 70/9999\n",
            "2/2 - 0s - loss: 1.0512 - categorical_accuracy: 0.9231 - 7ms/epoch - 3ms/step\n",
            "Epoch 71/9999\n",
            "2/2 - 0s - loss: 1.0249 - categorical_accuracy: 0.9231 - 6ms/epoch - 3ms/step\n",
            "Epoch 72/9999\n",
            "2/2 - 0s - loss: 0.9979 - categorical_accuracy: 0.9231 - 6ms/epoch - 3ms/step\n",
            "Epoch 73/9999\n",
            "2/2 - 0s - loss: 0.9732 - categorical_accuracy: 0.9231 - 6ms/epoch - 3ms/step\n",
            "Epoch 74/9999\n",
            "2/2 - 0s - loss: 0.9481 - categorical_accuracy: 0.9231 - 9ms/epoch - 4ms/step\n",
            "Epoch 75/9999\n",
            "2/2 - 0s - loss: 0.9238 - categorical_accuracy: 0.9231 - 7ms/epoch - 3ms/step\n",
            "Epoch 76/9999\n",
            "2/2 - 0s - loss: 0.8996 - categorical_accuracy: 0.9231 - 8ms/epoch - 4ms/step\n",
            "Epoch 77/9999\n",
            "2/2 - 0s - loss: 0.8777 - categorical_accuracy: 0.9423 - 6ms/epoch - 3ms/step\n",
            "Epoch 78/9999\n",
            "2/2 - 0s - loss: 0.8568 - categorical_accuracy: 0.9423 - 7ms/epoch - 4ms/step\n",
            "Epoch 79/9999\n",
            "2/2 - 0s - loss: 0.8360 - categorical_accuracy: 0.9423 - 10ms/epoch - 5ms/step\n",
            "Epoch 80/9999\n",
            "2/2 - 0s - loss: 0.8147 - categorical_accuracy: 0.9423 - 12ms/epoch - 6ms/step\n",
            "Epoch 81/9999\n",
            "2/2 - 0s - loss: 0.7954 - categorical_accuracy: 0.9423 - 9ms/epoch - 4ms/step\n",
            "Epoch 82/9999\n",
            "2/2 - 0s - loss: 0.7765 - categorical_accuracy: 0.9423 - 8ms/epoch - 4ms/step\n",
            "Epoch 83/9999\n",
            "2/2 - 0s - loss: 0.7580 - categorical_accuracy: 0.9423 - 8ms/epoch - 4ms/step\n",
            "Epoch 84/9999\n",
            "2/2 - 0s - loss: 0.7394 - categorical_accuracy: 0.9615 - 7ms/epoch - 3ms/step\n",
            "Epoch 85/9999\n",
            "2/2 - 0s - loss: 0.7221 - categorical_accuracy: 0.9615 - 8ms/epoch - 4ms/step\n",
            "Epoch 86/9999\n",
            "2/2 - 0s - loss: 0.7058 - categorical_accuracy: 0.9615 - 7ms/epoch - 4ms/step\n",
            "Epoch 87/9999\n",
            "2/2 - 0s - loss: 0.6890 - categorical_accuracy: 0.9615 - 9ms/epoch - 4ms/step\n",
            "Epoch 88/9999\n",
            "2/2 - 0s - loss: 0.6747 - categorical_accuracy: 0.9615 - 6ms/epoch - 3ms/step\n",
            "Epoch 89/9999\n",
            "2/2 - 0s - loss: 0.6588 - categorical_accuracy: 0.9615 - 7ms/epoch - 3ms/step\n",
            "Epoch 90/9999\n",
            "2/2 - 0s - loss: 0.6438 - categorical_accuracy: 0.9615 - 7ms/epoch - 3ms/step\n",
            "Epoch 91/9999\n",
            "2/2 - 0s - loss: 0.6299 - categorical_accuracy: 0.9615 - 7ms/epoch - 4ms/step\n",
            "Epoch 92/9999\n",
            "2/2 - 0s - loss: 0.6154 - categorical_accuracy: 0.9615 - 7ms/epoch - 4ms/step\n",
            "Epoch 93/9999\n",
            "2/2 - 0s - loss: 0.6018 - categorical_accuracy: 0.9615 - 8ms/epoch - 4ms/step\n",
            "Epoch 94/9999\n",
            "2/2 - 0s - loss: 0.5889 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 95/9999\n",
            "2/2 - 0s - loss: 0.5772 - categorical_accuracy: 0.9808 - 13ms/epoch - 7ms/step\n",
            "Epoch 96/9999\n",
            "2/2 - 0s - loss: 0.5640 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 97/9999\n",
            "2/2 - 0s - loss: 0.5522 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 98/9999\n",
            "2/2 - 0s - loss: 0.5404 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 99/9999\n",
            "2/2 - 0s - loss: 0.5298 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 100/9999\n",
            "2/2 - 0s - loss: 0.5180 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 101/9999\n",
            "2/2 - 0s - loss: 0.5074 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 102/9999\n",
            "2/2 - 0s - loss: 0.4969 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 103/9999\n",
            "2/2 - 0s - loss: 0.4868 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 104/9999\n",
            "2/2 - 0s - loss: 0.4770 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 105/9999\n",
            "2/2 - 0s - loss: 0.4678 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 106/9999\n",
            "2/2 - 0s - loss: 0.4581 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 107/9999\n",
            "2/2 - 0s - loss: 0.4488 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 108/9999\n",
            "2/2 - 0s - loss: 0.4407 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 109/9999\n",
            "2/2 - 0s - loss: 0.4324 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 110/9999\n",
            "2/2 - 0s - loss: 0.4237 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 111/9999\n",
            "2/2 - 0s - loss: 0.4157 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 112/9999\n",
            "2/2 - 0s - loss: 0.4075 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 113/9999\n",
            "2/2 - 0s - loss: 0.3995 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 114/9999\n",
            "2/2 - 0s - loss: 0.3919 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 115/9999\n",
            "2/2 - 0s - loss: 0.3850 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 116/9999\n",
            "2/2 - 0s - loss: 0.3772 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 117/9999\n",
            "2/2 - 0s - loss: 0.3707 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 118/9999\n",
            "2/2 - 0s - loss: 0.3638 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 119/9999\n",
            "2/2 - 0s - loss: 0.3574 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 120/9999\n",
            "2/2 - 0s - loss: 0.3502 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 121/9999\n",
            "2/2 - 0s - loss: 0.3447 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 122/9999\n",
            "2/2 - 0s - loss: 0.3379 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 123/9999\n",
            "2/2 - 0s - loss: 0.3317 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 124/9999\n",
            "2/2 - 0s - loss: 0.3259 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 125/9999\n",
            "2/2 - 0s - loss: 0.3202 - categorical_accuracy: 1.0000 - 7ms/epoch - 3ms/step\n",
            "Epoch 126/9999\n",
            "2/2 - 0s - loss: 0.3146 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 127/9999\n",
            "2/2 - 0s - loss: 0.3093 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 128/9999\n",
            "2/2 - 0s - loss: 0.3041 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 129/9999\n",
            "2/2 - 0s - loss: 0.2986 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 130/9999\n",
            "2/2 - 0s - loss: 0.2940 - categorical_accuracy: 1.0000 - 7ms/epoch - 3ms/step\n",
            "Epoch 131/9999\n",
            "2/2 - 0s - loss: 0.2885 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 132/9999\n",
            "2/2 - 0s - loss: 0.2834 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 133/9999\n",
            "2/2 - 0s - loss: 0.2795 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 134/9999\n",
            "2/2 - 0s - loss: 0.2741 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 135/9999\n",
            "2/2 - 0s - loss: 0.2696 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 136/9999\n",
            "2/2 - 0s - loss: 0.2652 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 137/9999\n",
            "2/2 - 0s - loss: 0.2605 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 138/9999\n",
            "2/2 - 0s - loss: 0.2565 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 139/9999\n",
            "2/2 - 0s - loss: 0.2522 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 140/9999\n",
            "2/2 - 0s - loss: 0.2489 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 141/9999\n",
            "2/2 - 0s - loss: 0.2444 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 142/9999\n",
            "2/2 - 0s - loss: 0.2403 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 143/9999\n",
            "2/2 - 0s - loss: 0.2366 - categorical_accuracy: 1.0000 - 7ms/epoch - 3ms/step\n",
            "Epoch 144/9999\n",
            "2/2 - 0s - loss: 0.2330 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 145/9999\n",
            "2/2 - 0s - loss: 0.2290 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 146/9999\n",
            "2/2 - 0s - loss: 0.2256 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6fd420b10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
        "model.fit(x=TRAINING_SET_2D, y=unicode_cat_train, epochs=9999, verbose=2, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO1B3uswgWya"
      },
      "source": [
        "# 5. evaluate the model on TEST_SET. What accuracy do you obtain? If the accuracy is less than 100%, which test images are misclassified? (You may wish to use the show(image) function you defined in the previous project.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ATf3Nujn4nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c324c11c-6be7-462b-cb6b-7955dcadb131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 145ms/step - loss: 1.9801 - categorical_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9801195859909058, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.evaluate(x=TEST_SET_2D, y=unicode_cat_test, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(TEST_SET_2D)\n",
        "predicted"
      ],
      "metadata": {
        "id": "IEy44YTHDDzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d24aec-7fe3-46c4-90f7-443355523150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.02513505e-03, 3.05083375e-02, 1.79769713e-02, 5.16004376e-02,\n",
              "        4.39267084e-02, 7.06898107e-04, 1.44235343e-01, 2.03023804e-03,\n",
              "        9.75096226e-03, 1.76142610e-04, 3.09254765e-03, 5.35802722e-01,\n",
              "        9.51141957e-03, 4.13988065e-03, 7.14969635e-03, 5.21164900e-03,\n",
              "        5.12696616e-03, 4.98341247e-02, 1.90750360e-02, 2.04650103e-03,\n",
              "        4.53977194e-03, 1.98147772e-03, 1.55633613e-02, 2.43702685e-04,\n",
              "        2.17422564e-03, 3.25696878e-02],\n",
              "       [5.12878432e-05, 2.19780371e-01, 6.22161618e-03, 2.93662269e-02,\n",
              "        1.81281149e-01, 1.06917266e-02, 1.48294738e-03, 1.04586792e-03,\n",
              "        4.70995754e-02, 4.98107285e-04, 8.61471221e-02, 3.94728370e-02,\n",
              "        1.62258968e-02, 5.51158935e-03, 4.42455959e-04, 7.64014274e-02,\n",
              "        1.00408925e-03, 1.84742808e-02, 2.79935766e-02, 3.85767482e-02,\n",
              "        5.44883696e-05, 3.30274255e-04, 9.71107802e-04, 8.56651925e-03,\n",
              "        1.14271464e-02, 1.70881540e-01],\n",
              "       [2.75984546e-03, 2.37952080e-02, 5.48033118e-01, 3.83321382e-03,\n",
              "        1.12476386e-01, 1.22774076e-02, 5.54994158e-02, 5.28823584e-03,\n",
              "        1.33273238e-02, 8.22627638e-03, 8.48681573e-03, 2.05479190e-02,\n",
              "        2.46816780e-03, 2.17006207e-02, 2.27735359e-02, 4.78935875e-02,\n",
              "        1.51663339e-02, 2.78427061e-02, 1.31201709e-03, 2.07461324e-03,\n",
              "        2.28795200e-03, 3.74748255e-03, 3.06673604e-03, 3.03655537e-03,\n",
              "        2.72794836e-03, 2.93504559e-02],\n",
              "       [6.19108760e-05, 7.08681121e-02, 1.42018456e-04, 6.95837140e-01,\n",
              "        6.46126643e-03, 1.12571125e-03, 5.52699901e-04, 7.29381936e-05,\n",
              "        4.55647055e-03, 1.34899121e-04, 1.97463646e-03, 1.42793089e-01,\n",
              "        1.05225353e-03, 9.28070313e-06, 1.24340516e-03, 3.39572318e-03,\n",
              "        1.25666207e-04, 5.35752578e-03, 3.26572247e-02, 6.28258288e-03,\n",
              "        3.67434899e-04, 5.33092361e-06, 7.39669576e-06, 3.44244647e-04,\n",
              "        5.51163161e-04, 2.40200423e-02],\n",
              "       [4.43256084e-07, 1.65712822e-03, 6.66385167e-04, 5.63520298e-04,\n",
              "        7.02036738e-01, 2.84962934e-02, 1.21878821e-03, 1.86624195e-04,\n",
              "        5.96067449e-03, 1.14444224e-03, 5.66272661e-02, 1.92369975e-03,\n",
              "        5.78829669e-04, 2.04864773e-04, 2.42369561e-06, 9.15590441e-04,\n",
              "        4.41977958e-04, 1.83415739e-03, 9.06862132e-03, 7.33353803e-03,\n",
              "        4.65657013e-06, 1.98953257e-05, 4.47776983e-04, 1.63329951e-03,\n",
              "        5.87579422e-03, 1.71156615e-01],\n",
              "       [3.29901331e-06, 8.17127468e-04, 2.61086342e-03, 2.85655470e-03,\n",
              "        6.77853078e-02, 5.48854351e-01, 2.63118505e-04, 7.56429625e-04,\n",
              "        7.28853866e-02, 2.57268492e-02, 5.36406226e-02, 6.89926557e-04,\n",
              "        4.16613603e-03, 3.88001383e-04, 1.32193545e-05, 1.45444889e-02,\n",
              "        1.38246373e-03, 2.84425705e-03, 1.33837080e-02, 3.86888459e-02,\n",
              "        4.71248823e-05, 4.14620910e-04, 1.17345969e-03, 3.05755213e-02,\n",
              "        6.54384568e-02, 5.00498414e-02],\n",
              "       [2.70864257e-04, 4.19071317e-03, 4.56087552e-02, 7.09960528e-04,\n",
              "        1.33169025e-01, 2.58289947e-04, 6.53786898e-01, 7.14158174e-04,\n",
              "        2.12739594e-03, 2.08004596e-04, 3.69121949e-03, 4.99519296e-02,\n",
              "        4.44714940e-04, 6.52727159e-03, 9.94193833e-04, 5.92751137e-04,\n",
              "        3.01613822e-03, 2.04660967e-02, 5.87522751e-04, 1.73234861e-04,\n",
              "        2.52688100e-04, 2.96546612e-04, 2.64633098e-03, 4.84185621e-05,\n",
              "        1.47291561e-04, 6.91195428e-02],\n",
              "       [6.10811170e-04, 4.85044606e-02, 2.44010007e-03, 6.40468393e-03,\n",
              "        2.73702703e-02, 7.05672847e-03, 1.72936451e-02, 1.09466858e-01,\n",
              "        1.68548268e-03, 7.84420117e-05, 6.78845942e-02, 3.91872413e-02,\n",
              "        1.77909851e-01, 1.44426122e-01, 3.17429123e-03, 4.40935083e-02,\n",
              "        3.46977497e-03, 8.27549398e-02, 6.11336343e-02, 9.34216252e-04,\n",
              "        1.18221482e-02, 1.73747148e-02, 9.66428518e-02, 4.36431123e-03,\n",
              "        1.25610614e-02, 1.13552762e-02],\n",
              "       [7.95815358e-05, 2.16835819e-04, 1.83250348e-04, 4.88615420e-04,\n",
              "        4.64036071e-04, 1.55920556e-04, 6.19394559e-05, 4.76770278e-04,\n",
              "        8.35132003e-01, 6.10486278e-03, 2.88045645e-04, 8.01417802e-04,\n",
              "        1.21536187e-03, 1.51806904e-04, 2.52635855e-05, 1.75813329e-04,\n",
              "        6.87663152e-04, 5.84846457e-05, 2.50247773e-03, 1.33624837e-01,\n",
              "        1.10144883e-05, 9.21033861e-05, 4.33906825e-04, 8.54135375e-04,\n",
              "        3.91491456e-03, 1.17988894e-02],\n",
              "       [4.35285456e-03, 4.27934192e-02, 5.70743019e-03, 4.00470346e-02,\n",
              "        2.26793368e-03, 1.01099852e-02, 2.34444533e-03, 1.98640972e-02,\n",
              "        1.92696496e-03, 4.34184104e-01, 1.84525494e-02, 8.21114983e-03,\n",
              "        1.19188847e-03, 1.14707975e-03, 1.02754682e-01, 5.25504071e-03,\n",
              "        1.21330377e-02, 1.02215130e-02, 1.39024919e-02, 1.67233907e-02,\n",
              "        1.14596628e-01, 1.25194946e-02, 5.41147031e-03, 1.42031107e-02,\n",
              "        3.08230473e-03, 9.65958536e-02],\n",
              "       [2.40468198e-05, 1.26362052e-02, 1.61446561e-03, 3.13059650e-02,\n",
              "        1.86172664e-01, 1.62807345e-01, 2.24102451e-03, 8.19973822e-04,\n",
              "        1.17715793e-02, 2.87601654e-03, 3.14732909e-01, 2.02396847e-02,\n",
              "        5.61774755e-03, 1.17850839e-03, 4.11798246e-05, 1.54341031e-02,\n",
              "        7.99547415e-04, 2.55521350e-02, 6.94203898e-02, 8.23391043e-03,\n",
              "        2.72999896e-04, 5.24673378e-04, 1.84465363e-03, 6.27967119e-02,\n",
              "        4.01994064e-02, 2.08421219e-02],\n",
              "       [5.53935461e-05, 7.87356570e-02, 1.95363816e-03, 2.87125409e-02,\n",
              "        3.52314472e-01, 1.76062423e-03, 1.02562020e-02, 2.12492057e-04,\n",
              "        6.57367520e-03, 1.18305230e-04, 2.26908140e-02, 3.44053358e-01,\n",
              "        1.43573992e-03, 6.82458864e-04, 4.32382600e-04, 3.33075086e-03,\n",
              "        3.40313913e-04, 1.99298915e-02, 2.14531403e-02, 4.95751575e-03,\n",
              "        1.03637321e-04, 4.13785565e-05, 4.31163964e-04, 5.94150915e-04,\n",
              "        6.74829644e-04, 9.81554836e-02],\n",
              "       [3.89013626e-03, 2.05643382e-02, 9.88595784e-02, 8.79251119e-03,\n",
              "        7.08833784e-02, 2.04982206e-01, 5.09022595e-03, 3.18349525e-02,\n",
              "        2.37603523e-02, 7.55290454e-03, 1.55518249e-01, 9.12429206e-03,\n",
              "        3.14223021e-02, 6.55356348e-02, 5.69054391e-03, 8.78717229e-02,\n",
              "        1.01134777e-02, 1.18558025e-02, 2.70134658e-02, 1.21987406e-02,\n",
              "        6.04653126e-03, 1.32317431e-02, 9.87862982e-03, 4.27162126e-02,\n",
              "        2.65036821e-02, 9.06838290e-03],\n",
              "       [7.83858728e-03, 2.92091798e-02, 1.08638294e-02, 4.64687217e-03,\n",
              "        3.16975303e-02, 3.76755581e-03, 6.59508556e-02, 5.71771003e-02,\n",
              "        7.87665416e-03, 3.54554904e-05, 4.80601750e-02, 4.69006561e-02,\n",
              "        2.14193285e-01, 2.86989927e-01, 8.56866315e-03, 1.04131009e-02,\n",
              "        6.82553276e-03, 5.71926162e-02, 3.36227827e-02, 3.03438748e-03,\n",
              "        7.44966185e-03, 3.72269121e-03, 3.37167867e-02, 8.85306858e-04,\n",
              "        9.18610301e-03, 1.01747196e-02],\n",
              "       [2.08782014e-02, 8.43300000e-02, 4.18106504e-02, 2.58207709e-01,\n",
              "        1.49108879e-02, 2.56540347e-03, 3.74698713e-02, 6.21946529e-03,\n",
              "        9.95492190e-03, 3.09144543e-03, 3.36884684e-03, 1.77377731e-01,\n",
              "        8.07527266e-03, 2.74214870e-03, 1.65945306e-01, 1.93086602e-02,\n",
              "        1.42847914e-02, 3.26158367e-02, 2.13709287e-02, 4.70405445e-03,\n",
              "        2.98336521e-02, 1.92593248e-03, 1.91741716e-03, 2.27635424e-03,\n",
              "        1.94997608e-03, 3.28646004e-02],\n",
              "       [2.85390433e-05, 2.54367329e-02, 4.27431334e-03, 4.53445576e-02,\n",
              "        1.08351754e-02, 1.55489191e-01, 7.14040798e-05, 4.46946971e-04,\n",
              "        5.66651896e-02, 6.99815759e-03, 3.51799205e-02, 2.30175792e-03,\n",
              "        1.62242390e-02, 1.27001293e-03, 3.31294636e-04, 4.95137751e-01,\n",
              "        1.25254551e-03, 1.51983919e-02, 3.77876544e-03, 4.26525734e-02,\n",
              "        1.16255818e-04, 1.82644487e-03, 3.67578090e-04, 3.56253497e-02,\n",
              "        3.19259167e-02, 1.12209059e-02],\n",
              "       [3.17984680e-03, 2.49131839e-03, 1.21465087e-01, 5.62157147e-02,\n",
              "        1.06638204e-02, 1.78043614e-03, 4.31771785e-01, 1.94200326e-03,\n",
              "        8.78030807e-03, 1.41663942e-03, 1.72697578e-03, 2.23943014e-02,\n",
              "        9.65661649e-03, 4.55075828e-03, 1.67964417e-02, 1.35711450e-02,\n",
              "        6.51973933e-02, 1.73725560e-01, 8.18518456e-04, 2.76510778e-04,\n",
              "        5.90560073e-03, 1.28228422e-02, 1.01654073e-02, 9.51850321e-04,\n",
              "        7.47523922e-03, 1.42578539e-02],\n",
              "       [1.43913812e-05, 3.03191505e-02, 5.31719998e-03, 1.23088352e-01,\n",
              "        9.01515037e-02, 1.65460721e-01, 2.21121917e-03, 2.85296323e-04,\n",
              "        2.64622495e-02, 7.49741727e-03, 1.07109837e-01, 1.23296855e-02,\n",
              "        1.83919948e-02, 1.65430433e-03, 2.47990800e-04, 1.85496151e-01,\n",
              "        3.07584391e-03, 9.01719034e-02, 7.23255007e-03, 2.20737532e-02,\n",
              "        2.11417690e-04, 1.93512370e-03, 2.01240508e-03, 1.59281213e-02,\n",
              "        5.04990481e-02, 3.08223404e-02],\n",
              "       [1.46457728e-03, 1.13123462e-01, 1.17349364e-01, 6.90174475e-03,\n",
              "        2.67229199e-01, 4.20117099e-03, 5.94562329e-02, 1.67860165e-02,\n",
              "        8.67546909e-03, 2.78620305e-03, 2.14038901e-02, 1.35630414e-01,\n",
              "        2.65951757e-03, 2.43406966e-02, 1.34665212e-02, 2.62723584e-02,\n",
              "        8.16675555e-03, 2.77049933e-02, 7.20251910e-03, 3.00294138e-03,\n",
              "        1.74209545e-03, 2.82237073e-03, 4.11512470e-03, 2.63411808e-03,\n",
              "        1.68506056e-03, 1.19177185e-01],\n",
              "       [2.89177813e-04, 3.40186816e-04, 4.96351533e-03, 1.42671273e-03,\n",
              "        7.41451308e-02, 2.88034435e-02, 6.32246491e-03, 1.58097427e-02,\n",
              "        1.31878313e-02, 5.16178131e-01, 3.42425262e-03, 8.91446776e-04,\n",
              "        1.70649914e-03, 6.72273600e-05, 1.07531203e-03, 2.98016908e-04,\n",
              "        1.10686295e-01, 5.78129780e-04, 2.02031098e-02, 2.64786929e-02,\n",
              "        2.60788249e-03, 2.31322163e-04, 1.21025462e-03, 1.33203231e-02,\n",
              "        2.17503477e-02, 1.34004474e-01],\n",
              "       [1.22975120e-02, 1.21289030e-01, 2.47999430e-02, 3.05213295e-02,\n",
              "        4.07636128e-02, 3.39036225e-03, 1.15977652e-01, 6.51303232e-02,\n",
              "        1.18047895e-03, 4.77016874e-04, 8.58282670e-03, 2.30359569e-01,\n",
              "        1.15283960e-02, 3.61391976e-02, 6.74843043e-02, 1.19185038e-02,\n",
              "        8.37334618e-03, 2.56792475e-02, 6.80902824e-02, 8.34961713e-04,\n",
              "        6.50178045e-02, 4.23539523e-03, 2.10489891e-02, 1.37466635e-03,\n",
              "        1.24971149e-03, 2.22555622e-02],\n",
              "       [4.96485364e-03, 6.32037688e-03, 1.48889944e-01, 5.44893928e-02,\n",
              "        3.24345566e-02, 4.70578251e-03, 2.54938424e-01, 1.67881250e-02,\n",
              "        1.27813360e-02, 1.85100571e-03, 8.42677150e-03, 2.02983901e-01,\n",
              "        9.24152043e-03, 6.54368335e-03, 2.18248460e-02, 2.31992733e-02,\n",
              "        2.83070747e-02, 5.38879298e-02, 5.18097123e-03, 1.63933414e-03,\n",
              "        2.45661847e-02, 3.63926813e-02, 9.47604980e-03, 2.21628393e-03,\n",
              "        8.10638163e-03, 1.98433660e-02],\n",
              "       [4.76395572e-03, 2.25652754e-03, 1.86524287e-01, 2.77439989e-02,\n",
              "        5.31382486e-02, 9.79785696e-02, 8.33706781e-02, 4.24384885e-02,\n",
              "        2.07277364e-03, 5.02090640e-02, 4.90057319e-02, 2.68470645e-02,\n",
              "        1.49433746e-03, 1.48632815e-02, 1.71325952e-02, 3.06650791e-02,\n",
              "        2.95084435e-02, 4.27973010e-02, 6.36384590e-03, 2.81155371e-04,\n",
              "        5.92279136e-02, 5.75166270e-02, 8.13074633e-02, 1.24420756e-02,\n",
              "        5.72531810e-03, 1.43252686e-02],\n",
              "       [9.53244627e-04, 6.26380965e-02, 1.60750318e-02, 7.58886943e-03,\n",
              "        5.82895130e-02, 1.24361580e-02, 4.42649378e-03, 9.08653438e-03,\n",
              "        5.62901683e-02, 8.99583159e-04, 7.40010440e-02, 1.22078341e-02,\n",
              "        1.58233821e-01, 5.05561195e-02, 2.09556194e-03, 7.22895041e-02,\n",
              "        1.58775542e-02, 9.70991626e-02, 1.18255988e-02, 6.16380461e-02,\n",
              "        2.36470927e-03, 9.48773324e-03, 1.20980395e-02, 3.04117203e-02,\n",
              "        1.43676609e-01, 1.74532011e-02],\n",
              "       [7.31540844e-03, 2.53469069e-02, 1.55089600e-02, 9.40783042e-03,\n",
              "        8.62559602e-02, 7.58470548e-03, 1.35867773e-02, 4.93015200e-02,\n",
              "        1.49730034e-02, 1.69307273e-02, 4.87231463e-02, 1.06336679e-02,\n",
              "        4.30870801e-02, 2.52367798e-02, 1.15491785e-02, 1.34796714e-02,\n",
              "        1.22901902e-01, 4.44085561e-02, 3.10758650e-02, 2.89920550e-02,\n",
              "        1.47347162e-02, 6.69238763e-03, 2.85093505e-02, 1.22930497e-01,\n",
              "        1.52283475e-01, 4.85499315e-02],\n",
              "       [2.61663390e-05, 8.22469685e-03, 2.46869307e-03, 3.39589491e-02,\n",
              "        9.28003415e-02, 2.23535597e-02, 1.07186567e-02, 1.67810929e-03,\n",
              "        1.41151976e-02, 1.10978838e-02, 3.90806887e-03, 3.84975132e-03,\n",
              "        3.29364277e-03, 4.36054717e-04, 9.37772798e-04, 8.96787923e-03,\n",
              "        1.49557060e-02, 1.26170814e-02, 7.68965902e-03, 1.74412504e-02,\n",
              "        7.62986485e-04, 3.99823417e-04, 1.92064547e-03, 4.65307338e-03,\n",
              "        8.50923210e-02, 6.35632038e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_list = []\n",
        "for num in range(len(predicted)):\n",
        "    max_val = 0\n",
        "    actual = chr(num+ord('A'))\n",
        "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    result_list.append([actual, predict])\n",
        "result_list"
      ],
      "metadata": {
        "id": "fHqN8KB0DD8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c586e94-0ccb-4dac-aa6d-1372b31b5f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A', 'L'],\n",
              " ['B', 'B'],\n",
              " ['C', 'C'],\n",
              " ['D', 'D'],\n",
              " ['E', 'E'],\n",
              " ['F', 'F'],\n",
              " ['G', 'G'],\n",
              " ['H', 'M'],\n",
              " ['I', 'I'],\n",
              " ['J', 'J'],\n",
              " ['K', 'K'],\n",
              " ['L', 'E'],\n",
              " ['M', 'F'],\n",
              " ['N', 'N'],\n",
              " ['O', 'D'],\n",
              " ['P', 'P'],\n",
              " ['Q', 'G'],\n",
              " ['R', 'P'],\n",
              " ['S', 'E'],\n",
              " ['T', 'J'],\n",
              " ['U', 'L'],\n",
              " ['V', 'G'],\n",
              " ['W', 'C'],\n",
              " ['X', 'M'],\n",
              " ['Y', 'Y'],\n",
              " ['Z', 'Z']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_misclassified = []\n",
        "num_total, num_correct = 26, 26\n",
        "for i in range(len(result_list)):\n",
        "    if result_list[i][0] != result_list[i][1]:\n",
        "        num_correct -= 1\n",
        "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
        "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_misclassified) == 0:\n",
        "    print(\"All test images are classified correctly.\")\n",
        "else:\n",
        "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_misclassified)):\n",
        "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
      ],
      "metadata": {
        "id": "M2xUr3DrDEDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use show() to display the misclassified images.\n",
        "for j in range(len(list_misclassified)):\n",
        "    # Subtract unicode value of current letter from uppercase A to obtain the letter's position.\n",
        "    letter_train = ord(list_misclassified[j][0]) - ord('A')\n",
        "    letter_test = ord(list_misclassified[j][1]) - ord('A')\n",
        "    print(\"================================================\")\n",
        "    print(\"Expected letter:\")\n",
        "    show(TRAINING_SET_2D[letter_train])\n",
        "    print(\"Predicted letter:\")\n",
        "    show(TEST_SET_2D[letter_test])\n",
        "    print(\"================================================\")"
      ],
      "metadata": {
        "id": "cqnzRP98DEJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1dd7da-423b-469e-8afb-52d1f3adcb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================\n",
            "Expected letter:\n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ######        \n",
            "  ######        \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ############  \n",
            "  ############  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ####      ####\n",
            "  ####      ####\n",
            "  ##  ##  ##  ##\n",
            "  ##  ##  ##  ##\n",
            "  ##    ##    ##\n",
            "  ##    ##    ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "########        \n",
            "########        \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "##############  \n",
            "##############  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ############  \n",
            "  ############  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ######      \n",
            "    ######      \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ############  \n",
            "  ############  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "######  ######  \n",
            "######  ######  \n",
            "##############  \n",
            "##############  \n",
            "##############  \n",
            "##############  \n",
            "####  ##  ####  \n",
            "####  ##  ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ############  \n",
            "  ############  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ######      \n",
            "    ######      \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##          \n",
            "    ##          \n",
            "  ######        \n",
            "  ######        \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "    ######      \n",
            "    ######      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "    ######      \n",
            "    ######      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ########      \n",
            "  ########      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####  ######    \n",
            "####  ######    \n",
            "  ########      \n",
            "  ########      \n",
            "      ######    \n",
            "      ######    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "      ######    \n",
            "      ######    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##    ######  \n",
            "  ##    ######  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "      ########  \n",
            "      ########  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "############    \n",
            "############    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ##########    \n",
            "  ##########    \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "######    ####  \n",
            "######    ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "  ######        \n",
            "  ######        \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "######          \n",
            "######          \n",
            "  ######        \n",
            "  ######        \n",
            "      ######    \n",
            "      ######    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ############  \n",
            "  ############  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ######      \n",
            "    ######      \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ############  \n",
            "  ############  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "############    \n",
            "############    \n",
            "##  ####  ##    \n",
            "##  ####  ##    \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "        ######  \n",
            "        ######  \n",
            "          ##    \n",
            "          ##    \n",
            "          ##    \n",
            "          ##    \n",
            "          ##    \n",
            "          ##    \n",
            "  ##      ##    \n",
            "  ##      ##    \n",
            "  ##      ##    \n",
            "  ##      ##    \n",
            "    ######      \n",
            "    ######      \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ######        \n",
            "  ######        \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ############  \n",
            "  ############  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "    ####        \n",
            "    ####        \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "      ######    \n",
            "      ######    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##    ######  \n",
            "  ##    ######  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "      ########  \n",
            "      ########  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####  ##  ####  \n",
            "####  ##  ####  \n",
            "##############  \n",
            "##############  \n",
            "######  ######  \n",
            "######  ######  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "      ######    \n",
            "      ######    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "      ######    \n",
            "      ######    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "    ######      \n",
            "    ######      \n",
            "    ######      \n",
            "    ######      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ####      ####\n",
            "  ####      ####\n",
            "  ##  ##  ##  ##\n",
            "  ##  ##  ##  ##\n",
            "  ##    ##    ##\n",
            "  ##    ##    ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "                \n",
            "                \n",
            "================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-pRa2Pq4lg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75a8990-353b-45f3-d50d-8c94730cd864"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.85460361e-04, 3.89215129e-04, 5.43438643e-03, 1.60095620e-03,\n",
              "        9.30173323e-02, 3.20525505e-02, 6.33868622e-03, 1.66265219e-02,\n",
              "        1.35281542e-02, 5.47677934e-01, 4.26922273e-03, 9.60822916e-04,\n",
              "        1.27501099e-03, 8.39457571e-05, 1.43077364e-03, 3.82391299e-04,\n",
              "        9.52755883e-02, 7.08355568e-04, 2.32306886e-02, 1.43190464e-02,\n",
              "        3.60464142e-03, 2.04511511e-04, 1.29180704e-03, 1.47984093e-02,\n",
              "        1.13429297e-02, 1.09670691e-01],\n",
              "       [1.29211193e-03, 5.78566454e-02, 5.02690813e-03, 8.53054971e-03,\n",
              "        2.79186890e-02, 8.46019760e-03, 2.57579833e-02, 1.12247407e-01,\n",
              "        1.65133958e-03, 1.37217241e-04, 5.19385487e-02, 4.16669659e-02,\n",
              "        1.35838851e-01, 1.73883528e-01, 5.74918976e-03, 4.68292050e-02,\n",
              "        5.16822934e-03, 6.43497109e-02, 5.96362799e-02, 1.14208262e-03,\n",
              "        1.70126930e-02, 2.05447003e-02, 9.99923423e-02, 4.28957492e-03,\n",
              "        1.19399140e-02, 1.11391442e-02],\n",
              "       [2.89994641e-05, 9.91124474e-03, 4.20511654e-03, 3.05379252e-03,\n",
              "        7.57435143e-01, 5.85626699e-02, 1.75845448e-03, 1.23091962e-03,\n",
              "        3.06327082e-03, 1.35463473e-04, 6.43830001e-02, 3.59409302e-02,\n",
              "        8.60401662e-04, 4.42810851e-04, 5.91141797e-05, 5.95591869e-03,\n",
              "        4.00178309e-04, 1.10512320e-02, 1.86582599e-02, 7.05588143e-04,\n",
              "        1.09753048e-04, 1.92833686e-05, 3.99932760e-05, 2.49922369e-03,\n",
              "        4.66739904e-04, 1.90224778e-02],\n",
              "       [1.18216576e-05, 2.30478379e-03, 6.39405334e-03, 2.77591427e-03,\n",
              "        5.56796864e-02, 8.42256188e-01, 1.66087892e-04, 8.14555562e-04,\n",
              "        1.25580386e-03, 9.56424861e-04, 3.04666422e-02, 2.16563768e-03,\n",
              "        1.14602281e-03, 3.56378179e-04, 3.65602828e-05, 3.35867070e-02,\n",
              "        1.26733619e-04, 5.65527659e-03, 3.52373254e-03, 1.08951610e-03,\n",
              "        1.07642190e-04, 8.25362222e-05, 1.90546853e-05, 5.18860109e-03,\n",
              "        3.10620200e-03, 7.27517763e-04],\n",
              "       [5.02900293e-05, 3.82596590e-05, 7.58257520e-05, 8.61297303e-05,\n",
              "        1.17309028e-04, 1.23800855e-04, 1.47829996e-05, 1.85059704e-04,\n",
              "        6.26386702e-01, 1.44617148e-02, 8.28619086e-05, 8.92678363e-05,\n",
              "        8.03864677e-04, 1.12047070e-04, 2.99227895e-06, 4.94641026e-05,\n",
              "        1.16276415e-03, 1.09550338e-05, 1.62169081e-03, 3.41600299e-01,\n",
              "        2.27302144e-06, 7.55049987e-05, 5.38959634e-04, 1.54505321e-03,\n",
              "        7.30660278e-03, 3.45559721e-03],\n",
              "       [7.75757362e-05, 6.06324056e-06, 4.05934360e-03, 1.72097309e-04,\n",
              "        9.97097686e-07, 4.40805488e-05, 4.96322318e-05, 5.01194876e-03,\n",
              "        8.34680177e-05, 5.32272318e-03, 8.95276171e-05, 7.97851226e-06,\n",
              "        1.85248745e-03, 6.55444819e-05, 1.58676435e-03, 1.61289005e-04,\n",
              "        1.44576067e-02, 1.51901646e-03, 3.56418641e-05, 1.18011121e-04,\n",
              "        1.04899574e-02, 9.17267859e-01, 2.77613346e-02, 2.48753419e-03,\n",
              "        6.49288762e-03, 7.78532587e-04],\n",
              "       [5.29519411e-07, 3.12902196e-03, 6.45177730e-04, 5.34214079e-04,\n",
              "        7.14921176e-01, 1.24974698e-02, 1.46266434e-03, 1.16282106e-04,\n",
              "        4.74668574e-03, 5.32469421e-04, 6.21121116e-02, 2.08874885e-03,\n",
              "        7.03814789e-04, 2.73284852e-04, 1.82099825e-06, 6.38481171e-04,\n",
              "        4.34488175e-04, 1.42456940e-03, 9.69094224e-03, 7.65974354e-03,\n",
              "        2.65578024e-06, 1.97765257e-05, 3.86931904e-04, 1.85667363e-03,\n",
              "        4.89740865e-03, 1.69222921e-01],\n",
              "       [1.23872278e-05, 8.47266197e-01, 9.10782313e-04, 1.96642503e-02,\n",
              "        7.36909173e-03, 2.66452495e-04, 1.24461571e-04, 3.53413256e-04,\n",
              "        3.93823393e-05, 4.72458896e-06, 1.56374052e-02, 2.47476622e-02,\n",
              "        8.73782439e-04, 1.11460756e-03, 1.66679011e-03, 3.11333593e-02,\n",
              "        6.18545673e-05, 3.99409272e-02, 1.86162628e-03, 1.26729967e-04,\n",
              "        2.89260963e-04, 6.51863083e-05, 4.13572452e-05, 1.06260600e-03,\n",
              "        2.52981670e-04, 5.11263730e-03],\n",
              "       [2.63422187e-02, 1.67944543e-02, 1.15303537e-02, 1.66595709e-02,\n",
              "        4.28525120e-04, 1.69085877e-04, 1.15218945e-02, 5.09609692e-02,\n",
              "        1.44319769e-04, 2.57285166e-04, 2.02981624e-04, 4.33524372e-03,\n",
              "        2.89067365e-02, 4.47833305e-03, 4.92082328e-01, 6.14322908e-03,\n",
              "        8.37883800e-02, 1.27334436e-02, 4.42950102e-03, 2.53112084e-04,\n",
              "        2.09484667e-01, 5.70610166e-03, 7.34077860e-03, 5.86658833e-04,\n",
              "        3.03074368e-03, 1.68908201e-03],\n",
              "       [1.12602301e-03, 5.92959151e-02, 1.56010399e-02, 8.29581637e-03,\n",
              "        6.49855807e-02, 1.04260892e-02, 5.09126484e-03, 1.21247843e-02,\n",
              "        6.49957135e-02, 7.94723455e-04, 7.16884956e-02, 1.48902657e-02,\n",
              "        1.60537824e-01, 4.44312058e-02, 2.30278075e-03, 5.82053922e-02,\n",
              "        1.72858275e-02, 6.47356585e-02, 1.66507121e-02, 6.94184154e-02,\n",
              "        2.30590929e-03, 5.61040966e-03, 1.02281999e-02, 2.69998536e-02,\n",
              "        1.72513917e-01, 1.94581505e-02],\n",
              "       [1.40324244e-06, 4.15748800e-04, 5.56141567e-05, 1.60519790e-03,\n",
              "        1.19509257e-03, 4.88502556e-04, 3.53137625e-06, 3.14943318e-05,\n",
              "        8.59753072e-01, 2.40994021e-04, 8.06693686e-04, 1.35429611e-03,\n",
              "        2.40267604e-03, 1.77586971e-05, 1.61901755e-06, 1.08698045e-03,\n",
              "        5.13179330e-05, 1.28319851e-04, 2.51414487e-03, 1.15208656e-01,\n",
              "        6.72661770e-07, 1.31699235e-05, 2.33329501e-05, 1.14348973e-03,\n",
              "        3.50153842e-03, 7.95460865e-03],\n",
              "       [6.88613625e-03, 4.52897092e-03, 3.83720873e-03, 1.08462365e-04,\n",
              "        3.86902131e-04, 1.81696756e-04, 1.34253164e-03, 7.00725615e-02,\n",
              "        5.99131512e-04, 6.08618029e-06, 8.41996167e-03, 2.01995921e-04,\n",
              "        2.02002645e-01, 6.52513385e-01, 5.91429044e-03, 2.73010950e-03,\n",
              "        3.50781716e-03, 5.14599355e-03, 3.30319931e-03, 4.54147696e-04,\n",
              "        2.81745056e-03, 2.82523059e-03, 1.78976227e-02, 6.05473353e-04,\n",
              "        2.93098879e-03, 7.79907859e-04],\n",
              "       [1.59726143e-04, 7.49217754e-04, 6.72708377e-02, 1.05614323e-04,\n",
              "        1.93735061e-03, 2.11682686e-06, 8.98804188e-01, 7.35607828e-05,\n",
              "        3.19637875e-05, 6.50442325e-06, 1.01922371e-04, 1.89822018e-02,\n",
              "        1.78207865e-05, 3.00593465e-03, 1.18802185e-03, 5.40029505e-05,\n",
              "        5.64334565e-04, 3.31588183e-03, 1.21329995e-05, 2.18395940e-06,\n",
              "        5.65389273e-05, 1.51988337e-04, 5.51986974e-04, 1.11666543e-06,\n",
              "        4.56153975e-06, 2.84830364e-03],\n",
              "       [5.67118230e-04, 3.69191373e-04, 2.55142478e-03, 1.06130268e-04,\n",
              "        1.72666769e-04, 2.50075982e-06, 6.16159569e-03, 3.09988540e-02,\n",
              "        9.95040591e-06, 5.76596403e-06, 7.12755660e-04, 8.11809499e-04,\n",
              "        5.10544656e-03, 3.47988047e-02, 1.27574161e-03, 1.10538451e-04,\n",
              "        7.33387750e-03, 3.81744490e-03, 3.00348899e-03, 4.86796125e-06,\n",
              "        5.65105770e-03, 1.22938808e-02, 8.82758141e-01, 2.55589752e-04,\n",
              "        6.43041392e-04, 4.78282454e-04],\n",
              "       [9.86742525e-05, 4.05284773e-05, 1.36292598e-04, 9.57477896e-05,\n",
              "        1.98122318e-04, 8.50778597e-05, 1.73889421e-05, 2.90117110e-04,\n",
              "        7.93173254e-01, 1.45528587e-02, 6.37944249e-05, 1.51988250e-04,\n",
              "        1.02393795e-03, 6.91889218e-05, 6.16325451e-06, 5.58147913e-05,\n",
              "        2.72596627e-03, 1.33036565e-05, 1.68306031e-03, 1.74386576e-01,\n",
              "        3.65258620e-06, 1.15481977e-04, 5.21118578e-04, 2.61146156e-03,\n",
              "        5.33756474e-03, 2.54279142e-03],\n",
              "       [3.94363633e-05, 9.63012781e-03, 2.66772136e-03, 4.87762578e-02,\n",
              "        1.19608857e-01, 3.25463638e-02, 1.26158893e-02, 2.58096075e-03,\n",
              "        1.62626132e-02, 1.26865897e-02, 7.72363693e-03, 4.79450123e-03,\n",
              "        4.06310568e-03, 7.09027459e-04, 1.05639547e-03, 1.00882128e-02,\n",
              "        1.50764715e-02, 1.90057550e-02, 1.40143801e-02, 1.37578761e-02,\n",
              "        1.10930495e-03, 3.40852392e-04, 2.51248642e-03, 5.53432992e-03,\n",
              "        7.85633400e-02, 5.64235508e-01],\n",
              "       [7.47162521e-01, 8.79907107e-04, 5.87515347e-03, 2.76143150e-03,\n",
              "        1.90425508e-05, 7.18215369e-06, 7.42197642e-03, 9.31694824e-03,\n",
              "        5.91569289e-04, 2.00619088e-05, 8.46487674e-05, 1.42667966e-03,\n",
              "        4.96971654e-03, 9.97972488e-03, 1.80116296e-01, 4.13595786e-04,\n",
              "        1.09231584e-02, 5.77096781e-03, 6.12270145e-04, 6.89299704e-05,\n",
              "        6.79392135e-03, 8.33831204e-04, 2.93819583e-03, 2.24699528e-04,\n",
              "        2.40762296e-04, 5.46756142e-04],\n",
              "       [2.30153291e-05, 3.59345302e-02, 6.48112316e-03, 1.21479265e-01,\n",
              "        6.66550621e-02, 1.23418875e-01, 3.43330903e-03, 5.34649822e-04,\n",
              "        2.53245793e-02, 6.84389472e-03, 1.03491329e-01, 1.36491749e-02,\n",
              "        2.91242860e-02, 3.13857687e-03, 4.58027323e-04, 2.12024331e-01,\n",
              "        3.86829511e-03, 1.21850431e-01, 8.03322531e-03, 1.91234667e-02,\n",
              "        4.33096284e-04, 3.19545041e-03, 4.19324916e-03, 9.09724459e-03,\n",
              "        4.79256213e-02, 3.02658174e-02],\n",
              "       [4.75794732e-05, 6.44813851e-02, 1.77224167e-04, 6.91848338e-01,\n",
              "        6.39950251e-03, 1.04427978e-03, 6.60010439e-04, 5.15759530e-05,\n",
              "        3.55218677e-03, 1.66425249e-04, 1.46781001e-03, 1.72948346e-01,\n",
              "        4.22051322e-04, 6.77961225e-06, 1.09268643e-03, 3.92934633e-03,\n",
              "        9.18636215e-05, 7.13064894e-03, 1.69350654e-02, 3.20388726e-03,\n",
              "        3.30895331e-04, 4.61647778e-06, 3.98996690e-06, 3.00084706e-04,\n",
              "        3.33268166e-04, 2.33700648e-02],\n",
              "       [2.38046004e-03, 1.40071549e-02, 2.46170704e-04, 8.42725858e-03,\n",
              "        3.69877778e-02, 1.02647848e-03, 1.64563360e-03, 2.32427102e-02,\n",
              "        7.38659361e-03, 1.74148299e-04, 3.77840432e-03, 9.79191996e-03,\n",
              "        1.89798828e-02, 5.43491973e-04, 1.15304952e-03, 3.52896895e-04,\n",
              "        6.99088164e-03, 1.18817273e-03, 7.75899947e-01, 8.96670297e-03,\n",
              "        2.69712135e-03, 2.37238492e-05, 7.81247043e-04, 4.87833982e-03,\n",
              "        4.60878806e-03, 6.38410598e-02],\n",
              "       [2.67450523e-04, 4.79908595e-05, 6.80417870e-05, 1.82419506e-04,\n",
              "        2.39056517e-05, 1.93626096e-04, 1.79459439e-05, 1.67871243e-03,\n",
              "        2.65428185e-04, 9.78787601e-01, 4.49671963e-04, 1.48623794e-05,\n",
              "        2.11950337e-05, 6.50601532e-06, 6.78808603e-04, 3.83955921e-06,\n",
              "        1.59719156e-03, 2.12692266e-05, 3.14798672e-03, 1.64117932e-03,\n",
              "        4.83687781e-03, 1.12309441e-04, 6.05305191e-04, 3.41413659e-03,\n",
              "        2.87539558e-04, 1.62824045e-03],\n",
              "       [5.12941834e-03, 7.85508892e-05, 1.04215369e-03, 4.46718739e-04,\n",
              "        4.08908782e-05, 7.09293599e-05, 1.58198483e-04, 2.31353030e-01,\n",
              "        3.37598394e-05, 3.75053519e-03, 4.39990428e-04, 1.51668341e-04,\n",
              "        1.12540368e-03, 8.01049027e-05, 2.35554837e-02, 5.18508314e-05,\n",
              "        1.58067252e-02, 7.40308606e-04, 1.14786578e-02, 3.93701521e-05,\n",
              "        6.94515467e-01, 1.18556118e-03, 6.99341344e-03, 8.28523422e-04,\n",
              "        4.54213354e-04, 4.49082378e-04],\n",
              "       [3.85517924e-04, 4.64328751e-03, 2.86400231e-04, 1.70716175e-05,\n",
              "        6.71927002e-04, 2.82922301e-05, 2.60776345e-04, 4.43816334e-02,\n",
              "        9.87517487e-05, 2.75649256e-07, 3.42131360e-03, 1.81702257e-04,\n",
              "        8.44832897e-01, 6.24854192e-02, 4.80141811e-04, 7.71359075e-04,\n",
              "        2.41553341e-03, 4.16087452e-04, 2.01392714e-02, 4.23657184e-04,\n",
              "        4.34613001e-04, 1.31448614e-04, 5.63951256e-03, 9.11477953e-04,\n",
              "        6.44546887e-03, 9.61364858e-05],\n",
              "       [1.62667711e-05, 4.63477224e-02, 1.14223417e-02, 1.47808902e-02,\n",
              "        2.19277386e-02, 1.23543218e-01, 1.16351039e-04, 3.13481927e-04,\n",
              "        8.45293631e-04, 2.77638494e-04, 1.41685586e-02, 4.78535146e-03,\n",
              "        1.64389587e-03, 6.10173331e-04, 5.09895675e-04, 6.87226474e-01,\n",
              "        2.55196588e-04, 4.67752479e-02, 5.02981478e-04, 5.13375388e-04,\n",
              "        2.01022369e-04, 2.04694938e-04, 6.69034353e-06, 1.58071239e-02,\n",
              "        6.68273401e-03, 5.15598280e-04],\n",
              "       [1.82505157e-02, 1.29098981e-03, 1.13754170e-02, 5.28828707e-03,\n",
              "        5.81531611e-04, 2.54489423e-05, 9.34364833e-03, 1.88770294e-02,\n",
              "        1.62458018e-04, 5.00195543e-04, 1.95833811e-04, 1.29722781e-03,\n",
              "        1.02297850e-02, 6.03863853e-04, 7.06202239e-02, 2.70362798e-04,\n",
              "        7.66372800e-01, 5.07362885e-03, 3.00654164e-03, 1.62960787e-04,\n",
              "        4.71575372e-02, 6.36458769e-03, 1.35563537e-02, 2.28685490e-03,\n",
              "        4.74337442e-03, 2.36244663e-03],\n",
              "       [1.94986090e-02, 1.12149930e-02, 3.29073076e-03, 5.62704727e-03,\n",
              "        1.17867392e-04, 1.27638807e-04, 3.30703356e-03, 9.89985168e-02,\n",
              "        6.10396091e-05, 2.99878709e-04, 5.24694799e-04, 1.92626682e-03,\n",
              "        2.59685200e-02, 8.15189071e-03, 1.96835548e-01, 1.61657541e-03,\n",
              "        3.53471972e-02, 1.39754387e-02, 6.99670520e-03, 2.21952068e-04,\n",
              "        4.97019649e-01, 2.40656529e-02, 3.89970280e-02, 1.36193633e-03,\n",
              "        2.87405821e-03, 1.57356216e-03],\n",
              "       [1.11804206e-06, 2.01031216e-04, 2.30032292e-05, 7.28629704e-04,\n",
              "        4.79575945e-04, 1.81790383e-04, 1.82069368e-06, 1.80339284e-05,\n",
              "        8.66638899e-01, 1.92222171e-04, 5.08482684e-04, 6.84390892e-04,\n",
              "        1.45624590e-03, 8.41011934e-06, 9.59950626e-07, 3.88315937e-04,\n",
              "        3.44407672e-05, 5.15662978e-05, 2.27772049e-03, 1.17322303e-01,\n",
              "        3.37514024e-07, 6.45432783e-06, 2.10971139e-05, 4.59551491e-04,\n",
              "        2.22053425e-03, 6.09302288e-03],\n",
              "       [4.72837547e-03, 1.10230409e-03, 7.72970140e-01, 1.31681969e-03,\n",
              "        6.18566852e-03, 1.04765175e-03, 1.36869326e-01, 5.53517928e-03,\n",
              "        6.81748323e-04, 4.82083182e-04, 8.77247076e-04, 1.75778810e-02,\n",
              "        7.31067790e-04, 1.42060705e-02, 7.37901079e-03, 3.15636722e-03,\n",
              "        7.73979910e-03, 4.15663188e-03, 4.93877567e-04, 1.49989850e-04,\n",
              "        3.05199553e-03, 4.24409891e-03, 3.30697675e-03, 1.79237206e-04,\n",
              "        7.24703714e-04, 1.10575592e-03],\n",
              "       [2.47602147e-04, 2.29374971e-02, 2.48405710e-03, 7.18594820e-04,\n",
              "        1.94970481e-02, 1.07572442e-02, 2.85407179e-04, 3.97913158e-03,\n",
              "        2.38352269e-03, 7.66489393e-05, 8.53320420e-01, 4.66544693e-03,\n",
              "        7.71347200e-03, 1.08787036e-02, 1.57778486e-04, 6.64754491e-03,\n",
              "        3.65224143e-04, 2.02944651e-02, 7.69476593e-03, 2.14557350e-03,\n",
              "        1.62826822e-04, 4.07094398e-04, 2.33706669e-04, 1.37882605e-02,\n",
              "        1.81359367e-03, 6.34435099e-03],\n",
              "       [1.87242895e-05, 2.51791328e-02, 1.41220272e-03, 1.11697596e-02,\n",
              "        5.00943847e-02, 8.84399051e-05, 5.80410659e-03, 9.35566568e-05,\n",
              "        1.75903217e-04, 2.01569242e-06, 1.98539603e-03, 8.76964569e-01,\n",
              "        5.10001300e-05, 1.44192629e-04, 2.16054526e-04, 6.28648442e-04,\n",
              "        1.71739885e-05, 1.39663285e-02, 4.65900963e-03, 4.01629623e-05,\n",
              "        8.67460330e-05, 4.03338618e-06, 3.22666419e-05, 3.34597498e-05,\n",
              "        1.39259419e-05, 7.11884769e-03],\n",
              "       [1.65186048e-05, 1.80639478e-03, 4.57714923e-04, 4.59512929e-04,\n",
              "        2.94774654e-03, 5.29960496e-03, 1.93780768e-04, 7.70060578e-04,\n",
              "        1.70065614e-03, 1.89022068e-03, 1.20693417e-02, 1.26320156e-04,\n",
              "        8.99276044e-03, 1.03412103e-03, 2.77153194e-05, 1.47865224e-03,\n",
              "        6.73942687e-03, 1.34221301e-03, 3.93562298e-03, 4.90011349e-02,\n",
              "        1.19591350e-04, 5.39961737e-04, 1.05995371e-03, 4.65253517e-02,\n",
              "        8.48123610e-01, 3.34199984e-03]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Predict how the message will appear via Keras Sequential model.\n",
        "predicted_message = model.predict(MESSAGE_2D)\n",
        "predicted_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4oXJz4r4mp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beff4190-5290-4c98-be73-8f8d9cd2c30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JNEFIVEBOYINGWIZAPDSJUMPQUICKLY\n"
          ]
        }
      ],
      "source": [
        "message_list = \"\"\n",
        "for num in range(len(predicted_message)):\n",
        "    max_val = 0\n",
        "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    message_list += predict\n",
        "print(message_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YdutdB8SX6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef69d0c-7518-4144-f1c0-f794f1b1581c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy between predicted and actual message: 87.09677419354838%\n",
            "Here is the list of letters that are a mismatch between predicted and actual message:\n",
            "Actual    Predicted\n",
            "T         J\n",
            "H         N\n",
            "X         Y\n",
            "R         P\n"
          ]
        }
      ],
      "source": [
        "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
        "list_mismatch = []\n",
        "num_total, num_correct = len(actual_message), len(message_list)\n",
        "for i in range(len(actual_message)):\n",
        "    if message_list[i] != actual_message[i]:\n",
        "        num_correct -= 1\n",
        "        list_mismatch.append([actual_message[i], message_list[i]])\n",
        "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_mismatch) == 0:\n",
        "    print(\"The message appears to be decoded correctly.\")\n",
        "else:\n",
        "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_mismatch)):\n",
        "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGOHSW6sgZlw"
      },
      "source": [
        "# 6. How does this model compare with the performance of your perceptron models in Project 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afSkHPK3on6v"
      },
      "source": [
        "## A: The evaluation accuracy of the Keras Sequential model for TEST_SET is 50%, which is lower than the ~70% evaluation accuracy of the Project 1 perceptron models for TEST_SET. However, the prediction accuracy of the model on MESSAGE is 87%, which is higher than the 76-77% prediction accuracy for MESSAGE of the perceptron models in Project 1. \n",
        "\n",
        "## To ensure that the large discrepancy between TEST_SET and MESSAGE accuracy was not due to coding or model errors, we swapped one set of letters in TRAINING_SET with the set of letters in TEST_SET and retrained the model on the swapped sets, then evaluated its accuracy and ran it again on MESSAGE. We obtained results similar to the ones for the unswapped sets, indicating that there is likely no issue with our model or code. \n",
        "\n",
        "## We therefore propose that the discrepancy between TEST_SET and MESSAGE accuracy for our Keras sequential model is due to the fact that the Keras sequential model performed better than the Project 1 perceptron model on TRAINING_SET (100% for the Keras model vs. 96% for the Project 1 perceptron), which makes up a majority of the letters in MESSAGE. Because only a minority of letters in MESSAGE come from TEST_SET, the Keras model's decreased accuracy on TEST_SET did not have much of an effect on its accuracy for MESSAGE.\n",
        "\n",
        "## Note: Although TEST_SET was only meant to be used with model.evaluate, we decided to also run model.predict on it to see if our results were consistent, due to the low accuracy we obtained with model.evaluate. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EThYHOgrgbVF"
      },
      "source": [
        "# 7. All of the letters in MESSAGE were likely not decoded correctly, so letâ€™s try to improve the performance of the model by adding additional hidden layers. Add two additional hidden layers of the same size as your original hidden layer, then repeat experiments (4) and (5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CeNdBjX4E2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f2227c-1352-4c59-be3a-9084f812f265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31, 256)\n",
            "  ##############\n",
            "  ############# \n",
            "  ##    ##    ##\n",
            "  ##    ##    ##\n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "   #    ##    # \n",
            "        ##      \n",
            "        ##      \n",
            "      ######    \n",
            "      ######    \n",
            "  #             \n",
            "                \n",
            "[[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "print(MESSAGE_2D.shape)\n",
        "show(MESSAGE_2D[0])\n",
        "print(MESSAGE)\n",
        "message_list=['']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATyBdSQfqf5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04b782a-00b9-4e68-9ddb-5d33bf8cf6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Hidden_layer1 (Dense)       (None, 32)                8224      \n",
            "                                                                 \n",
            " Hidden_layer2 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer3 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Output_layer (Dense)        (None, 26)                858       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,194\n",
            "Trainable params: 11,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Do experiments 4 and 5 again, but this time, add two more hidden layers.\n",
        "model2 = tf.keras.models.Sequential()\n",
        "model2.add(tf.keras.Input(shape=(256,)))\n",
        "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer1\"))\n",
        "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer2\"))\n",
        "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer3\"))\n",
        "model2.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
        "\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhz_tgqGqpZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b2c0a2-bec9-46aa-dd41-98e3e79837e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9999\n",
            "2/2 - 0s - loss: 3.2773 - categorical_accuracy: 0.0385 - 423ms/epoch - 211ms/step\n",
            "Epoch 2/9999\n",
            "2/2 - 0s - loss: 3.2457 - categorical_accuracy: 0.0577 - 15ms/epoch - 8ms/step\n",
            "Epoch 3/9999\n",
            "2/2 - 0s - loss: 3.2202 - categorical_accuracy: 0.1346 - 8ms/epoch - 4ms/step\n",
            "Epoch 4/9999\n",
            "2/2 - 0s - loss: 3.1997 - categorical_accuracy: 0.1154 - 8ms/epoch - 4ms/step\n",
            "Epoch 5/9999\n",
            "2/2 - 0s - loss: 3.1795 - categorical_accuracy: 0.1538 - 7ms/epoch - 3ms/step\n",
            "Epoch 6/9999\n",
            "2/2 - 0s - loss: 3.1603 - categorical_accuracy: 0.2115 - 13ms/epoch - 6ms/step\n",
            "Epoch 7/9999\n",
            "2/2 - 0s - loss: 3.1441 - categorical_accuracy: 0.2308 - 8ms/epoch - 4ms/step\n",
            "Epoch 8/9999\n",
            "2/2 - 0s - loss: 3.1242 - categorical_accuracy: 0.2115 - 8ms/epoch - 4ms/step\n",
            "Epoch 9/9999\n",
            "2/2 - 0s - loss: 3.1028 - categorical_accuracy: 0.2500 - 8ms/epoch - 4ms/step\n",
            "Epoch 10/9999\n",
            "2/2 - 0s - loss: 3.0810 - categorical_accuracy: 0.2500 - 7ms/epoch - 4ms/step\n",
            "Epoch 11/9999\n",
            "2/2 - 0s - loss: 3.0560 - categorical_accuracy: 0.2692 - 10ms/epoch - 5ms/step\n",
            "Epoch 12/9999\n",
            "2/2 - 0s - loss: 3.0313 - categorical_accuracy: 0.2692 - 8ms/epoch - 4ms/step\n",
            "Epoch 13/9999\n",
            "2/2 - 0s - loss: 3.0007 - categorical_accuracy: 0.3269 - 8ms/epoch - 4ms/step\n",
            "Epoch 14/9999\n",
            "2/2 - 0s - loss: 2.9723 - categorical_accuracy: 0.2885 - 8ms/epoch - 4ms/step\n",
            "Epoch 15/9999\n",
            "2/2 - 0s - loss: 2.9399 - categorical_accuracy: 0.2692 - 11ms/epoch - 6ms/step\n",
            "Epoch 16/9999\n",
            "2/2 - 0s - loss: 2.9048 - categorical_accuracy: 0.3077 - 7ms/epoch - 4ms/step\n",
            "Epoch 17/9999\n",
            "2/2 - 0s - loss: 2.8683 - categorical_accuracy: 0.3269 - 11ms/epoch - 6ms/step\n",
            "Epoch 18/9999\n",
            "2/2 - 0s - loss: 2.8286 - categorical_accuracy: 0.3269 - 8ms/epoch - 4ms/step\n",
            "Epoch 19/9999\n",
            "2/2 - 0s - loss: 2.7858 - categorical_accuracy: 0.3269 - 9ms/epoch - 5ms/step\n",
            "Epoch 20/9999\n",
            "2/2 - 0s - loss: 2.7416 - categorical_accuracy: 0.3077 - 8ms/epoch - 4ms/step\n",
            "Epoch 21/9999\n",
            "2/2 - 0s - loss: 2.6973 - categorical_accuracy: 0.2885 - 9ms/epoch - 5ms/step\n",
            "Epoch 22/9999\n",
            "2/2 - 0s - loss: 2.6494 - categorical_accuracy: 0.3077 - 10ms/epoch - 5ms/step\n",
            "Epoch 23/9999\n",
            "2/2 - 0s - loss: 2.5962 - categorical_accuracy: 0.3269 - 7ms/epoch - 4ms/step\n",
            "Epoch 24/9999\n",
            "2/2 - 0s - loss: 2.5473 - categorical_accuracy: 0.3269 - 12ms/epoch - 6ms/step\n",
            "Epoch 25/9999\n",
            "2/2 - 0s - loss: 2.4918 - categorical_accuracy: 0.3269 - 10ms/epoch - 5ms/step\n",
            "Epoch 26/9999\n",
            "2/2 - 0s - loss: 2.4341 - categorical_accuracy: 0.3462 - 8ms/epoch - 4ms/step\n",
            "Epoch 27/9999\n",
            "2/2 - 0s - loss: 2.3781 - categorical_accuracy: 0.3462 - 9ms/epoch - 5ms/step\n",
            "Epoch 28/9999\n",
            "2/2 - 0s - loss: 2.3165 - categorical_accuracy: 0.4038 - 8ms/epoch - 4ms/step\n",
            "Epoch 29/9999\n",
            "2/2 - 0s - loss: 2.2551 - categorical_accuracy: 0.4231 - 12ms/epoch - 6ms/step\n",
            "Epoch 30/9999\n",
            "2/2 - 0s - loss: 2.1941 - categorical_accuracy: 0.4615 - 7ms/epoch - 4ms/step\n",
            "Epoch 31/9999\n",
            "2/2 - 0s - loss: 2.1276 - categorical_accuracy: 0.4615 - 9ms/epoch - 4ms/step\n",
            "Epoch 32/9999\n",
            "2/2 - 0s - loss: 2.0627 - categorical_accuracy: 0.4808 - 11ms/epoch - 5ms/step\n",
            "Epoch 33/9999\n",
            "2/2 - 0s - loss: 1.9985 - categorical_accuracy: 0.5000 - 10ms/epoch - 5ms/step\n",
            "Epoch 34/9999\n",
            "2/2 - 0s - loss: 1.9320 - categorical_accuracy: 0.5385 - 9ms/epoch - 4ms/step\n",
            "Epoch 35/9999\n",
            "2/2 - 0s - loss: 1.8637 - categorical_accuracy: 0.5385 - 8ms/epoch - 4ms/step\n",
            "Epoch 36/9999\n",
            "2/2 - 0s - loss: 1.7961 - categorical_accuracy: 0.5577 - 10ms/epoch - 5ms/step\n",
            "Epoch 37/9999\n",
            "2/2 - 0s - loss: 1.7345 - categorical_accuracy: 0.5385 - 9ms/epoch - 4ms/step\n",
            "Epoch 38/9999\n",
            "2/2 - 0s - loss: 1.6696 - categorical_accuracy: 0.5385 - 14ms/epoch - 7ms/step\n",
            "Epoch 39/9999\n",
            "2/2 - 0s - loss: 1.6067 - categorical_accuracy: 0.6538 - 9ms/epoch - 4ms/step\n",
            "Epoch 40/9999\n",
            "2/2 - 0s - loss: 1.5418 - categorical_accuracy: 0.6538 - 7ms/epoch - 4ms/step\n",
            "Epoch 41/9999\n",
            "2/2 - 0s - loss: 1.4773 - categorical_accuracy: 0.7115 - 7ms/epoch - 4ms/step\n",
            "Epoch 42/9999\n",
            "2/2 - 0s - loss: 1.4159 - categorical_accuracy: 0.7115 - 7ms/epoch - 4ms/step\n",
            "Epoch 43/9999\n",
            "2/2 - 0s - loss: 1.3553 - categorical_accuracy: 0.7308 - 7ms/epoch - 4ms/step\n",
            "Epoch 44/9999\n",
            "2/2 - 0s - loss: 1.2935 - categorical_accuracy: 0.8077 - 9ms/epoch - 5ms/step\n",
            "Epoch 45/9999\n",
            "2/2 - 0s - loss: 1.2326 - categorical_accuracy: 0.8077 - 8ms/epoch - 4ms/step\n",
            "Epoch 46/9999\n",
            "2/2 - 0s - loss: 1.1724 - categorical_accuracy: 0.8269 - 7ms/epoch - 4ms/step\n",
            "Epoch 47/9999\n",
            "2/2 - 0s - loss: 1.1164 - categorical_accuracy: 0.8654 - 8ms/epoch - 4ms/step\n",
            "Epoch 48/9999\n",
            "2/2 - 0s - loss: 1.0612 - categorical_accuracy: 0.8846 - 7ms/epoch - 4ms/step\n",
            "Epoch 49/9999\n",
            "2/2 - 0s - loss: 1.0085 - categorical_accuracy: 0.8846 - 8ms/epoch - 4ms/step\n",
            "Epoch 50/9999\n",
            "2/2 - 0s - loss: 0.9547 - categorical_accuracy: 0.8846 - 11ms/epoch - 6ms/step\n",
            "Epoch 51/9999\n",
            "2/2 - 0s - loss: 0.9037 - categorical_accuracy: 0.9423 - 10ms/epoch - 5ms/step\n",
            "Epoch 52/9999\n",
            "2/2 - 0s - loss: 0.8565 - categorical_accuracy: 0.9423 - 12ms/epoch - 6ms/step\n",
            "Epoch 53/9999\n",
            "2/2 - 0s - loss: 0.8098 - categorical_accuracy: 0.9615 - 8ms/epoch - 4ms/step\n",
            "Epoch 54/9999\n",
            "2/2 - 0s - loss: 0.7693 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 55/9999\n",
            "2/2 - 0s - loss: 0.7263 - categorical_accuracy: 0.9808 - 9ms/epoch - 4ms/step\n",
            "Epoch 56/9999\n",
            "2/2 - 0s - loss: 0.6870 - categorical_accuracy: 0.9615 - 9ms/epoch - 5ms/step\n",
            "Epoch 57/9999\n",
            "2/2 - 0s - loss: 0.6492 - categorical_accuracy: 0.9615 - 7ms/epoch - 3ms/step\n",
            "Epoch 58/9999\n",
            "2/2 - 0s - loss: 0.6118 - categorical_accuracy: 0.9615 - 8ms/epoch - 4ms/step\n",
            "Epoch 59/9999\n",
            "2/2 - 0s - loss: 0.5781 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 60/9999\n",
            "2/2 - 0s - loss: 0.5468 - categorical_accuracy: 0.9615 - 9ms/epoch - 5ms/step\n",
            "Epoch 61/9999\n",
            "2/2 - 0s - loss: 0.5183 - categorical_accuracy: 0.9615 - 8ms/epoch - 4ms/step\n",
            "Epoch 62/9999\n",
            "2/2 - 0s - loss: 0.4925 - categorical_accuracy: 0.9615 - 7ms/epoch - 4ms/step\n",
            "Epoch 63/9999\n",
            "2/2 - 0s - loss: 0.4636 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 64/9999\n",
            "2/2 - 0s - loss: 0.4409 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 65/9999\n",
            "2/2 - 0s - loss: 0.4172 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 66/9999\n",
            "2/2 - 0s - loss: 0.3983 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 67/9999\n",
            "2/2 - 0s - loss: 0.3760 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 68/9999\n",
            "2/2 - 0s - loss: 0.3569 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 69/9999\n",
            "2/2 - 0s - loss: 0.3389 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 70/9999\n",
            "2/2 - 0s - loss: 0.3211 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 71/9999\n",
            "2/2 - 0s - loss: 0.3073 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 72/9999\n",
            "2/2 - 0s - loss: 0.2929 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 73/9999\n",
            "2/2 - 0s - loss: 0.2787 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 74/9999\n",
            "2/2 - 0s - loss: 0.2650 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 75/9999\n",
            "2/2 - 0s - loss: 0.2541 - categorical_accuracy: 1.0000 - 13ms/epoch - 6ms/step\n",
            "Epoch 76/9999\n",
            "2/2 - 0s - loss: 0.2433 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 77/9999\n",
            "2/2 - 0s - loss: 0.2310 - categorical_accuracy: 1.0000 - 7ms/epoch - 3ms/step\n",
            "Epoch 78/9999\n",
            "2/2 - 0s - loss: 0.2211 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 79/9999\n",
            "2/2 - 0s - loss: 0.2121 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 80/9999\n",
            "2/2 - 0s - loss: 0.2032 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 81/9999\n",
            "2/2 - 0s - loss: 0.1962 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 82/9999\n",
            "2/2 - 0s - loss: 0.1878 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 83/9999\n",
            "2/2 - 0s - loss: 0.1787 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 84/9999\n",
            "2/2 - 0s - loss: 0.1715 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 85/9999\n",
            "2/2 - 0s - loss: 0.1648 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 86/9999\n",
            "2/2 - 0s - loss: 0.1601 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 87/9999\n",
            "2/2 - 0s - loss: 0.1516 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 88/9999\n",
            "2/2 - 0s - loss: 0.1463 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 89/9999\n",
            "2/2 - 0s - loss: 0.1415 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 90/9999\n",
            "2/2 - 0s - loss: 0.1357 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 91/9999\n",
            "2/2 - 0s - loss: 0.1310 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 92/9999\n",
            "2/2 - 0s - loss: 0.1269 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 93/9999\n",
            "2/2 - 0s - loss: 0.1227 - categorical_accuracy: 1.0000 - 6ms/epoch - 3ms/step\n",
            "Epoch 94/9999\n",
            "2/2 - 0s - loss: 0.1182 - categorical_accuracy: 1.0000 - 7ms/epoch - 3ms/step\n",
            "Epoch 95/9999\n",
            "2/2 - 0s - loss: 0.1144 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 96/9999\n",
            "2/2 - 0s - loss: 0.1106 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 97/9999\n",
            "2/2 - 0s - loss: 0.1069 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 98/9999\n",
            "2/2 - 0s - loss: 0.1038 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 99/9999\n",
            "2/2 - 0s - loss: 0.0994 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 100/9999\n",
            "2/2 - 0s - loss: 0.0968 - categorical_accuracy: 1.0000 - 6ms/epoch - 3ms/step\n",
            "Epoch 101/9999\n",
            "2/2 - 0s - loss: 0.0943 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 102/9999\n",
            "2/2 - 0s - loss: 0.0920 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 103/9999\n",
            "2/2 - 0s - loss: 0.0890 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 104/9999\n",
            "2/2 - 0s - loss: 0.0854 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 105/9999\n",
            "2/2 - 0s - loss: 0.0829 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 106/9999\n",
            "2/2 - 0s - loss: 0.0804 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 107/9999\n",
            "2/2 - 0s - loss: 0.0786 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 108/9999\n",
            "2/2 - 0s - loss: 0.0765 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 109/9999\n",
            "2/2 - 0s - loss: 0.0734 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 110/9999\n",
            "2/2 - 0s - loss: 0.0710 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 111/9999\n",
            "2/2 - 0s - loss: 0.0693 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 112/9999\n",
            "2/2 - 0s - loss: 0.0673 - categorical_accuracy: 1.0000 - 7ms/epoch - 3ms/step\n",
            "Epoch 113/9999\n",
            "2/2 - 0s - loss: 0.0662 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 114/9999\n",
            "2/2 - 0s - loss: 0.0639 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 115/9999\n",
            "2/2 - 0s - loss: 0.0623 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 116/9999\n",
            "2/2 - 0s - loss: 0.0607 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6fd2e3650>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "             metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
        "model2.fit(x=TRAINING_SET_2D, y=unicode_cat_train, epochs=9999, verbose=2, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hZ322rMqwXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c538ec4-c1ca-43aa-95d4-5159389dfe00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 123ms/step - loss: 2.4804 - categorical_accuracy: 0.3077\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.480441093444824, 0.3076923191547394]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model2.evaluate(x=TEST_SET_2D, y=unicode_cat_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_ZZkjrOz-tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d822a3a1-efa5-4dbd-abcd-c3d4c08207b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.82913095e-02, 1.68427125e-01, 1.74321376e-05, 4.22803700e-01,\n",
              "        3.48201167e-04, 3.15022908e-06, 1.96054634e-02, 9.47539320e-06,\n",
              "        1.58396381e-06, 5.25133172e-03, 9.22899395e-02, 1.76730286e-02,\n",
              "        1.63285695e-05, 5.67880961e-05, 1.80735125e-03, 2.03774645e-04,\n",
              "        7.19323056e-04, 1.92173272e-01, 5.04782945e-02, 2.45397352e-03,\n",
              "        2.30785296e-03, 1.75157184e-04, 4.37747501e-03, 3.50862538e-04,\n",
              "        1.04195598e-04, 5.35887666e-05],\n",
              "       [2.81373541e-05, 5.91632366e-01, 3.12450551e-03, 8.81156418e-03,\n",
              "        1.14239263e-03, 4.66961268e-04, 1.07252020e-02, 2.10545477e-06,\n",
              "        3.52582720e-04, 2.53889966e-03, 1.94138605e-02, 8.93260632e-03,\n",
              "        1.35413256e-05, 1.90700416e-06, 5.43442256e-05, 2.58376330e-01,\n",
              "        1.02746271e-05, 3.67104053e-03, 5.51310144e-02, 3.29028592e-02,\n",
              "        1.90439896e-05, 2.70125292e-05, 1.46289076e-05, 3.16921910e-06,\n",
              "        8.56798506e-05, 2.51801638e-03],\n",
              "       [1.40115895e-04, 4.01023030e-02, 6.77361786e-01, 1.49498504e-04,\n",
              "        6.04680739e-03, 8.56324658e-03, 5.84941730e-02, 1.40761316e-04,\n",
              "        7.33520719e-04, 1.57742202e-02, 7.34000001e-04, 6.15078461e-05,\n",
              "        1.48026593e-04, 1.07883148e-04, 1.40094652e-03, 6.44892827e-02,\n",
              "        3.58102943e-06, 2.13307765e-04, 6.06712550e-02, 1.36764807e-04,\n",
              "        7.96971784e-04, 2.18365276e-05, 2.40164227e-05, 7.50958588e-06,\n",
              "        2.41317815e-04, 6.34353608e-02],\n",
              "       [1.85904628e-05, 4.16278213e-01, 1.53869810e-03, 1.13815144e-01,\n",
              "        1.67685721e-04, 4.05198138e-04, 2.57530529e-03, 6.84904853e-06,\n",
              "        6.73423056e-04, 2.87422235e-03, 2.49081384e-03, 1.35096293e-02,\n",
              "        2.47223506e-05, 4.08950427e-06, 2.10591767e-04, 3.44965577e-01,\n",
              "        2.07788456e-04, 1.66666252e-03, 1.68616958e-02, 7.94442445e-02,\n",
              "        4.71607782e-05, 2.00188137e-03, 1.23443815e-05, 4.76962896e-06,\n",
              "        3.06097099e-05, 1.63988618e-04],\n",
              "       [1.38155683e-05, 1.25757858e-01, 2.08869646e-03, 2.52519058e-05,\n",
              "        9.00158882e-02, 2.08795562e-01, 1.18367933e-03, 1.58523671e-05,\n",
              "        1.04740774e-03, 8.21062922e-03, 2.11164936e-01, 3.05882352e-03,\n",
              "        1.66354603e-05, 6.86092972e-06, 6.54739779e-07, 2.77064852e-02,\n",
              "        1.82243255e-07, 3.59467347e-03, 1.87573895e-01, 2.81803450e-03,\n",
              "        3.74672663e-06, 2.83279622e-08, 1.61521893e-05, 1.54100219e-03,\n",
              "        9.87589918e-03, 1.15467347e-01],\n",
              "       [7.91520947e-08, 1.62574220e-02, 4.11149813e-04, 1.70178424e-07,\n",
              "        2.28048512e-03, 8.34798336e-01, 2.61268324e-05, 2.26137263e-06,\n",
              "        7.22136721e-03, 2.83817993e-04, 6.32620184e-03, 8.75017940e-05,\n",
              "        5.98044039e-07, 1.46243138e-07, 1.31385480e-09, 1.12479534e-02,\n",
              "        1.05599645e-08, 2.17249271e-05, 1.06831104e-01, 6.97513763e-03,\n",
              "        1.39130441e-08, 1.55879629e-08, 2.26776848e-07, 2.14395463e-04,\n",
              "        5.03619248e-03, 1.97762088e-03],\n",
              "       [1.07418373e-02, 1.36894315e-01, 9.25279967e-03, 4.68559051e-03,\n",
              "        9.29514226e-03, 3.52634561e-05, 6.90622628e-01, 1.36809076e-05,\n",
              "        5.38307941e-05, 6.88231923e-03, 2.98325848e-02, 1.21654291e-03,\n",
              "        8.30333647e-06, 2.86991890e-05, 5.98955341e-03, 2.19349982e-03,\n",
              "        2.27525179e-05, 1.84418317e-02, 5.82131445e-02, 3.20986059e-04,\n",
              "        3.94022558e-03, 6.12096665e-06, 4.62075113e-04, 3.40841216e-05,\n",
              "        1.30148401e-04, 1.06818667e-02],\n",
              "       [3.58080288e-04, 1.12353754e-03, 1.72886348e-06, 6.54629106e-03,\n",
              "        3.04500200e-03, 5.95476959e-05, 8.72757155e-05, 1.60451455e-03,\n",
              "        1.85050091e-08, 8.39113025e-04, 2.70647332e-02, 3.12826522e-02,\n",
              "        2.33156625e-02, 1.51065283e-03, 6.77489370e-05, 2.83789250e-05,\n",
              "        2.78825464e-05, 8.23435485e-01, 6.26504188e-03, 1.43364596e-04,\n",
              "        7.66561716e-04, 1.00800889e-05, 2.61336472e-02, 4.59291749e-02,\n",
              "        1.93636282e-04, 1.60198702e-04],\n",
              "       [4.20387732e-05, 1.30421458e-05, 2.18375790e-04, 5.05704179e-07,\n",
              "        1.27437303e-07, 7.77348905e-05, 1.11430236e-05, 1.37191569e-06,\n",
              "        9.88869548e-01, 1.05597464e-05, 2.33010160e-05, 1.04525543e-06,\n",
              "        3.31479697e-07, 4.55687950e-06, 9.36264996e-07, 4.21167861e-05,\n",
              "        2.21555092e-04, 6.04510262e-07, 2.72395415e-03, 7.62213999e-03,\n",
              "        1.20935204e-06, 2.21880182e-05, 1.06743676e-06, 1.38839397e-07,\n",
              "        6.92007161e-05, 2.12021623e-05],\n",
              "       [1.03244314e-03, 1.57187935e-02, 1.95995852e-01, 4.55037877e-03,\n",
              "        9.11245239e-04, 4.14842973e-03, 2.38762125e-01, 6.06644608e-04,\n",
              "        1.17845088e-03, 3.71796370e-01, 2.06625933e-04, 1.41797073e-05,\n",
              "        9.10892442e-04, 2.22240225e-03, 1.08669534e-01, 4.88229794e-03,\n",
              "        2.28850418e-04, 2.28560343e-03, 1.06399404e-02, 7.10928289e-04,\n",
              "        2.50037685e-02, 1.33811511e-04, 1.41440993e-04, 6.51977389e-05,\n",
              "        2.41611648e-04, 8.94213002e-03],\n",
              "       [2.74639722e-04, 2.92661996e-03, 6.27863255e-06, 8.68209463e-05,\n",
              "        1.46428717e-03, 1.77515838e-02, 3.38614627e-05, 5.63939693e-06,\n",
              "        8.34119623e-04, 2.92156963e-03, 8.41500759e-01, 1.83789171e-02,\n",
              "        8.89299918e-06, 3.97921176e-05, 1.18188623e-07, 2.27371673e-03,\n",
              "        5.83520341e-06, 2.24464061e-03, 6.03392534e-02, 1.78851467e-03,\n",
              "        1.61125809e-06, 1.38515918e-06, 8.11121572e-05, 1.86848994e-02,\n",
              "        2.77130473e-02, 6.31967210e-04],\n",
              "       [2.36831966e-05, 2.87899107e-01, 2.64914503e-04, 1.11905178e-02,\n",
              "        9.40291956e-03, 3.11024138e-03, 1.39423262e-03, 1.50186515e-05,\n",
              "        1.32049681e-04, 3.08403675e-03, 1.06740803e-01, 2.69721568e-01,\n",
              "        4.82782525e-05, 3.65394112e-06, 1.55444013e-05, 2.44763196e-01,\n",
              "        1.12850676e-05, 1.97183695e-02, 1.87623259e-02, 1.92569867e-02,\n",
              "        2.54339138e-05, 6.74878684e-05, 1.11365080e-04, 3.45560460e-04,\n",
              "        4.95567336e-04, 3.39591224e-03],\n",
              "       [4.83170646e-04, 1.58150122e-02, 3.09731811e-04, 1.48225314e-04,\n",
              "        1.12788826e-01, 1.16920389e-01, 1.57353003e-04, 3.63077794e-04,\n",
              "        2.03474818e-04, 8.33151210e-03, 3.38198751e-01, 1.45351663e-02,\n",
              "        8.77526531e-04, 3.70203867e-04, 1.55203145e-06, 1.45893311e-02,\n",
              "        3.85021258e-06, 4.28459188e-03, 8.85044038e-02, 5.75822662e-04,\n",
              "        7.34119894e-05, 6.75342289e-06, 5.87772462e-04, 1.39366627e-01,\n",
              "        9.60813165e-02, 4.64221761e-02],\n",
              "       [8.54069833e-03, 3.13733774e-03, 1.29377604e-05, 2.35959776e-02,\n",
              "        1.33289874e-03, 2.94212114e-05, 1.62875059e-03, 1.11268274e-03,\n",
              "        2.23661516e-07, 4.63280827e-03, 4.99863401e-02, 7.21758418e-03,\n",
              "        2.18340736e-02, 1.09316930e-02, 1.44259469e-03, 2.40854588e-05,\n",
              "        4.07830608e-04, 6.82498515e-01, 3.82797867e-02, 6.10956573e-04,\n",
              "        6.31175702e-03, 2.59233020e-05, 1.14593916e-01, 2.09014099e-02,\n",
              "        5.30357880e-04, 3.79502628e-04],\n",
              "       [6.36271900e-03, 1.36283398e-01, 6.84324503e-02, 1.35742113e-01,\n",
              "        7.78496731e-04, 9.88309112e-05, 2.71557301e-01, 4.59251314e-04,\n",
              "        1.07056787e-03, 1.98762026e-02, 6.44282089e-04, 1.27274462e-03,\n",
              "        9.93685680e-05, 3.45166860e-04, 2.50155091e-01, 3.01062111e-02,\n",
              "        3.64614744e-03, 3.03761591e-03, 2.03589313e-02, 3.10931448e-03,\n",
              "        3.40404250e-02, 1.06722396e-02, 3.70084541e-04, 2.26917964e-05,\n",
              "        4.41221746e-05, 1.41419226e-03],\n",
              "       [5.24924781e-06, 1.45727605e-01, 1.41566172e-02, 3.97196854e-04,\n",
              "        5.16317086e-04, 2.05486864e-02, 1.92018738e-03, 8.30995668e-06,\n",
              "        3.49873193e-02, 1.35365222e-03, 4.90770955e-03, 7.52124761e-04,\n",
              "        3.45509325e-05, 4.29827423e-06, 6.33078071e-06, 4.44004267e-01,\n",
              "        1.40017846e-05, 3.22910753e-04, 1.42211333e-01, 1.86279565e-01,\n",
              "        2.29807779e-06, 4.41415286e-05, 4.22341918e-06, 9.37946243e-06,\n",
              "        2.94997997e-04, 1.48668268e-03],\n",
              "       [1.65453739e-02, 7.88786337e-02, 2.30353617e-04, 5.45274079e-01,\n",
              "        1.47515548e-05, 4.13711973e-07, 1.55075386e-01, 6.77425396e-06,\n",
              "        4.40860204e-05, 1.97680015e-03, 6.02917571e-04, 5.26597469e-05,\n",
              "        2.02072606e-05, 1.46532024e-04, 9.84112322e-02, 4.09233508e-05,\n",
              "        5.00845313e-02, 1.63016971e-02, 1.72726717e-02, 1.35440715e-02,\n",
              "        2.63158022e-03, 1.33668550e-03, 1.49156374e-03, 9.55052383e-06,\n",
              "        3.46496290e-06, 3.04449509e-06],\n",
              "       [5.02759467e-05, 1.89489231e-01, 3.05312662e-03, 9.37012397e-03,\n",
              "        7.37648515e-04, 6.92792516e-03, 4.82857181e-03, 3.15499938e-06,\n",
              "        1.36907222e-02, 4.35688999e-03, 5.88098951e-02, 1.70712918e-03,\n",
              "        6.42188534e-05, 1.31867782e-05, 3.80187412e-05, 8.21038038e-02,\n",
              "        6.53794996e-05, 1.93513371e-02, 3.94144684e-01, 2.10790589e-01,\n",
              "        3.67275470e-06, 1.29531618e-05, 1.39545900e-05, 1.01658945e-04,\n",
              "        1.13406153e-04, 1.58491661e-04],\n",
              "       [3.13846685e-04, 2.54370689e-01, 2.35308617e-01, 2.39977916e-03,\n",
              "        9.72466357e-03, 1.63116353e-03, 1.53168917e-01, 9.02335523e-05,\n",
              "        2.51656282e-04, 9.95406788e-03, 2.95172515e-03, 3.84814368e-04,\n",
              "        2.99442618e-04, 5.44156646e-05, 3.94883752e-03, 1.54604390e-01,\n",
              "        1.25451779e-05, 4.31684125e-03, 1.51649788e-01, 1.22181326e-03,\n",
              "        1.08693587e-03, 8.45353206e-05, 5.47002710e-05, 8.50372453e-06,\n",
              "        7.31014807e-05, 1.20339114e-02],\n",
              "       [3.26290983e-03, 5.39815314e-02, 1.50568392e-02, 1.37577835e-03,\n",
              "        1.41382462e-03, 1.84053276e-02, 2.39733700e-02, 1.47261671e-04,\n",
              "        2.54520208e-01, 5.31960726e-02, 1.12316459e-02, 1.33193578e-04,\n",
              "        4.25390608e-04, 4.33930662e-03, 1.21178234e-03, 2.81603448e-03,\n",
              "        4.16742731e-03, 3.06964037e-03, 4.00542587e-01, 1.27994716e-01,\n",
              "        1.00204896e-03, 4.75366724e-05, 8.77408311e-04, 1.42371946e-03,\n",
              "        6.95421919e-03, 8.43035709e-03],\n",
              "       [2.44685523e-02, 1.60283893e-02, 6.18062504e-02, 2.04947521e-03,\n",
              "        1.95294470e-01, 2.12494735e-04, 1.12052195e-01, 7.04901293e-03,\n",
              "        6.07851216e-05, 6.65393984e-03, 4.41264268e-03, 7.34085776e-03,\n",
              "        2.64862960e-04, 8.53862846e-04, 5.37115186e-02, 2.05106079e-03,\n",
              "        2.84424226e-04, 5.27285738e-03, 3.35117360e-03, 4.27839150e-05,\n",
              "        2.14667812e-01, 1.87200203e-04, 5.60493069e-03, 2.03081663e-03,\n",
              "        1.27534603e-03, 2.72972286e-01],\n",
              "       [5.05493656e-02, 1.67931113e-02, 2.10827636e-03, 2.11213678e-02,\n",
              "        2.32776795e-02, 5.76413295e-04, 9.98574961e-03, 8.08197260e-03,\n",
              "        1.73450704e-03, 3.71705694e-03, 4.13861908e-02, 6.39925361e-01,\n",
              "        1.89116196e-04, 8.17045511e-04, 3.05066886e-03, 3.52500863e-02,\n",
              "        3.83035559e-03, 4.35204571e-03, 7.10745761e-03, 3.08582443e-03,\n",
              "        3.17007080e-02, 3.04357205e-02, 7.47398660e-03, 9.14918818e-03,\n",
              "        1.68755557e-02, 2.74252538e-02],\n",
              "       [5.86936204e-03, 1.07758135e-01, 1.57957652e-03, 7.66333938e-02,\n",
              "        1.52159840e-01, 1.17989667e-02, 1.50421113e-02, 3.10953427e-03,\n",
              "        2.69808254e-04, 2.65944470e-02, 8.46199691e-02, 3.96061391e-02,\n",
              "        3.92811978e-03, 2.18643202e-03, 3.83734424e-03, 8.65190011e-03,\n",
              "        1.32801745e-03, 1.40171349e-01, 3.03497855e-02, 4.12987778e-03,\n",
              "        5.90156391e-03, 9.00717918e-04, 8.02306645e-03, 2.53670961e-01,\n",
              "        6.86789397e-03, 5.01176342e-03],\n",
              "       [9.48966891e-02, 1.40902633e-03, 1.10819143e-04, 9.96317132e-04,\n",
              "        3.56697237e-05, 4.85970639e-04, 9.93457390e-04, 9.59833851e-05,\n",
              "        2.79939976e-02, 8.32290761e-03, 4.73254956e-02, 1.92611548e-03,\n",
              "        8.20649730e-04, 3.13566849e-02, 3.28279595e-04, 3.76815267e-04,\n",
              "        6.53816611e-02, 2.80267536e-03, 2.61733830e-02, 3.30695838e-01,\n",
              "        8.55524500e-04, 9.14642646e-04, 1.94258150e-02, 3.23238666e-03,\n",
              "        3.22450936e-01, 1.05922520e-02],\n",
              "       [1.47406280e-01, 1.06242020e-02, 1.00889395e-03, 1.82567518e-02,\n",
              "        3.31178890e-04, 5.28974342e-04, 2.46009305e-02, 1.46325526e-03,\n",
              "        1.20258657e-02, 4.13603112e-02, 1.66912619e-02, 1.53420097e-03,\n",
              "        3.79718747e-03, 1.02741890e-01, 3.02367341e-02, 3.47728317e-04,\n",
              "        2.95874506e-01, 1.87880397e-02, 2.37722564e-02, 8.85813683e-02,\n",
              "        1.95646547e-02, 6.14131801e-03, 9.71918330e-02, 8.08659662e-03,\n",
              "        2.34063640e-02, 5.63746877e-03],\n",
              "       [6.37291465e-04, 1.29651740e-01, 1.10566176e-01, 2.09809179e-04,\n",
              "        1.56137953e-02, 3.95134091e-02, 8.61899629e-02, 5.90615600e-05,\n",
              "        6.81746900e-02, 3.03249136e-02, 2.61549205e-02, 3.44081927e-04,\n",
              "        3.57633871e-05, 1.50056076e-04, 2.49614037e-04, 3.70201319e-02,\n",
              "        2.37742552e-05, 1.29098119e-03, 2.25219414e-01, 1.00597078e-02,\n",
              "        2.12874409e-04, 4.33758169e-06, 9.64355277e-05, 2.37223678e-04,\n",
              "        5.07043861e-03, 2.12889448e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "predicted = model2.predict(TEST_SET_2D)\n",
        "predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd_I7bri0DMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7d2bf50-e66b-4176-8476-3a6020d82ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A', 'D'],\n",
              " ['B', 'B'],\n",
              " ['C', 'C'],\n",
              " ['D', 'B'],\n",
              " ['E', 'K'],\n",
              " ['F', 'F'],\n",
              " ['G', 'G'],\n",
              " ['H', 'R'],\n",
              " ['I', 'I'],\n",
              " ['J', 'J'],\n",
              " ['K', 'K'],\n",
              " ['L', 'B'],\n",
              " ['M', 'K'],\n",
              " ['N', 'R'],\n",
              " ['O', 'G'],\n",
              " ['P', 'P'],\n",
              " ['Q', 'D'],\n",
              " ['R', 'S'],\n",
              " ['S', 'B'],\n",
              " ['T', 'S'],\n",
              " ['U', 'Z'],\n",
              " ['V', 'L'],\n",
              " ['W', 'X'],\n",
              " ['X', 'T'],\n",
              " ['Y', 'Q'],\n",
              " ['Z', 'S']]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "result_list = []\n",
        "for num in range(len(predicted)):\n",
        "    max_val = 0\n",
        "    actual = chr(num+ord('A'))\n",
        "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    result_list.append([actual, predict])\n",
        "result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHEgSKw70ERA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53b2d87-49b5-414f-846f-27126a91b9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images correctly classified: 30.76923076923077%\n",
            "Here are the list of test images that are misclassified and how they appear:\n",
            "Actual    Predicted\n",
            "A         D\n",
            "D         B\n",
            "E         K\n",
            "H         R\n",
            "L         B\n",
            "M         K\n",
            "N         R\n",
            "O         G\n",
            "Q         D\n",
            "R         S\n",
            "S         B\n",
            "T         S\n",
            "U         Z\n",
            "V         L\n",
            "W         X\n",
            "X         T\n",
            "Y         Q\n",
            "Z         S\n"
          ]
        }
      ],
      "source": [
        "list_misclassified = []\n",
        "num_total, num_correct = 26, 26\n",
        "for i in range(len(result_list)):\n",
        "    if result_list[i][0] != result_list[i][1]:\n",
        "        num_correct -= 1\n",
        "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
        "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_misclassified) == 0:\n",
        "    print(\"All test images are classified correctly.\")\n",
        "else:\n",
        "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_misclassified)):\n",
        "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use show() to display the misclassified images.\n",
        "for j in range(len(list_misclassified)):\n",
        "    # Subtract unicode value of current letter from uppercase A to obtain the letter's position.\n",
        "    letter_train = ord(list_misclassified[j][0]) - ord('A')\n",
        "    letter_test = ord(list_misclassified[j][1]) - ord('A')\n",
        "    print(\"================================================\")\n",
        "    print(\"Expected letter:\")\n",
        "    show(TRAINING_SET_2D[letter_train])\n",
        "    print(\"Predicted letter:\")\n",
        "    show(TEST_SET_2D[letter_test])\n",
        "    print(\"================================================\")"
      ],
      "metadata": {
        "id": "UcwScgv1DLzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ebad06-8ee0-4d33-a05d-f0451fc4d692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================\n",
            "Expected letter:\n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ########      \n",
            "  ########      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "##########      \n",
            "##########      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "##########      \n",
            "##########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##########    \n",
            "  ##########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "##############  \n",
            "##############  \n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "  ####  ##      \n",
            "  ####  ##      \n",
            "  ########      \n",
            "  ########      \n",
            "  ####  ##      \n",
            "  ####  ##      \n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "##############  \n",
            "##############  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ####        \n",
            "    ####        \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ####      ####\n",
            "  ####      ####\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ######    ##  \n",
            "  ######    ##  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "########        \n",
            "########        \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "##############  \n",
            "##############  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##########    \n",
            "  ##########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "######  ######  \n",
            "######  ######  \n",
            "##############  \n",
            "##############  \n",
            "##############  \n",
            "##############  \n",
            "####  ##  ####  \n",
            "####  ##  ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ####        \n",
            "    ####        \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ####      ####\n",
            "  ####      ####\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "######    ####  \n",
            "######    ####  \n",
            "########  ####  \n",
            "########  ####  \n",
            "####  ########  \n",
            "####  ########  \n",
            "####    ######  \n",
            "####    ######  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ######    ##  \n",
            "  ######    ##  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "    ######      \n",
            "    ######      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "    ######      \n",
            "    ######      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "      ######    \n",
            "      ######    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##    ######  \n",
            "  ##    ######  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "      ########  \n",
            "      ########  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####  ######    \n",
            "####  ######    \n",
            "  ########      \n",
            "  ########      \n",
            "      ######    \n",
            "      ######    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ########      \n",
            "  ########      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "############    \n",
            "############    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ##########    \n",
            "  ##########    \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "######    ####  \n",
            "######    ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##            \n",
            "  ##            \n",
            "    ########    \n",
            "    ########    \n",
            "            ##  \n",
            "            ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "    ########    \n",
            "    ########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "######          \n",
            "######          \n",
            "  ######        \n",
            "  ######        \n",
            "      ######    \n",
            "      ######    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##########    \n",
            "  ##########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "############    \n",
            "############    \n",
            "##  ####  ##    \n",
            "##  ####  ##    \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##            \n",
            "  ##            \n",
            "    ########    \n",
            "    ########    \n",
            "            ##  \n",
            "            ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "    ########    \n",
            "    ########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##############\n",
            "  ##############\n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "          ##    \n",
            "          ##    \n",
            "        ##      \n",
            "        ##      \n",
            "      ##        \n",
            "      ##        \n",
            "    ##        ##\n",
            "    ##        ##\n",
            "  ##############\n",
            "  ##############\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "    ####        \n",
            "    ####        \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ######        \n",
            "  ######        \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ############  \n",
            "  ############  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####  ##  ####  \n",
            "####  ##  ####  \n",
            "##############  \n",
            "##############  \n",
            "######  ######  \n",
            "######  ######  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "      ##  ##    \n",
            "      ##  ##    \n",
            "        ##      \n",
            "        ##      \n",
            "      ##  ##    \n",
            "      ##  ##    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##          ##\n",
            "  ##          ##\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "    ######      \n",
            "    ######      \n",
            "    ######      \n",
            "    ######      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##############\n",
            "  ##############\n",
            "  ##    ##    ##\n",
            "  ##    ##    ##\n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "      ######    \n",
            "      ######    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##    ##  ##  \n",
            "  ##    ##  ##  \n",
            "    ########    \n",
            "    ########    \n",
            "            ####\n",
            "            ####\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "##############  \n",
            "##############  \n",
            "####      ####  \n",
            "####      ####  \n",
            "##      ####    \n",
            "##      ####    \n",
            "      ####      \n",
            "      ####      \n",
            "    ####    ##  \n",
            "    ####    ##  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "##############  \n",
            "##############  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##            \n",
            "  ##            \n",
            "    ########    \n",
            "    ########    \n",
            "            ##  \n",
            "            ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "    ########    \n",
            "    ########    \n",
            "                \n",
            "                \n",
            "================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEEngNBxSkHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90696a7e-2c84-41a9-9dd9-f943c263af27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.19235788e-03, 9.54179466e-02, 3.98580991e-02, 3.27252597e-03,\n",
              "        4.40965174e-03, 2.23761629e-02, 6.98719174e-02, 2.13275052e-04,\n",
              "        1.14055410e-01, 1.13658823e-01, 1.36912242e-02, 1.79470517e-04,\n",
              "        7.24958838e-04, 3.97980120e-03, 3.62006319e-03, 5.11000445e-03,\n",
              "        2.46747746e-03, 6.73686527e-03, 4.15984005e-01, 5.50851226e-02,\n",
              "        2.39575678e-03, 4.35636139e-05, 9.15596844e-04, 1.66770665e-03,\n",
              "        4.82228771e-03, 1.52499396e-02],\n",
              "       [2.50387122e-04, 1.37714832e-03, 3.91815320e-06, 8.17092042e-03,\n",
              "        3.33481492e-03, 1.11306188e-04, 9.74598006e-05, 2.37979181e-03,\n",
              "        2.78065837e-08, 1.04575558e-03, 1.85229387e-02, 2.15692353e-02,\n",
              "        5.21276072e-02, 2.21207179e-03, 1.07238506e-04, 5.26612275e-05,\n",
              "        3.83013721e-05, 8.22416008e-01, 7.16520939e-03, 1.97754489e-04,\n",
              "        8.01461574e-04, 1.45595804e-05, 2.22544745e-02, 3.54472287e-02,\n",
              "        1.30320885e-04, 1.71255873e-04],\n",
              "       [1.68585677e-06, 3.12777306e-03, 6.21357831e-05, 3.51869767e-06,\n",
              "        9.05727267e-01, 8.62917583e-03, 9.77724631e-06, 1.50371461e-05,\n",
              "        1.50290091e-08, 1.32642192e-04, 4.77446802e-02, 1.19755007e-02,\n",
              "        6.68503026e-06, 1.55332032e-07, 1.69477605e-08, 6.23970618e-03,\n",
              "        2.11374737e-10, 1.34200812e-03, 1.31248543e-03, 1.12536156e-07,\n",
              "        3.36839821e-06, 1.24044499e-08, 4.99188786e-07, 1.94912753e-03,\n",
              "        3.29827453e-04, 1.13867540e-02],\n",
              "       [3.57809471e-09, 8.07326811e-04, 1.86722042e-04, 2.70438374e-08,\n",
              "        2.93762293e-02, 9.46283877e-01, 3.81872212e-07, 5.33608409e-06,\n",
              "        8.43246184e-08, 6.29513233e-05, 1.48242794e-03, 5.57471612e-05,\n",
              "        1.17869449e-05, 2.74022103e-08, 8.81539702e-11, 1.19122462e-02,\n",
              "        3.89703894e-12, 1.39597496e-05, 8.20674095e-03, 2.20007152e-07,\n",
              "        2.01643520e-08, 1.45737233e-09, 1.41309773e-08, 3.36547091e-04,\n",
              "        3.52418108e-04, 9.04799323e-04],\n",
              "       [1.30958768e-04, 3.53784617e-06, 4.88144281e-07, 2.41358748e-07,\n",
              "        8.17571166e-09, 4.79452501e-05, 9.15420685e-07, 6.32322497e-08,\n",
              "        9.01992977e-01, 1.70262228e-05, 1.22740894e-04, 5.20214940e-07,\n",
              "        7.91376920e-08, 2.62884205e-05, 2.30001778e-08, 1.78612981e-06,\n",
              "        9.78532247e-04, 7.27092811e-07, 1.90507993e-03, 9.32722613e-02,\n",
              "        1.89241305e-07, 3.40106749e-06, 1.21126982e-06, 3.64477842e-06,\n",
              "        1.48590584e-03, 3.44118121e-06],\n",
              "       [1.73240231e-04, 6.01847614e-06, 6.96920470e-05, 9.62800987e-05,\n",
              "        5.10458023e-08, 2.22952949e-07, 6.19695174e-06, 2.80586840e-03,\n",
              "        3.91513913e-06, 1.16114534e-05, 3.22632197e-08, 1.94323343e-06,\n",
              "        1.15078772e-04, 5.39391622e-05, 1.37044175e-04, 2.09912650e-05,\n",
              "        4.20707883e-03, 4.29017604e-07, 6.07375732e-05, 2.37795859e-04,\n",
              "        6.09425688e-03, 9.85685885e-01, 1.51025306e-04, 1.13476733e-06,\n",
              "        5.69711628e-05, 2.50564835e-06],\n",
              "       [2.74946015e-05, 9.30770412e-02, 2.20457162e-03, 3.13634264e-05,\n",
              "        4.54759821e-02, 1.54075503e-01, 1.18361530e-03, 1.17678937e-05,\n",
              "        1.45972602e-03, 9.03287809e-03, 2.79684126e-01, 2.12456146e-03,\n",
              "        2.11201423e-05, 1.00412026e-05, 8.45929492e-07, 2.31975093e-02,\n",
              "        3.06995105e-07, 4.39815689e-03, 2.97950864e-01, 4.33818670e-03,\n",
              "        3.73670764e-06, 3.11463531e-08, 1.53460896e-05, 1.17853761e-03,\n",
              "        1.07619446e-02, 6.97347745e-02],\n",
              "       [6.26688006e-07, 9.33008552e-01, 1.82057902e-05, 2.02740543e-02,\n",
              "        5.35790110e-04, 2.82930500e-06, 6.90700952e-04, 4.52240165e-08,\n",
              "        2.55282484e-09, 3.57165816e-04, 8.88795767e-04, 2.48582289e-03,\n",
              "        2.93318550e-07, 6.56101573e-09, 7.84338135e-06, 2.12496687e-02,\n",
              "        3.40286448e-08, 1.83122028e-02, 2.04345258e-03, 9.23934349e-05,\n",
              "        1.23576365e-05, 8.94289087e-07, 1.45246560e-07, 4.05803434e-07,\n",
              "        5.80902864e-08, 1.76152753e-05],\n",
              "       [7.67357554e-03, 8.91476928e-04, 4.92686452e-03, 7.27752165e-04,\n",
              "        1.15810860e-06, 1.32483136e-09, 4.37530875e-02, 4.23260935e-05,\n",
              "        1.13905924e-06, 5.68740797e-05, 1.60664115e-07, 2.73492361e-07,\n",
              "        2.54728116e-06, 8.17993787e-05, 8.48288953e-01, 1.37227721e-06,\n",
              "        6.02513552e-03, 7.02024181e-06, 3.93786759e-04, 7.84119675e-06,\n",
              "        8.59729648e-02, 3.03312641e-04, 8.25433293e-04, 2.23396288e-08,\n",
              "        8.69458177e-08, 1.49576890e-05],\n",
              "       [1.18368745e-01, 1.18762860e-03, 2.06468962e-04, 1.11120078e-03,\n",
              "        3.28291171e-05, 4.66931931e-04, 1.32741919e-03, 1.36855771e-04,\n",
              "        2.22325362e-02, 9.93802585e-03, 3.38934883e-02, 1.48313469e-03,\n",
              "        1.17661385e-03, 5.11973538e-02, 5.75204322e-04, 2.84721813e-04,\n",
              "        7.44993910e-02, 2.47492595e-03, 2.46842094e-02, 2.63470888e-01,\n",
              "        1.47628086e-03, 8.32914666e-04, 2.31807791e-02, 3.01373890e-03,\n",
              "        3.47968966e-01, 1.47787584e-02],\n",
              "       [3.17527878e-07, 1.40833090e-05, 2.80470158e-06, 7.59020580e-08,\n",
              "        1.47887347e-08, 2.96976796e-05, 3.99411789e-07, 1.21119772e-08,\n",
              "        9.34968889e-01, 4.05124752e-07, 3.26506888e-05, 2.51472488e-06,\n",
              "        7.67958497e-09, 2.82199633e-08, 1.02571074e-09, 1.68504776e-04,\n",
              "        5.45027342e-06, 8.73072565e-08, 1.99119328e-03, 6.27635941e-02,\n",
              "        2.61134736e-09, 2.46367631e-06, 2.45295269e-08, 1.17113750e-08,\n",
              "        1.54263016e-05, 1.44069293e-06],\n",
              "       [9.60200214e-06, 8.42767520e-07, 2.03223726e-06, 7.17889488e-05,\n",
              "        1.77259196e-07, 1.16072172e-07, 2.00812137e-05, 2.99413484e-02,\n",
              "        1.97015328e-08, 1.63715631e-05, 3.23348019e-07, 1.06038613e-06,\n",
              "        3.12526673e-01, 6.34720027e-01, 9.43824343e-05, 1.02858763e-07,\n",
              "        9.06476707e-05, 1.04113969e-04, 4.74475208e-04, 1.45133117e-05,\n",
              "        1.52450660e-03, 4.01436810e-05, 2.03130040e-02, 1.15118373e-05,\n",
              "        7.12276290e-07, 2.12856921e-05],\n",
              "       [4.88683721e-03, 1.37313118e-03, 8.11654329e-03, 9.14811244e-05,\n",
              "        4.19657317e-06, 7.78572229e-09, 9.43651617e-01, 2.61461935e-07,\n",
              "        3.49513675e-06, 2.12720872e-04, 6.19231650e-06, 4.41284733e-08,\n",
              "        8.47990549e-08, 5.50216828e-06, 3.68061177e-02, 2.96033841e-06,\n",
              "        1.16550245e-05, 1.42192475e-05, 2.71865865e-03, 2.50966946e-06,\n",
              "        2.01319717e-03, 3.52758377e-07, 1.43758743e-05, 3.95804900e-09,\n",
              "        1.22494839e-07, 6.35963879e-05],\n",
              "       [4.52305627e-04, 1.77180581e-07, 2.65246958e-09, 2.58772143e-05,\n",
              "        4.10957526e-08, 7.22132840e-11, 2.13465355e-05, 5.59755194e-04,\n",
              "        1.77316370e-10, 2.25542954e-06, 9.71875011e-07, 2.16184276e-06,\n",
              "        3.48904228e-04, 2.61619315e-02, 1.21965677e-04, 6.37199668e-11,\n",
              "        3.20015009e-04, 5.39913635e-05, 1.97824611e-05, 6.98173892e-07,\n",
              "        1.63486891e-03, 2.03256059e-06, 9.70225871e-01, 4.21310433e-05,\n",
              "        9.47966498e-07, 1.95326152e-06],\n",
              "       [1.46301449e-04, 1.54072188e-06, 1.86658539e-07, 1.91726443e-07,\n",
              "        2.44281884e-09, 8.32189380e-06, 4.87456873e-07, 2.63438515e-08,\n",
              "        9.23497915e-01, 6.86495696e-06, 7.30746178e-05, 2.77377694e-07,\n",
              "        3.17699644e-08, 1.82774729e-05, 1.79397457e-08, 6.88705200e-07,\n",
              "        1.53770763e-03, 3.27575947e-07, 8.51715857e-04, 7.29039088e-02,\n",
              "        1.19404163e-07, 3.37879078e-06, 1.10694771e-06, 8.52831761e-07,\n",
              "        9.44453117e-04, 2.25661688e-06],\n",
              "       [4.40313248e-04, 1.73408017e-01, 9.05973539e-02, 2.66063784e-04,\n",
              "        1.53687028e-02, 4.24875803e-02, 9.73543376e-02, 4.88389451e-05,\n",
              "        5.05906790e-02, 4.05425690e-02, 2.44154036e-02, 2.16434506e-04,\n",
              "        4.30477194e-05, 1.40680102e-04, 2.54623621e-04, 3.50321680e-02,\n",
              "        1.88409449e-05, 1.86222687e-03, 2.79190361e-01, 1.03156241e-02,\n",
              "        1.58278621e-04, 3.00268380e-06, 7.48175953e-05, 2.11348844e-04,\n",
              "        3.75897205e-03, 1.33199766e-01],\n",
              "       [9.39762414e-01, 1.53984161e-04, 4.46145759e-06, 1.64671347e-03,\n",
              "        2.69953205e-07, 3.39102052e-10, 7.35387998e-03, 3.98509246e-06,\n",
              "        9.98178848e-07, 6.00491912e-05, 3.31214105e-05, 1.40106579e-06,\n",
              "        5.41657016e-07, 3.86152096e-04, 1.49150528e-02, 8.80209114e-08,\n",
              "        1.96818057e-02, 2.87667772e-05, 9.95424110e-04, 1.14222457e-05,\n",
              "        1.08591663e-02, 4.05765779e-04, 3.68944649e-03, 1.05087406e-06,\n",
              "        2.79014421e-06, 1.32302432e-06],\n",
              "       [4.47396269e-05, 1.96599305e-01, 3.81314033e-03, 1.46277463e-02,\n",
              "        3.79127654e-04, 3.40059563e-03, 7.41690071e-03, 1.71411943e-06,\n",
              "        1.06588770e-02, 5.22140972e-03, 4.23384570e-02, 1.20926322e-03,\n",
              "        5.11376893e-05, 9.65667004e-06, 6.42790546e-05, 8.36537480e-02,\n",
              "        6.22725929e-05, 2.05476489e-02, 3.93724889e-01, 2.15958402e-01,\n",
              "        3.92632001e-06, 1.10121618e-05, 9.60920170e-06, 3.07568953e-05,\n",
              "        5.16761829e-05, 1.09695691e-04],\n",
              "       [2.43810300e-05, 3.34769547e-01, 3.60572664e-03, 9.26352590e-02,\n",
              "        1.50528314e-04, 5.24587871e-04, 3.65249277e-03, 1.09886578e-05,\n",
              "        1.31088321e-03, 3.65047809e-03, 1.78695016e-03, 8.13874509e-03,\n",
              "        3.66036111e-05, 6.57673763e-06, 3.61277431e-04, 4.42642808e-01,\n",
              "        2.83266505e-04, 1.18865876e-03, 2.08259001e-02, 8.10208842e-02,\n",
              "        6.59659054e-05, 3.05619906e-03, 1.26103378e-05, 3.63192612e-06,\n",
              "        3.09029783e-05, 2.04150565e-04],\n",
              "       [1.69657485e-03, 2.36200485e-02, 1.37380045e-03, 2.30077398e-03,\n",
              "        7.96146342e-05, 7.36350848e-05, 5.53136272e-03, 2.92523528e-05,\n",
              "        4.49703039e-05, 4.25415579e-03, 1.51314614e-02, 3.52469709e-04,\n",
              "        4.22478653e-04, 1.35151393e-04, 2.29778845e-04, 1.94402365e-03,\n",
              "        8.81641172e-05, 1.37372846e-02, 9.21766818e-01, 5.01365308e-03,\n",
              "        3.14991310e-04, 7.99523114e-05, 1.51069497e-03, 1.87017340e-05,\n",
              "        1.12766960e-04, 1.37566210e-04],\n",
              "       [9.36972137e-05, 1.37530419e-03, 2.03061197e-03, 3.34712939e-04,\n",
              "        2.62771682e-05, 6.11114874e-03, 3.40521103e-03, 1.62441502e-05,\n",
              "        2.88354611e-04, 9.75380957e-01, 1.03020160e-04, 7.59489637e-07,\n",
              "        2.96317041e-04, 6.92277797e-04, 1.28758873e-03, 3.58810095e-04,\n",
              "        2.07828271e-05, 5.62105037e-04, 4.65886574e-03, 2.74346559e-04,\n",
              "        1.31165690e-03, 5.49889273e-06, 1.59292758e-05, 4.73752407e-05,\n",
              "        2.84014357e-04, 1.01820088e-03],\n",
              "       [2.22323870e-04, 3.27483122e-06, 6.56373892e-03, 1.01823643e-05,\n",
              "        1.12480429e-05, 1.21379844e-06, 1.34021495e-04, 2.94925794e-02,\n",
              "        2.24162682e-06, 4.32298984e-05, 1.40932448e-08, 2.48149126e-06,\n",
              "        1.43320125e-04, 1.01024845e-04, 1.95607799e-03, 2.21362407e-05,\n",
              "        1.72898744e-05, 6.21905258e-07, 8.04579686e-05, 4.09167143e-07,\n",
              "        9.59140360e-01, 9.55512631e-04, 1.37000397e-05, 1.22200549e-06,\n",
              "        5.69726617e-05, 1.02434726e-03],\n",
              "       [6.91272106e-08, 3.62847672e-08, 5.34073230e-10, 5.46053798e-06,\n",
              "        7.20596802e-07, 1.38921649e-07, 2.97631271e-08, 1.29426960e-02,\n",
              "        8.64097335e-12, 3.09673305e-06, 2.60300953e-06, 8.16013471e-06,\n",
              "        9.37106788e-01, 4.36793715e-02, 1.31586219e-07, 8.51885851e-09,\n",
              "        7.55896323e-08, 3.16616357e-03, 3.34067518e-05, 1.98724877e-07,\n",
              "        2.63863558e-06, 1.83457303e-08, 2.71263509e-03, 3.31318442e-04,\n",
              "        5.76997195e-07, 3.65763663e-06],\n",
              "       [4.08954755e-08, 1.80359092e-02, 9.70476976e-05, 1.56816859e-05,\n",
              "        1.17106987e-02, 1.79533120e-02, 1.88215847e-06, 1.90900710e-06,\n",
              "        1.57436332e-07, 1.44371355e-04, 5.01204841e-03, 1.39824012e-02,\n",
              "        1.72877990e-05, 8.24840853e-08, 6.55333698e-09, 9.26518023e-01,\n",
              "        8.69957606e-10, 2.08025842e-04, 4.59797215e-03, 2.31544454e-05,\n",
              "        3.44989331e-07, 6.49209653e-07, 1.66032990e-07, 7.61922784e-05,\n",
              "        3.65531705e-05, 1.56608527e-03],\n",
              "       [2.65061446e-02, 7.65022560e-05, 8.40890607e-06, 6.17468788e-04,\n",
              "        2.12340412e-09, 2.30786518e-11, 9.60515055e-04, 2.57366878e-06,\n",
              "        7.77513833e-07, 1.02006115e-05, 7.06504935e-08, 3.97675528e-08,\n",
              "        5.13877865e-07, 2.06632933e-04, 8.31860825e-02, 2.02491552e-08,\n",
              "        8.65292549e-01, 6.74853766e-07, 4.71262792e-05, 9.60485268e-05,\n",
              "        1.19515462e-02, 6.64205942e-03, 4.39359900e-03, 3.81004872e-09,\n",
              "        1.03873958e-07, 3.10442061e-07],\n",
              "       [3.00392415e-02, 4.70033046e-05, 5.07575460e-04, 1.83927477e-04,\n",
              "        1.27621604e-06, 1.35811606e-09, 2.39088433e-03, 1.19826081e-03,\n",
              "        1.14541399e-06, 1.68573097e-05, 1.51050457e-07, 1.85910187e-06,\n",
              "        2.48689794e-05, 8.90938623e-04, 1.01359010e-01, 1.61742193e-07,\n",
              "        1.75843760e-02, 3.46252409e-06, 1.83393786e-04, 3.61968023e-06,\n",
              "        8.34117532e-01, 8.68507079e-04, 1.04291439e-02, 2.79285899e-07,\n",
              "        2.51314441e-06, 1.43922996e-04],\n",
              "       [9.10089000e-08, 7.48211323e-06, 1.42363240e-06, 1.94858778e-08,\n",
              "        1.58119917e-08, 6.57196724e-05, 1.12555718e-07, 6.45968834e-09,\n",
              "        9.51702356e-01, 1.85790270e-07, 2.77821964e-05, 3.44731188e-06,\n",
              "        2.05769801e-09, 8.31643998e-09, 1.50213897e-10, 1.41220211e-04,\n",
              "        1.33526339e-06, 2.82904722e-08, 8.68358067e-04, 4.71392013e-02,\n",
              "        1.01689501e-09, 8.30983595e-07, 7.46428430e-09, 1.35595295e-08,\n",
              "        3.81986611e-05, 2.22299218e-06],\n",
              "       [1.12623136e-04, 4.47756436e-04, 9.67323244e-01, 1.83760255e-06,\n",
              "        1.22029349e-04, 2.15105156e-05, 1.50920711e-02, 2.91087235e-05,\n",
              "        1.75660516e-05, 2.53621809e-04, 2.36330948e-06, 4.76704457e-07,\n",
              "        2.16366652e-06, 1.44096020e-05, 1.57014513e-03, 8.51632576e-05,\n",
              "        7.54911866e-07, 1.50179267e-06, 1.61383592e-03, 4.25695021e-07,\n",
              "        2.16338621e-03, 8.10334882e-07, 6.11049336e-06, 2.62168328e-07,\n",
              "        1.00965071e-05, 1.11068320e-02],\n",
              "       [4.01256148e-05, 2.12365855e-03, 1.81680576e-07, 5.82367320e-05,\n",
              "        1.90817639e-02, 7.27632214e-05, 2.47115895e-06, 3.33954659e-07,\n",
              "        1.90533478e-09, 1.30843226e-04, 8.78444135e-01, 5.55050299e-02,\n",
              "        2.24499809e-06, 2.19118732e-07, 9.87212712e-09, 4.79976792e-04,\n",
              "        4.13932577e-09, 3.49255539e-02, 1.57201779e-03, 1.01568207e-06,\n",
              "        6.86549186e-07, 9.52334922e-09, 5.71420105e-06, 7.32459640e-03,\n",
              "        1.08485045e-04, 1.19977260e-04],\n",
              "       [5.66397169e-08, 1.16837947e-02, 5.23195709e-08, 1.54592970e-04,\n",
              "        2.12777406e-03, 6.18284002e-06, 1.13970464e-06, 3.62651882e-08,\n",
              "        1.50246071e-09, 9.49625701e-06, 4.78504738e-03, 9.61041987e-01,\n",
              "        6.21914609e-09, 4.91016672e-10, 5.36326450e-09, 1.96998175e-02,\n",
              "        9.80934889e-10, 2.99740321e-04, 1.49560747e-05, 5.26305530e-06,\n",
              "        3.47849607e-07, 1.40765266e-07, 1.04881565e-07, 1.02523736e-05,\n",
              "        3.12348902e-06, 1.56078211e-04],\n",
              "       [2.21984042e-03, 7.18896672e-06, 1.93275113e-07, 1.57933215e-07,\n",
              "        4.28259312e-07, 4.62455500e-05, 1.10769849e-06, 1.73485148e-07,\n",
              "        2.82299647e-04, 1.31162509e-04, 3.46012739e-03, 1.34174434e-05,\n",
              "        9.95556206e-07, 4.54441411e-04, 2.37618547e-08, 9.01614555e-07,\n",
              "        3.91866743e-05, 2.86543445e-06, 2.11250153e-03, 1.51086680e-03,\n",
              "        9.52192124e-07, 1.95934774e-07, 1.87436715e-04, 4.31278395e-03,\n",
              "        9.84694660e-01, 5.19780908e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "predicted_message = model2.predict(MESSAGE_2D)\n",
        "predicted_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3jvVHJvSclF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61056cc9-9e81-4b0e-8762-fc798cde9f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SREFIVSBOYINGWISASPSJUMPQUICKLY\n"
          ]
        }
      ],
      "source": [
        "message_list = \"\"\n",
        "for num in range(len(predicted_message)):\n",
        "    max_val = 0\n",
        "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    message_list += predict\n",
        "print(message_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv11X09hSb3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c081ffc-c72f-43bf-df40-d776dc248f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy between predicted and actual message: 77.41935483870968%\n",
            "Here is the list of letters that are a mismatch between predicted and actual message:\n",
            "Actual    Predicted\n",
            "T         S\n",
            "H         R\n",
            "E         S\n",
            "X         Y\n",
            "Z         S\n",
            "R         S\n",
            "D         P\n"
          ]
        }
      ],
      "source": [
        "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
        "list_mismatch = []\n",
        "num_total, num_correct = len(actual_message), len(message_list)\n",
        "for i in range(len(actual_message)):\n",
        "    if message_list[i] != actual_message[i]:\n",
        "        num_correct -= 1\n",
        "        list_mismatch.append([actual_message[i], message_list[i]])\n",
        "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_mismatch) == 0:\n",
        "    print(\"The message appears to be decoded correctly.\")\n",
        "else:\n",
        "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_mismatch)):\n",
        "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZPpiD5BrI6i"
      },
      "source": [
        "# Does the performance improve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-GVfQg0rJyH"
      },
      "source": [
        "## A: No, the performance does not improve simply by adding more hidden layers. The test set accuracy decreases after we add more layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFxJNZe6gfw-"
      },
      "source": [
        "# 8. Repeat experiment (7), adding additional layers of the same size until the message is decoded correctly. What results do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKLc7F4KpJ2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a153a2a6-41a4-4d29-addd-b53e7fa058cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Hidden_layer1 (Dense)       (None, 32)                8224      \n",
            "                                                                 \n",
            " Hidden_layer2 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer3 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer4 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer5 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer6 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer7 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer8 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer9 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer10 (Dense)      (None, 32)                1056      \n",
            "                                                                 \n",
            " Output_layer (Dense)        (None, 26)                858       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,586\n",
            "Trainable params: 18,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/9999\n",
            "2/2 - 1s - loss: 3.2597 - categorical_accuracy: 0.0577 - 548ms/epoch - 274ms/step\n",
            "Epoch 2/9999\n",
            "2/2 - 0s - loss: 3.2576 - categorical_accuracy: 0.0192 - 9ms/epoch - 4ms/step\n",
            "Epoch 3/9999\n",
            "2/2 - 0s - loss: 3.2565 - categorical_accuracy: 0.0577 - 9ms/epoch - 4ms/step\n",
            "Epoch 4/9999\n",
            "2/2 - 0s - loss: 3.2555 - categorical_accuracy: 0.0385 - 7ms/epoch - 4ms/step\n",
            "Epoch 5/9999\n",
            "2/2 - 0s - loss: 3.2541 - categorical_accuracy: 0.0577 - 8ms/epoch - 4ms/step\n",
            "Epoch 6/9999\n",
            "2/2 - 0s - loss: 3.2528 - categorical_accuracy: 0.0962 - 8ms/epoch - 4ms/step\n",
            "Epoch 7/9999\n",
            "2/2 - 0s - loss: 3.2510 - categorical_accuracy: 0.0577 - 10ms/epoch - 5ms/step\n",
            "Epoch 8/9999\n",
            "2/2 - 0s - loss: 3.2486 - categorical_accuracy: 0.0962 - 8ms/epoch - 4ms/step\n",
            "Epoch 9/9999\n",
            "2/2 - 0s - loss: 3.2459 - categorical_accuracy: 0.1154 - 10ms/epoch - 5ms/step\n",
            "Epoch 10/9999\n",
            "2/2 - 0s - loss: 3.2428 - categorical_accuracy: 0.0962 - 9ms/epoch - 4ms/step\n",
            "Epoch 11/9999\n",
            "2/2 - 0s - loss: 3.2382 - categorical_accuracy: 0.0962 - 10ms/epoch - 5ms/step\n",
            "Epoch 12/9999\n",
            "2/2 - 0s - loss: 3.2331 - categorical_accuracy: 0.0769 - 10ms/epoch - 5ms/step\n",
            "Epoch 13/9999\n",
            "2/2 - 0s - loss: 3.2265 - categorical_accuracy: 0.0769 - 10ms/epoch - 5ms/step\n",
            "Epoch 14/9999\n",
            "2/2 - 0s - loss: 3.2191 - categorical_accuracy: 0.0962 - 11ms/epoch - 5ms/step\n",
            "Epoch 15/9999\n",
            "2/2 - 0s - loss: 3.2094 - categorical_accuracy: 0.0962 - 16ms/epoch - 8ms/step\n",
            "Epoch 16/9999\n",
            "2/2 - 0s - loss: 3.1966 - categorical_accuracy: 0.0962 - 9ms/epoch - 4ms/step\n",
            "Epoch 17/9999\n",
            "2/2 - 0s - loss: 3.1855 - categorical_accuracy: 0.1154 - 9ms/epoch - 4ms/step\n",
            "Epoch 18/9999\n",
            "2/2 - 0s - loss: 3.1667 - categorical_accuracy: 0.0962 - 8ms/epoch - 4ms/step\n",
            "Epoch 19/9999\n",
            "2/2 - 0s - loss: 3.1448 - categorical_accuracy: 0.0962 - 9ms/epoch - 4ms/step\n",
            "Epoch 20/9999\n",
            "2/2 - 0s - loss: 3.1169 - categorical_accuracy: 0.1154 - 8ms/epoch - 4ms/step\n",
            "Epoch 21/9999\n",
            "2/2 - 0s - loss: 3.0825 - categorical_accuracy: 0.0962 - 7ms/epoch - 4ms/step\n",
            "Epoch 22/9999\n",
            "2/2 - 0s - loss: 3.0455 - categorical_accuracy: 0.0962 - 10ms/epoch - 5ms/step\n",
            "Epoch 23/9999\n",
            "2/2 - 0s - loss: 3.0007 - categorical_accuracy: 0.1154 - 9ms/epoch - 5ms/step\n",
            "Epoch 24/9999\n",
            "2/2 - 0s - loss: 2.9489 - categorical_accuracy: 0.1154 - 8ms/epoch - 4ms/step\n",
            "Epoch 25/9999\n",
            "2/2 - 0s - loss: 2.8977 - categorical_accuracy: 0.1154 - 9ms/epoch - 4ms/step\n",
            "Epoch 26/9999\n",
            "2/2 - 0s - loss: 2.8478 - categorical_accuracy: 0.1154 - 8ms/epoch - 4ms/step\n",
            "Epoch 27/9999\n",
            "2/2 - 0s - loss: 2.8131 - categorical_accuracy: 0.1154 - 9ms/epoch - 5ms/step\n",
            "Epoch 28/9999\n",
            "2/2 - 0s - loss: 2.7392 - categorical_accuracy: 0.1154 - 9ms/epoch - 4ms/step\n",
            "Epoch 29/9999\n",
            "2/2 - 0s - loss: 2.7167 - categorical_accuracy: 0.1346 - 12ms/epoch - 6ms/step\n",
            "Epoch 30/9999\n",
            "2/2 - 0s - loss: 2.6533 - categorical_accuracy: 0.1346 - 9ms/epoch - 4ms/step\n",
            "Epoch 31/9999\n",
            "2/2 - 0s - loss: 2.6011 - categorical_accuracy: 0.1154 - 8ms/epoch - 4ms/step\n",
            "Epoch 32/9999\n",
            "2/2 - 0s - loss: 2.5446 - categorical_accuracy: 0.1346 - 8ms/epoch - 4ms/step\n",
            "Epoch 33/9999\n",
            "2/2 - 0s - loss: 2.4814 - categorical_accuracy: 0.1538 - 8ms/epoch - 4ms/step\n",
            "Epoch 34/9999\n",
            "2/2 - 0s - loss: 2.4200 - categorical_accuracy: 0.1346 - 8ms/epoch - 4ms/step\n",
            "Epoch 35/9999\n",
            "2/2 - 0s - loss: 2.3595 - categorical_accuracy: 0.1538 - 7ms/epoch - 3ms/step\n",
            "Epoch 36/9999\n",
            "2/2 - 0s - loss: 2.2838 - categorical_accuracy: 0.1731 - 9ms/epoch - 5ms/step\n",
            "Epoch 37/9999\n",
            "2/2 - 0s - loss: 2.2348 - categorical_accuracy: 0.2308 - 9ms/epoch - 5ms/step\n",
            "Epoch 38/9999\n",
            "2/2 - 0s - loss: 2.2012 - categorical_accuracy: 0.2308 - 11ms/epoch - 5ms/step\n",
            "Epoch 39/9999\n",
            "2/2 - 0s - loss: 2.0947 - categorical_accuracy: 0.3077 - 9ms/epoch - 5ms/step\n",
            "Epoch 40/9999\n",
            "2/2 - 0s - loss: 2.0422 - categorical_accuracy: 0.2885 - 9ms/epoch - 5ms/step\n",
            "Epoch 41/9999\n",
            "2/2 - 0s - loss: 1.9936 - categorical_accuracy: 0.3269 - 9ms/epoch - 5ms/step\n",
            "Epoch 42/9999\n",
            "2/2 - 0s - loss: 1.9310 - categorical_accuracy: 0.4231 - 8ms/epoch - 4ms/step\n",
            "Epoch 43/9999\n",
            "2/2 - 0s - loss: 1.8697 - categorical_accuracy: 0.3654 - 11ms/epoch - 6ms/step\n",
            "Epoch 44/9999\n",
            "2/2 - 0s - loss: 1.8149 - categorical_accuracy: 0.4231 - 9ms/epoch - 4ms/step\n",
            "Epoch 45/9999\n",
            "2/2 - 0s - loss: 1.8000 - categorical_accuracy: 0.3654 - 9ms/epoch - 4ms/step\n",
            "Epoch 46/9999\n",
            "2/2 - 0s - loss: 1.6963 - categorical_accuracy: 0.5000 - 9ms/epoch - 5ms/step\n",
            "Epoch 47/9999\n",
            "2/2 - 0s - loss: 1.6767 - categorical_accuracy: 0.4615 - 10ms/epoch - 5ms/step\n",
            "Epoch 48/9999\n",
            "2/2 - 0s - loss: 1.6058 - categorical_accuracy: 0.4808 - 10ms/epoch - 5ms/step\n",
            "Epoch 49/9999\n",
            "2/2 - 0s - loss: 1.5437 - categorical_accuracy: 0.5000 - 10ms/epoch - 5ms/step\n",
            "Epoch 50/9999\n",
            "2/2 - 0s - loss: 1.4948 - categorical_accuracy: 0.5769 - 12ms/epoch - 6ms/step\n",
            "Epoch 51/9999\n",
            "2/2 - 0s - loss: 1.4541 - categorical_accuracy: 0.5385 - 9ms/epoch - 5ms/step\n",
            "Epoch 52/9999\n",
            "2/2 - 0s - loss: 1.3996 - categorical_accuracy: 0.6346 - 8ms/epoch - 4ms/step\n",
            "Epoch 53/9999\n",
            "2/2 - 0s - loss: 1.3827 - categorical_accuracy: 0.6154 - 8ms/epoch - 4ms/step\n",
            "Epoch 54/9999\n",
            "2/2 - 0s - loss: 1.3407 - categorical_accuracy: 0.5577 - 9ms/epoch - 4ms/step\n",
            "Epoch 55/9999\n",
            "2/2 - 0s - loss: 1.2686 - categorical_accuracy: 0.6154 - 8ms/epoch - 4ms/step\n",
            "Epoch 56/9999\n",
            "2/2 - 0s - loss: 1.2138 - categorical_accuracy: 0.5769 - 7ms/epoch - 3ms/step\n",
            "Epoch 57/9999\n",
            "2/2 - 0s - loss: 1.1674 - categorical_accuracy: 0.6923 - 7ms/epoch - 3ms/step\n",
            "Epoch 58/9999\n",
            "2/2 - 0s - loss: 1.1306 - categorical_accuracy: 0.6346 - 7ms/epoch - 4ms/step\n",
            "Epoch 59/9999\n",
            "2/2 - 0s - loss: 1.1016 - categorical_accuracy: 0.7115 - 6ms/epoch - 3ms/step\n",
            "Epoch 60/9999\n",
            "2/2 - 0s - loss: 1.0547 - categorical_accuracy: 0.6731 - 7ms/epoch - 3ms/step\n",
            "Epoch 61/9999\n",
            "2/2 - 0s - loss: 1.0023 - categorical_accuracy: 0.7308 - 7ms/epoch - 4ms/step\n",
            "Epoch 62/9999\n",
            "2/2 - 0s - loss: 1.0575 - categorical_accuracy: 0.6923 - 9ms/epoch - 5ms/step\n",
            "Epoch 63/9999\n",
            "2/2 - 0s - loss: 1.0190 - categorical_accuracy: 0.5962 - 9ms/epoch - 5ms/step\n",
            "Epoch 64/9999\n",
            "2/2 - 0s - loss: 1.0251 - categorical_accuracy: 0.6731 - 8ms/epoch - 4ms/step\n",
            "Epoch 65/9999\n",
            "2/2 - 0s - loss: 0.9593 - categorical_accuracy: 0.7500 - 7ms/epoch - 4ms/step\n",
            "Epoch 66/9999\n",
            "2/2 - 0s - loss: 0.8875 - categorical_accuracy: 0.7308 - 13ms/epoch - 6ms/step\n",
            "Epoch 67/9999\n",
            "2/2 - 0s - loss: 0.8808 - categorical_accuracy: 0.7500 - 8ms/epoch - 4ms/step\n",
            "Epoch 68/9999\n",
            "2/2 - 0s - loss: 0.8540 - categorical_accuracy: 0.7308 - 10ms/epoch - 5ms/step\n",
            "Epoch 69/9999\n",
            "2/2 - 0s - loss: 0.8260 - categorical_accuracy: 0.7308 - 9ms/epoch - 5ms/step\n",
            "Epoch 70/9999\n",
            "2/2 - 0s - loss: 0.7947 - categorical_accuracy: 0.7692 - 7ms/epoch - 4ms/step\n",
            "Epoch 71/9999\n",
            "2/2 - 0s - loss: 0.7525 - categorical_accuracy: 0.8462 - 7ms/epoch - 4ms/step\n",
            "Epoch 72/9999\n",
            "2/2 - 0s - loss: 0.7438 - categorical_accuracy: 0.8077 - 6ms/epoch - 3ms/step\n",
            "Epoch 73/9999\n",
            "2/2 - 0s - loss: 0.7299 - categorical_accuracy: 0.8654 - 8ms/epoch - 4ms/step\n",
            "Epoch 74/9999\n",
            "2/2 - 0s - loss: 0.6615 - categorical_accuracy: 0.8462 - 8ms/epoch - 4ms/step\n",
            "Epoch 75/9999\n",
            "2/2 - 0s - loss: 0.6113 - categorical_accuracy: 0.9231 - 10ms/epoch - 5ms/step\n",
            "Epoch 76/9999\n",
            "2/2 - 0s - loss: 0.6101 - categorical_accuracy: 0.9038 - 9ms/epoch - 4ms/step\n",
            "Epoch 77/9999\n",
            "2/2 - 0s - loss: 0.6011 - categorical_accuracy: 0.9038 - 9ms/epoch - 4ms/step\n",
            "Epoch 78/9999\n",
            "2/2 - 0s - loss: 0.5684 - categorical_accuracy: 0.9615 - 9ms/epoch - 4ms/step\n",
            "Epoch 79/9999\n",
            "2/2 - 0s - loss: 0.5458 - categorical_accuracy: 0.9231 - 9ms/epoch - 4ms/step\n",
            "Epoch 80/9999\n",
            "2/2 - 0s - loss: 0.5170 - categorical_accuracy: 0.9423 - 9ms/epoch - 4ms/step\n",
            "Epoch 81/9999\n",
            "2/2 - 0s - loss: 0.5105 - categorical_accuracy: 0.9423 - 8ms/epoch - 4ms/step\n",
            "Epoch 82/9999\n",
            "2/2 - 0s - loss: 0.4826 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 83/9999\n",
            "2/2 - 0s - loss: 0.4334 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 84/9999\n",
            "2/2 - 0s - loss: 0.4189 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 85/9999\n",
            "2/2 - 0s - loss: 0.4027 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 86/9999\n",
            "2/2 - 0s - loss: 0.3773 - categorical_accuracy: 0.9423 - 7ms/epoch - 4ms/step\n",
            "Epoch 87/9999\n",
            "2/2 - 0s - loss: 0.3568 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 88/9999\n",
            "2/2 - 0s - loss: 0.3512 - categorical_accuracy: 0.9808 - 12ms/epoch - 6ms/step\n",
            "Epoch 89/9999\n",
            "2/2 - 0s - loss: 0.3290 - categorical_accuracy: 0.9615 - 9ms/epoch - 5ms/step\n",
            "Epoch 90/9999\n",
            "2/2 - 0s - loss: 0.3157 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 91/9999\n",
            "2/2 - 0s - loss: 0.2881 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 92/9999\n",
            "2/2 - 0s - loss: 0.2796 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 93/9999\n",
            "2/2 - 0s - loss: 0.2822 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 94/9999\n",
            "2/2 - 0s - loss: 0.3277 - categorical_accuracy: 0.9423 - 11ms/epoch - 5ms/step\n",
            "Epoch 95/9999\n",
            "2/2 - 0s - loss: 0.3194 - categorical_accuracy: 0.9423 - 10ms/epoch - 5ms/step\n",
            "Epoch 96/9999\n",
            "2/2 - 0s - loss: 0.2855 - categorical_accuracy: 0.9615 - 9ms/epoch - 5ms/step\n",
            "Epoch 97/9999\n",
            "2/2 - 0s - loss: 0.2344 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 98/9999\n",
            "2/2 - 0s - loss: 0.1936 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 99/9999\n",
            "2/2 - 0s - loss: 0.2207 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 100/9999\n",
            "2/2 - 0s - loss: 0.1940 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 101/9999\n",
            "2/2 - 0s - loss: 0.1877 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 102/9999\n",
            "2/2 - 0s - loss: 0.1825 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 103/9999\n",
            "2/2 - 0s - loss: 0.1525 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 104/9999\n",
            "2/2 - 0s - loss: 0.1430 - categorical_accuracy: 1.0000 - 6ms/epoch - 3ms/step\n",
            "Epoch 105/9999\n",
            "2/2 - 0s - loss: 0.1276 - categorical_accuracy: 0.9808 - 9ms/epoch - 5ms/step\n",
            "Epoch 106/9999\n",
            "2/2 - 0s - loss: 0.1149 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 107/9999\n",
            "2/2 - 0s - loss: 0.1271 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 108/9999\n",
            "2/2 - 0s - loss: 0.1006 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 109/9999\n",
            "2/2 - 0s - loss: 0.1156 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 110/9999\n",
            "2/2 - 0s - loss: 0.1356 - categorical_accuracy: 0.9615 - 8ms/epoch - 4ms/step\n",
            "Epoch 111/9999\n",
            "2/2 - 0s - loss: 0.1225 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 112/9999\n",
            "2/2 - 0s - loss: 0.1444 - categorical_accuracy: 0.9615 - 11ms/epoch - 5ms/step\n",
            "Epoch 113/9999\n",
            "2/2 - 0s - loss: 0.1443 - categorical_accuracy: 0.9808 - 7ms/epoch - 4ms/step\n",
            "Epoch 114/9999\n",
            "2/2 - 0s - loss: 0.1419 - categorical_accuracy: 0.9808 - 9ms/epoch - 5ms/step\n",
            "Epoch 115/9999\n",
            "2/2 - 0s - loss: 0.0799 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 116/9999\n",
            "2/2 - 0s - loss: 0.0979 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 117/9999\n",
            "2/2 - 0s - loss: 0.0734 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 118/9999\n",
            "2/2 - 0s - loss: 0.0610 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 119/9999\n",
            "2/2 - 0s - loss: 0.0619 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 120/9999\n",
            "2/2 - 0s - loss: 0.0518 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 121/9999\n",
            "2/2 - 0s - loss: 0.0519 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 122/9999\n",
            "2/2 - 0s - loss: 0.0440 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 123/9999\n",
            "2/2 - 0s - loss: 0.0418 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 124/9999\n",
            "2/2 - 0s - loss: 0.0375 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 125/9999\n",
            "2/2 - 0s - loss: 0.0349 - categorical_accuracy: 1.0000 - 7ms/epoch - 3ms/step\n",
            "Epoch 126/9999\n",
            "2/2 - 0s - loss: 0.0335 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 127/9999\n",
            "2/2 - 0s - loss: 0.0309 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 128/9999\n",
            "2/2 - 0s - loss: 0.0304 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 129/9999\n",
            "2/2 - 0s - loss: 0.0270 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 130/9999\n",
            "2/2 - 0s - loss: 0.0268 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 131/9999\n",
            "2/2 - 0s - loss: 0.0252 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 132/9999\n",
            "2/2 - 0s - loss: 0.0235 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 133/9999\n",
            "2/2 - 0s - loss: 0.0229 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f225152a250>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model3 = tf.keras.models.Sequential()\n",
        "model3.add(tf.keras.Input(shape=(256,)))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer1\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer2\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer3\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer4\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer5\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer6\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer7\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer8\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer9\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer10\"))\n",
        "model3.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
        "\n",
        "model3.summary()\n",
        "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "             metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
        "model3.fit(x=TRAINING_SET_2D, y=unicode_cat_train, epochs=9999, verbose=2, callbacks=[callback])\n",
        "#model3.evaluate(x=TEST_SET_2D, y=unicode_cat_test, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.evaluate(x=TEST_SET_2D, y=unicode_cat_test, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42NJoBIq-odm",
        "outputId": "9e64bc87-3620-4c93-eace-5af669a60a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 16ms/step - loss: 11.7844 - categorical_accuracy: 0.1538\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.784433364868164, 0.1538461595773697]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSGEDvpk6hBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b15825-1c4a-47fc-f7bb-b8ebb864ab48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.19115591e-24, 3.47288892e-10, 0.00000000e+00, 1.14879796e-04,\n",
              "        2.72376493e-18, 3.20587897e-11, 1.72486680e-03, 2.27490879e-15,\n",
              "        9.87545550e-01, 6.76126473e-18, 2.13170955e-26, 2.62273871e-03,\n",
              "        2.16314318e-24, 1.56285226e-23, 1.12422742e-27, 7.98841566e-03,\n",
              "        1.06680418e-26, 2.08743138e-14, 9.29345111e-18, 3.49724451e-06,\n",
              "        4.18243228e-27, 1.14926187e-24, 2.39768393e-29, 4.06823016e-23,\n",
              "        2.59739392e-37, 2.95056608e-27],\n",
              "       [7.04015722e-16, 2.36432629e-06, 1.01475889e-28, 2.93519793e-06,\n",
              "        4.16951362e-10, 3.39308535e-05, 1.14123663e-03, 4.06296004e-12,\n",
              "        2.41795387e-02, 1.74913964e-14, 5.81316764e-15, 6.62881757e-06,\n",
              "        1.00548751e-21, 3.12958958e-21, 1.86666292e-27, 9.74417329e-01,\n",
              "        2.05733993e-23, 1.08562153e-06, 8.60766197e-14, 2.14811691e-04,\n",
              "        1.30510347e-23, 5.33824245e-25, 3.94454303e-26, 9.88188976e-16,\n",
              "        1.67475106e-30, 2.27709224e-17],\n",
              "       [2.68326562e-06, 8.45216028e-03, 3.07506277e-11, 1.95274106e-06,\n",
              "        1.60003768e-03, 2.43041337e-01, 3.41998925e-03, 1.76371614e-07,\n",
              "        2.64590001e-03, 2.53982737e-08, 2.24685282e-05, 5.48131038e-06,\n",
              "        1.05118149e-12, 5.38926542e-13, 1.08600463e-16, 6.42110288e-01,\n",
              "        9.23011532e-14, 9.86047611e-02, 2.35945365e-07, 8.45557515e-05,\n",
              "        2.73318965e-13, 2.74127899e-15, 2.44746274e-15, 2.66230791e-06,\n",
              "        1.46131778e-16, 5.28377632e-06],\n",
              "       [1.50351605e-33, 3.00901373e-14, 0.00000000e+00, 1.03174784e-06,\n",
              "        1.16125002e-20, 3.24922008e-15, 1.24513200e-08, 1.70798321e-17,\n",
              "        5.55065237e-02, 1.30541308e-22, 3.09239391e-31, 9.44470227e-01,\n",
              "        1.66651832e-29, 1.93510893e-26, 9.24302596e-33, 1.59067131e-05,\n",
              "        2.50225076e-33, 2.01359982e-19, 4.26091467e-22, 6.36033474e-06,\n",
              "        2.12249185e-32, 4.36360806e-29, 1.04395653e-34, 1.16624844e-28,\n",
              "        0.00000000e+00, 3.78855429e-35],\n",
              "       [2.22564382e-20, 9.13892819e-08, 6.06030061e-34, 1.72654114e-14,\n",
              "        3.60805047e-04, 1.35112941e-01, 1.98040362e-10, 7.55200278e-14,\n",
              "        1.99392729e-04, 8.31818614e-19, 5.30779023e-12, 1.69692180e-06,\n",
              "        1.22064368e-31, 1.35320508e-28, 0.00000000e+00, 8.13487411e-01,\n",
              "        3.45711488e-34, 7.36104721e-06, 2.83571014e-17, 5.08302636e-02,\n",
              "        6.89049956e-33, 1.18158864e-35, 1.60927157e-35, 5.09835060e-16,\n",
              "        1.35637790e-36, 1.29818930e-16],\n",
              "       [6.13678840e-24, 5.71823387e-11, 0.00000000e+00, 2.63727816e-15,\n",
              "        2.14431917e-09, 2.61106092e-04, 5.47335111e-09, 1.69764213e-17,\n",
              "        1.62421970e-03, 1.58062987e-21, 2.97131367e-19, 2.77734870e-07,\n",
              "        3.06172829e-35, 2.40878952e-32, 0.00000000e+00, 7.40995288e-01,\n",
              "        3.81977998e-37, 5.46315826e-10, 2.24243753e-19, 2.57119179e-01,\n",
              "        2.67259499e-36, 0.00000000e+00, 0.00000000e+00, 5.06579705e-21,\n",
              "        0.00000000e+00, 1.35347491e-22],\n",
              "       [1.18348767e-17, 1.10901135e-06, 1.09369264e-30, 1.44537617e-05,\n",
              "        9.66950409e-10, 2.20635393e-05, 1.29792286e-04, 9.01379041e-11,\n",
              "        2.52777427e-01, 6.99012720e-14, 2.08081500e-15, 5.78312622e-03,\n",
              "        6.08173075e-21, 1.48029670e-19, 1.33596084e-25, 7.06248820e-01,\n",
              "        2.60940862e-23, 9.83819248e-08, 1.68846233e-13, 3.50231715e-02,\n",
              "        5.01384719e-23, 1.40938978e-23, 4.40477743e-25, 4.95703957e-16,\n",
              "        5.12232546e-29, 2.90965847e-18],\n",
              "       [6.51911370e-23, 8.99676583e-11, 4.48574789e-24, 2.70715941e-06,\n",
              "        2.84906551e-23, 2.70995233e-30, 4.08700454e-22, 4.11837436e-02,\n",
              "        2.76341099e-23, 7.58233000e-06, 2.35230949e-16, 3.49349882e-08,\n",
              "        7.06963316e-02, 8.61401320e-01, 3.23807944e-05, 1.65727276e-25,\n",
              "        1.18141472e-02, 2.36325418e-17, 8.42300119e-11, 7.57673842e-24,\n",
              "        8.91332235e-03, 6.58205943e-04, 4.50622663e-03, 5.04658648e-10,\n",
              "        7.83898635e-04, 6.09366078e-30],\n",
              "       [1.35385773e-29, 6.31408920e-14, 0.00000000e+00, 6.29255814e-10,\n",
              "        1.33511838e-20, 2.62565859e-12, 6.84623774e-06, 9.31430702e-20,\n",
              "        9.98862028e-01, 2.23614662e-23, 4.07957148e-31, 3.02468135e-04,\n",
              "        1.19422726e-31, 3.25006576e-29, 1.01091952e-34, 7.63944641e-04,\n",
              "        8.91259541e-34, 1.27762603e-17, 1.53250203e-21, 6.47493434e-05,\n",
              "        2.87157038e-34, 5.84083865e-32, 1.71032760e-36, 2.32342860e-27,\n",
              "        0.00000000e+00, 9.40820246e-32],\n",
              "       [1.07392752e-08, 1.77926111e-07, 1.32021540e-08, 5.67279912e-08,\n",
              "        8.40186704e-18, 2.67178640e-19, 1.65386842e-13, 2.64846450e-10,\n",
              "        6.94838531e-17, 9.00063396e-01, 1.06637908e-16, 8.70847058e-12,\n",
              "        3.68911028e-03, 5.56593172e-08, 5.39448150e-02, 3.59457192e-17,\n",
              "        3.62219794e-06, 5.06976893e-13, 3.78229916e-02, 3.48512801e-21,\n",
              "        4.23004990e-03, 2.31485959e-04, 2.57755506e-08, 1.51111055e-06,\n",
              "        1.26062560e-05, 2.72243884e-17],\n",
              "       [8.02913712e-15, 8.91907621e-05, 7.58236240e-25, 6.52890408e-10,\n",
              "        1.38987944e-01, 4.28215504e-01, 5.38056000e-09, 1.23691937e-08,\n",
              "        5.93763543e-04, 3.88817694e-13, 1.06278469e-06, 3.29995761e-04,\n",
              "        4.05587470e-22, 1.11799017e-19, 6.49506007e-28, 3.55954736e-01,\n",
              "        1.59243328e-24, 1.77093816e-03, 1.27317579e-12, 7.40568116e-02,\n",
              "        3.36876612e-23, 2.90557316e-25, 3.13766146e-25, 1.19911137e-10,\n",
              "        1.52449120e-25, 6.08168238e-11],\n",
              "       [2.61440468e-33, 4.95293910e-14, 0.00000000e+00, 7.47387485e-10,\n",
              "        2.72665464e-16, 1.12580708e-13, 2.05951331e-13, 6.90464571e-16,\n",
              "        1.34029760e-04, 2.66718441e-22, 5.70607252e-26, 9.98396575e-01,\n",
              "        1.26242855e-30, 3.67951042e-26, 1.02795948e-34, 8.23763060e-07,\n",
              "        5.34538929e-34, 1.33277801e-18, 9.46721439e-22, 1.46859151e-03,\n",
              "        1.54765527e-32, 3.83213744e-31, 2.36212399e-34, 4.87095293e-26,\n",
              "        0.00000000e+00, 1.34184899e-32],\n",
              "       [7.08350711e-09, 9.05807479e-04, 1.58554302e-14, 2.08928896e-09,\n",
              "        1.46618903e-01, 7.08508968e-01, 4.41978671e-07, 5.19029122e-08,\n",
              "        5.23047311e-05, 1.55455593e-10, 1.32626877e-03, 1.88441015e-06,\n",
              "        3.92491166e-16, 1.78240804e-15, 5.09155759e-21, 1.49009926e-02,\n",
              "        1.00506671e-17, 1.27618060e-01, 5.58476487e-10, 5.70547018e-05,\n",
              "        5.19074231e-17, 1.03981856e-18, 1.02638008e-18, 7.64566266e-07,\n",
              "        1.96216464e-18, 8.47743286e-06],\n",
              "       [4.06785584e-05, 1.53622008e-03, 7.81123777e-11, 6.62326634e-01,\n",
              "        1.26828141e-13, 7.88109880e-14, 1.61671062e-06, 4.73119144e-07,\n",
              "        3.54927442e-11, 3.32667649e-01, 2.38271447e-12, 1.08953202e-09,\n",
              "        2.67304061e-03, 1.18191046e-07, 6.78650977e-05, 1.07812460e-10,\n",
              "        7.45741199e-05, 1.57262178e-07, 4.14291972e-05, 1.26878759e-16,\n",
              "        4.78318514e-04, 9.07763897e-05, 4.00448528e-08, 3.81621390e-07,\n",
              "        7.05022374e-10, 1.35063496e-13],\n",
              "       [8.26138047e-10, 1.58750336e-03, 3.24998823e-19, 9.86266613e-01,\n",
              "        3.04536403e-15, 5.51146698e-11, 1.19087966e-02, 5.37876987e-09,\n",
              "        1.75868976e-04, 5.37945343e-05, 2.82432962e-18, 4.25297594e-06,\n",
              "        8.54506776e-09, 4.31726009e-12, 1.99060359e-08, 3.92374034e-07,\n",
              "        1.95755842e-10, 6.18917584e-10, 2.82670226e-06, 2.18391191e-12,\n",
              "        8.75366168e-10, 2.58793538e-08, 7.78258317e-13, 3.40802408e-12,\n",
              "        1.48858480e-17, 1.42529159e-17],\n",
              "       [2.62561999e-24, 2.72126245e-11, 0.00000000e+00, 8.48677850e-10,\n",
              "        2.85335860e-16, 1.34934393e-08, 1.00728284e-04, 1.98210276e-17,\n",
              "        9.49563384e-01, 1.27794946e-20, 2.66228032e-25, 3.49499423e-05,\n",
              "        1.27233492e-29, 1.73412908e-27, 9.12547468e-34, 4.78143729e-02,\n",
              "        2.77073375e-31, 3.71550799e-13, 6.04945151e-19, 2.48659635e-03,\n",
              "        8.53097334e-32, 2.94819320e-31, 4.04644002e-34, 4.35636728e-23,\n",
              "        0.00000000e+00, 5.59504343e-26],\n",
              "       [2.39506504e-03, 1.54962176e-02, 3.86332477e-09, 4.33248788e-01,\n",
              "        8.19474100e-13, 1.01184252e-11, 3.66060733e-04, 1.03761359e-07,\n",
              "        3.57722585e-09, 5.45607626e-01, 6.99496234e-13, 4.87326679e-10,\n",
              "        7.14671682e-04, 7.88873233e-09, 1.34677772e-04, 2.05355422e-09,\n",
              "        1.80300427e-04, 6.01551051e-07, 1.37473678e-03, 2.79858119e-15,\n",
              "        3.71973234e-04, 1.08379405e-04, 3.16593720e-08, 8.30461829e-07,\n",
              "        1.52995325e-10, 2.26940884e-12],\n",
              "       [1.95015724e-29, 2.12650413e-13, 0.00000000e+00, 2.03696435e-10,\n",
              "        1.24795705e-19, 7.57199164e-11, 8.79060735e-06, 6.82757915e-20,\n",
              "        9.86599803e-01, 6.92165728e-24, 3.27884194e-30, 5.69794647e-05,\n",
              "        6.01737303e-34, 2.53088024e-31, 1.84761048e-38, 1.27593819e-02,\n",
              "        3.00806791e-36, 3.83094050e-16, 2.28795360e-22, 5.74987265e-04,\n",
              "        1.02408407e-36, 1.65473100e-35, 0.00000000e+00, 1.17299986e-27,\n",
              "        0.00000000e+00, 1.98629863e-31],\n",
              "       [8.86674345e-01, 5.98094352e-02, 3.05263754e-02, 1.09387787e-04,\n",
              "        2.87618533e-08, 6.81007293e-07, 1.44511682e-03, 3.02325596e-08,\n",
              "        9.12913762e-08, 1.85327290e-03, 7.12811357e-07, 5.44806734e-11,\n",
              "        3.32972486e-05, 3.57780294e-09, 2.76914651e-08, 9.14533302e-07,\n",
              "        4.41451675e-06, 1.00937521e-03, 4.99131624e-03, 1.19370345e-12,\n",
              "        3.70411253e-05, 5.08930640e-08, 1.28185629e-09, 1.34891300e-02,\n",
              "        5.35024425e-09, 1.49079906e-05],\n",
              "       [7.72044899e-08, 1.38134731e-03, 1.66755357e-12, 2.45693105e-10,\n",
              "        2.52562642e-01, 2.62793958e-01, 3.92780137e-08, 3.45810385e-08,\n",
              "        9.40051734e-07, 9.36979869e-11, 4.95268404e-02, 5.18176151e-08,\n",
              "        4.72762419e-15, 2.90013176e-15, 2.34851181e-20, 2.01041199e-04,\n",
              "        2.32208425e-17, 4.33009237e-01, 2.62796535e-10, 4.52527388e-07,\n",
              "        2.42457358e-16, 8.35885779e-18, 3.09121159e-18, 5.72221415e-06,\n",
              "        3.23710548e-17, 5.17660519e-04],\n",
              "       [8.94028883e-13, 4.40880648e-08, 1.92862965e-13, 2.02892370e-05,\n",
              "        1.95232306e-18, 1.29492590e-22, 9.25325155e-16, 1.96095314e-07,\n",
              "        4.98309276e-18, 3.38974923e-01, 2.00832901e-16, 6.35102859e-09,\n",
              "        2.14838922e-01, 1.37973053e-04, 4.09653038e-01, 6.22359578e-19,\n",
              "        1.49093772e-04, 1.35373673e-13, 2.34767678e-04, 2.61899882e-21,\n",
              "        2.24611703e-02, 1.31995333e-02, 2.72673742e-06, 2.45930465e-07,\n",
              "        3.27097747e-04, 1.32455882e-20],\n",
              "       [3.19221094e-08, 4.71201129e-05, 5.40563185e-08, 2.66329822e-04,\n",
              "        1.01572882e-10, 2.56871537e-13, 5.92039306e-10, 2.92768003e-03,\n",
              "        9.57764493e-11, 1.34598374e-01, 1.20625501e-08, 6.64523805e-06,\n",
              "        1.57294378e-01, 1.81959718e-02, 1.74935479e-02, 2.12760157e-11,\n",
              "        1.27858013e-01, 9.68816938e-09, 3.13984160e-03, 1.99782108e-10,\n",
              "        2.36089095e-01, 1.50295198e-02, 6.48199301e-03, 2.28024716e-03,\n",
              "        2.78291196e-01, 7.50548408e-12],\n",
              "       [4.33240776e-17, 1.21000196e-10, 1.03911514e-18, 3.17044413e-09,\n",
              "        1.59676255e-23, 6.76146529e-28, 9.84403349e-20, 7.49520357e-09,\n",
              "        1.80210872e-23, 7.91885518e-03, 9.49674530e-22, 1.54230309e-10,\n",
              "        2.72487034e-03, 6.71898169e-06, 7.09189355e-01, 2.60327211e-26,\n",
              "        1.14657050e-05, 1.58416602e-17, 4.06474754e-07, 1.47899759e-29,\n",
              "        1.56407878e-02, 2.64492661e-01, 1.41048340e-05, 7.94694380e-11,\n",
              "        6.64344839e-07, 6.50707739e-28],\n",
              "       [4.04761694e-02, 5.66442728e-01, 8.56388069e-04, 3.79125595e-05,\n",
              "        8.63375972e-05, 7.65875229e-05, 1.88674258e-05, 1.01349551e-05,\n",
              "        9.59788395e-07, 4.43430239e-04, 4.08558128e-03, 5.00622654e-08,\n",
              "        5.50881123e-05, 7.07661414e-08, 6.06044326e-09, 5.45463308e-06,\n",
              "        6.13686041e-07, 1.93070117e-02, 4.92852530e-04, 3.02760284e-09,\n",
              "        2.13239036e-05, 6.46095302e-08, 1.64531644e-09, 3.52072388e-01,\n",
              "        2.08886206e-07, 1.55097358e-02],\n",
              "       [1.01490545e-15, 8.91348151e-08, 6.05590970e-16, 4.94682672e-06,\n",
              "        4.65299451e-17, 1.35064458e-21, 5.26949996e-16, 9.78193954e-02,\n",
              "        4.56042208e-17, 4.17154573e-04, 6.63620244e-12, 2.57544542e-07,\n",
              "        5.17175682e-02, 2.75112092e-01, 1.62755474e-04, 6.20753400e-19,\n",
              "        3.41612130e-01, 3.31802782e-13, 2.60171674e-07, 1.13957457e-16,\n",
              "        1.15627132e-01, 2.83146580e-03, 5.92640154e-02, 3.00099191e-06,\n",
              "        5.54276295e-02, 6.06854392e-21],\n",
              "       [1.41401203e-12, 4.17038427e-05, 3.09527334e-20, 2.26717733e-13,\n",
              "        2.64758587e-01, 7.06058621e-01, 2.54507138e-10, 9.95006230e-11,\n",
              "        7.75174101e-07, 1.47653530e-14, 1.85063138e-04, 4.03071283e-08,\n",
              "        3.94883549e-22, 3.95351014e-21, 1.37081795e-28, 7.52234773e-04,\n",
              "        6.88514100e-25, 2.81986371e-02, 1.42257869e-13, 4.24262180e-06,\n",
              "        1.26952056e-23, 2.12097383e-25, 1.16203821e-25, 3.48121110e-09,\n",
              "        6.85374779e-25, 1.36441159e-07]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "predicted = model3.predict(TEST_SET_2D)\n",
        "predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0JsYeXi6h2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32455928-2599-4fa8-8d11-6c8aab09e7c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A', 'B'],\n",
              " ['B', 'R'],\n",
              " ['C', 'E'],\n",
              " ['D', 'L'],\n",
              " ['E', 'F'],\n",
              " ['F', 'K'],\n",
              " ['G', 'E'],\n",
              " ['H', 'P'],\n",
              " ['I', 'T'],\n",
              " ['J', 'J'],\n",
              " ['K', 'I'],\n",
              " ['L', 'R'],\n",
              " ['M', 'B'],\n",
              " ['N', 'E'],\n",
              " ['O', 'A'],\n",
              " ['P', 'P'],\n",
              " ['Q', 'B'],\n",
              " ['R', 'P'],\n",
              " ['S', 'G'],\n",
              " ['T', 'M'],\n",
              " ['U', 'A'],\n",
              " ['V', 'D'],\n",
              " ['W', 'O'],\n",
              " ['X', 'Y'],\n",
              " ['Y', 'J'],\n",
              " ['Z', 'E']]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "result_list = []\n",
        "for num in range(len(predicted)):\n",
        "    max_val = 0\n",
        "    actual = chr(num+ord('A'))\n",
        "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    result_list.append([actual, predict])\n",
        "result_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3qg5Hv66nmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1f8243-a6f0-4f99-8230-891ebcbc966b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images correctly classified: 30.76923076923077%\n",
            "Here are the list of test images that are misclassified and how they appear:\n",
            "Actual    Predicted\n",
            "A         D\n",
            "D         B\n",
            "E         K\n",
            "H         R\n",
            "L         B\n",
            "M         K\n",
            "N         R\n",
            "O         G\n",
            "Q         D\n",
            "R         S\n",
            "S         B\n",
            "T         S\n",
            "U         Z\n",
            "V         L\n",
            "W         X\n",
            "X         T\n",
            "Y         Q\n",
            "Z         S\n"
          ]
        }
      ],
      "source": [
        "list_misclassified = []\n",
        "num_total, num_correct = 26, 26\n",
        "for i in range(len(result_list)):\n",
        "    if result_list[i][0] != result_list[i][1]:\n",
        "        num_correct -= 1\n",
        "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
        "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_misclassified) == 0:\n",
        "    print(\"All test images are classified correctly.\")\n",
        "else:\n",
        "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_misclassified)):\n",
        "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use show() to display the misclassified images.\n",
        "for j in range(len(list_misclassified)):\n",
        "    # Subtract unicode value of current letter from uppercase A to obtain the letter's position.\n",
        "    letter_train = ord(list_misclassified[j][0]) - ord('A')\n",
        "    letter_test = ord(list_misclassified[j][1]) - ord('A')\n",
        "    print(\"================================================\")\n",
        "    print(\"Expected letter:\")\n",
        "    show(TRAINING_SET_2D[letter_train])\n",
        "    print(\"Predicted letter:\")\n",
        "    show(TEST_SET_2D[letter_test])\n",
        "    print(\"================================================\")"
      ],
      "metadata": {
        "id": "hEshf3ZODHnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dec0a5-339b-4e98-a778-cc2aa658b766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================\n",
            "Expected letter:\n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ########      \n",
            "  ########      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "##########      \n",
            "##########      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "##########      \n",
            "##########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##########    \n",
            "  ##########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "##############  \n",
            "##############  \n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "  ####  ##      \n",
            "  ####  ##      \n",
            "  ########      \n",
            "  ########      \n",
            "  ####  ##      \n",
            "  ####  ##      \n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "##############  \n",
            "##############  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ####        \n",
            "    ####        \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ####      ####\n",
            "  ####      ####\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ######    ##  \n",
            "  ######    ##  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "########        \n",
            "########        \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####          \n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "##############  \n",
            "##############  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##########    \n",
            "  ##########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "######  ######  \n",
            "######  ######  \n",
            "##############  \n",
            "##############  \n",
            "##############  \n",
            "##############  \n",
            "####  ##  ####  \n",
            "####  ##  ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ####      ##  \n",
            "  ####      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ####        \n",
            "    ####        \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ####      ####\n",
            "  ####      ####\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "######    ####  \n",
            "######    ####  \n",
            "########  ####  \n",
            "########  ####  \n",
            "####  ########  \n",
            "####  ########  \n",
            "####    ######  \n",
            "####    ######  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##  ##      \n",
            "    ##  ##      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ######    ##  \n",
            "  ######    ##  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "    ######      \n",
            "    ######      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "    ######      \n",
            "    ######      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "      ######    \n",
            "      ######    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##            \n",
            "  ##    ######  \n",
            "  ##    ######  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "      ########  \n",
            "      ########  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####  ######    \n",
            "####  ######    \n",
            "  ########      \n",
            "  ########      \n",
            "      ######    \n",
            "      ######    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ########      \n",
            "  ########      \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##    ##    \n",
            "    ##    ##    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "############    \n",
            "############    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "  ##########    \n",
            "  ##########    \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "######    ####  \n",
            "######    ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##            \n",
            "  ##            \n",
            "    ########    \n",
            "    ########    \n",
            "            ##  \n",
            "            ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "    ########    \n",
            "    ########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "######          \n",
            "######          \n",
            "  ######        \n",
            "  ######        \n",
            "      ######    \n",
            "      ######    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##########    \n",
            "  ##########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ########    \n",
            "    ########    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##########    \n",
            "  ##########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "############    \n",
            "############    \n",
            "##  ####  ##    \n",
            "##  ####  ##    \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##            \n",
            "  ##            \n",
            "    ########    \n",
            "    ########    \n",
            "            ##  \n",
            "            ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "    ########    \n",
            "    ########    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##############\n",
            "  ##############\n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "          ##    \n",
            "          ##    \n",
            "        ##      \n",
            "        ##      \n",
            "      ##        \n",
            "      ##        \n",
            "    ##        ##\n",
            "    ##        ##\n",
            "  ##############\n",
            "  ##############\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "    ####        \n",
            "    ####        \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ######        \n",
            "  ######        \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##          \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ############  \n",
            "  ############  \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####  ##  ####  \n",
            "####  ##  ####  \n",
            "##############  \n",
            "##############  \n",
            "######  ######  \n",
            "######  ######  \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##          ##\n",
            "  ##          ##\n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "      ##  ##    \n",
            "      ##  ##    \n",
            "        ##      \n",
            "        ##      \n",
            "      ##  ##    \n",
            "      ##  ##    \n",
            "    ##      ##  \n",
            "    ##      ##  \n",
            "  ##          ##\n",
            "  ##          ##\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "####      ####  \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "    ######      \n",
            "    ######      \n",
            "    ######      \n",
            "    ######      \n",
            "  ####  ####    \n",
            "  ####  ####    \n",
            "####      ####  \n",
            "####      ####  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "  ##############\n",
            "  ##############\n",
            "  ##    ##    ##\n",
            "  ##    ##    ##\n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "      ######    \n",
            "      ######    \n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "  ########      \n",
            "  ########      \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##    ##  ##  \n",
            "  ##    ##  ##  \n",
            "    ########    \n",
            "    ########    \n",
            "            ####\n",
            "            ####\n",
            "                \n",
            "                \n",
            "================================================\n",
            "================================================\n",
            "Expected letter:\n",
            "##############  \n",
            "##############  \n",
            "####      ####  \n",
            "####      ####  \n",
            "##      ####    \n",
            "##      ####    \n",
            "      ####      \n",
            "      ####      \n",
            "    ####    ##  \n",
            "    ####    ##  \n",
            "  ####    ####  \n",
            "  ####    ####  \n",
            "##############  \n",
            "##############  \n",
            "                \n",
            "                \n",
            "Predicted letter:\n",
            "    ########    \n",
            "    ########    \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "  ##            \n",
            "  ##            \n",
            "    ########    \n",
            "    ########    \n",
            "            ##  \n",
            "            ##  \n",
            "  ##        ##  \n",
            "  ##        ##  \n",
            "    ########    \n",
            "    ########    \n",
            "                \n",
            "                \n",
            "================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU1DX5QZ6oyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ace2f45-d069-48fa-caa5-760473a19525"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.03253190e-08, 1.35254313e-03, 1.19270392e-12, 2.86561830e-10,\n",
              "        3.88984323e-01, 2.17499882e-01, 1.97457339e-08, 5.73914356e-08,\n",
              "        8.01802059e-07, 1.10944420e-10, 6.88879266e-02, 9.78031736e-08,\n",
              "        7.11033422e-15, 5.47312456e-15, 4.71556086e-20, 1.36504867e-04,\n",
              "        2.70760285e-17, 3.22608203e-01, 2.70247852e-10, 5.96115854e-07,\n",
              "        3.44538169e-16, 1.60512578e-17, 5.58285206e-18, 6.06653839e-06,\n",
              "        6.78174547e-17, 5.22957824e-04],\n",
              "       [3.35512413e-22, 2.57635774e-10, 3.47190085e-23, 2.65994595e-06,\n",
              "        2.74391445e-22, 4.59070770e-29, 1.55936208e-21, 1.01727620e-01,\n",
              "        2.11786712e-22, 6.77385742e-06, 2.78586955e-15, 3.70148392e-08,\n",
              "        4.91247959e-02, 8.09161663e-01, 1.06896505e-05, 1.33921060e-24,\n",
              "        2.18711719e-02, 8.48398126e-17, 1.21497673e-10, 2.30808811e-22,\n",
              "        9.15256981e-03, 3.38473474e-04, 6.56698504e-03, 2.08188333e-09,\n",
              "        2.03650608e-03, 1.51672255e-28],\n",
              "       [1.00777507e-13, 1.62162160e-05, 1.26290075e-20, 8.88506730e-15,\n",
              "        9.47771192e-01, 3.55666457e-03, 2.97769181e-15, 3.13855775e-10,\n",
              "        4.20145446e-11, 1.65413909e-15, 4.14822660e-02, 6.13535889e-10,\n",
              "        2.78669818e-21, 1.70367024e-20, 2.33647056e-28, 3.28014309e-08,\n",
              "        1.70308118e-25, 7.17164623e-03, 1.59009662e-15, 3.35033862e-10,\n",
              "        2.44258931e-23, 8.64975234e-25, 1.79097914e-25, 9.24011179e-09,\n",
              "        1.68982936e-23, 2.02194246e-06],\n",
              "       [4.68595271e-15, 9.38954599e-07, 2.32575621e-24, 9.27522338e-15,\n",
              "        3.93816130e-03, 9.52746630e-01, 2.30386177e-09, 2.70229612e-13,\n",
              "        1.88755130e-05, 1.33377236e-16, 2.11322000e-08, 2.57516444e-08,\n",
              "        1.22817885e-26, 5.37752921e-25, 4.29259203e-33, 4.22367491e-02,\n",
              "        1.60582245e-28, 8.32441903e-04, 7.67794031e-15, 2.26058895e-04,\n",
              "        8.76157050e-28, 5.31764417e-30, 6.81575849e-30, 4.83022910e-12,\n",
              "        4.85290185e-30, 3.06665908e-11],\n",
              "       [1.57933498e-34, 3.72104247e-16, 0.00000000e+00, 3.20630827e-12,\n",
              "        8.85030244e-23, 1.89158205e-13, 1.61767701e-07, 1.02939028e-22,\n",
              "        9.99341667e-01, 2.52428961e-27, 4.40517197e-35, 9.72857233e-05,\n",
              "        1.43127269e-37, 3.35551406e-34, 0.00000000e+00, 3.36942467e-04,\n",
              "        0.00000000e+00, 6.18703052e-20, 3.71036635e-25, 2.23988915e-04,\n",
              "        0.00000000e+00, 2.98308389e-38, 0.00000000e+00, 2.13054754e-31,\n",
              "        0.00000000e+00, 2.43779154e-36],\n",
              "       [0.00000000e+00, 1.65360900e-26, 0.00000000e+00, 2.16358662e-25,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.45112795e-13,\n",
              "        0.00000000e+00, 4.54720872e-10, 0.00000000e+00, 7.66245367e-24,\n",
              "        1.47828030e-06, 1.51408006e-08, 1.24764774e-04, 0.00000000e+00,\n",
              "        2.58816293e-08, 0.00000000e+00, 1.75259034e-21, 0.00000000e+00,\n",
              "        5.61144343e-03, 9.94220257e-01, 4.20193937e-05, 9.55285226e-26,\n",
              "        6.68845811e-14, 0.00000000e+00],\n",
              "       [2.58538067e-21, 5.61646836e-08, 1.48822090e-35, 5.74737130e-14,\n",
              "        1.28731495e-04, 3.88672464e-02, 1.48257920e-10, 1.39609786e-13,\n",
              "        3.01105931e-04, 7.32425005e-19, 9.54218150e-13, 6.88356067e-06,\n",
              "        8.85626907e-32, 1.48102809e-28, 0.00000000e+00, 8.23491991e-01,\n",
              "        1.53111709e-34, 1.62022457e-06, 1.71379068e-17, 1.37202337e-01,\n",
              "        4.76865659e-33, 8.45701461e-36, 9.50767941e-36, 9.11918517e-17,\n",
              "        5.57834632e-37, 1.03593560e-17],\n",
              "       [7.37193550e-05, 9.51316476e-01, 1.53744545e-10, 1.37380525e-07,\n",
              "        7.59434727e-07, 5.52500580e-07, 3.33384631e-09, 1.81873128e-09,\n",
              "        1.14206778e-11, 6.69911859e-09, 1.25565857e-03, 1.29514522e-14,\n",
              "        7.56890644e-12, 6.91886899e-16, 1.21971877e-20, 8.34711678e-09,\n",
              "        1.03229758e-15, 4.68146317e-02, 4.24518687e-09, 2.13919064e-17,\n",
              "        1.98374086e-13, 3.17395157e-18, 6.70966908e-20, 5.16119238e-04,\n",
              "        3.45205383e-18, 2.19818812e-05],\n",
              "       [2.15428735e-17, 4.74398689e-12, 3.32711688e-18, 2.76177275e-10,\n",
              "        6.12722515e-29, 1.32268856e-33, 3.56911218e-23, 1.12841025e-13,\n",
              "        9.30524030e-28, 2.46755436e-01, 5.73918457e-27, 3.62259474e-15,\n",
              "        2.66458630e-03, 3.89049415e-09, 7.48075366e-01, 1.43914573e-29,\n",
              "        5.17759879e-08, 3.44975149e-21, 3.19241481e-05, 8.36098080e-35,\n",
              "        1.70742127e-03, 7.65201868e-04, 3.42800760e-10, 1.64217650e-11,\n",
              "        8.24957525e-08, 3.14134220e-31],\n",
              "       [4.36559245e-02, 3.01641911e-01, 1.63737230e-03, 4.84826342e-06,\n",
              "        6.73577379e-05, 8.86651324e-05, 5.62495279e-06, 2.73035698e-06,\n",
              "        4.82452549e-07, 3.73888237e-04, 2.85886670e-03, 3.25618963e-08,\n",
              "        4.34835347e-05, 2.42393394e-08, 5.99134298e-09, 2.39589076e-06,\n",
              "        3.76304598e-07, 1.13359643e-02, 4.37159266e-04, 3.13311843e-09,\n",
              "        1.81493288e-05, 6.62111006e-08, 8.16521850e-10, 6.00328684e-01,\n",
              "        3.26072211e-07, 3.74956727e-02],\n",
              "       [0.00000000e+00, 1.00670186e-18, 0.00000000e+00, 2.07466531e-13,\n",
              "        1.53072111e-26, 4.62351664e-16, 6.96298175e-09, 5.53564466e-26,\n",
              "        9.99772608e-01, 1.36712603e-31, 0.00000000e+00, 1.25740873e-04,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.36533659e-05,\n",
              "        0.00000000e+00, 1.96292527e-23, 3.12001674e-29, 3.80190249e-05,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.73151652e-37,\n",
              "        0.00000000e+00, 0.00000000e+00],\n",
              "       [4.20847696e-33, 2.93656074e-15, 1.27250891e-34, 7.84483056e-09,\n",
              "        1.00412592e-33, 0.00000000e+00, 2.14552419e-32, 1.46482084e-02,\n",
              "        5.36838765e-34, 7.59557466e-08, 1.80754113e-23, 6.49852394e-12,\n",
              "        4.33025211e-02, 9.36560452e-01, 2.25836345e-07, 3.54077787e-37,\n",
              "        3.01602716e-03, 3.60686336e-25, 3.74910462e-15, 2.30316169e-34,\n",
              "        1.92994066e-03, 2.35720727e-05, 4.08855907e-04, 6.43913295e-14,\n",
              "        1.10028115e-04, 0.00000000e+00],\n",
              "       [2.58898586e-01, 1.15466712e-03, 3.39876395e-04, 2.66820210e-09,\n",
              "        2.89245787e-13, 4.37477502e-06, 7.39430010e-01, 4.72756087e-16,\n",
              "        3.92828241e-08, 7.52704743e-09, 2.34786886e-12, 2.80002292e-18,\n",
              "        1.17112139e-14, 8.09601021e-20, 5.60152817e-20, 1.26021105e-05,\n",
              "        9.31146919e-16, 1.50578082e-04, 8.84550172e-06, 5.20199908e-18,\n",
              "        5.56328800e-14, 1.45588854e-19, 2.12500567e-20, 4.38729046e-07,\n",
              "        4.52520907e-21, 1.07636222e-09],\n",
              "       [0.00000000e+00, 6.72385593e-27, 0.00000000e+00, 7.34395582e-25,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.04771573e-02,\n",
              "        0.00000000e+00, 2.75790169e-16, 0.00000000e+00, 6.89272501e-24,\n",
              "        1.56791842e-07, 7.32604647e-03, 5.58612003e-15, 0.00000000e+00,\n",
              "        1.78617220e-02, 0.00000000e+00, 6.67437811e-28, 0.00000000e+00,\n",
              "        4.05757164e-04, 4.07080535e-07, 9.63928819e-01, 3.31662999e-23,\n",
              "        1.75838082e-08, 0.00000000e+00],\n",
              "       [1.01265093e-35, 1.38539504e-16, 0.00000000e+00, 3.29798688e-12,\n",
              "        2.14091848e-23, 5.70408046e-14, 8.35953031e-08, 3.95340514e-23,\n",
              "        9.99394536e-01, 5.48506100e-28, 4.77511074e-36, 1.48868436e-04,\n",
              "        1.90883405e-38, 6.50207940e-35, 0.00000000e+00, 2.59205874e-04,\n",
              "        0.00000000e+00, 1.39622594e-20, 6.71488387e-26, 1.97255518e-04,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.44586212e-32,\n",
              "        0.00000000e+00, 1.43397181e-37],\n",
              "       [2.58158794e-12, 2.79538963e-05, 3.90975253e-20, 2.39940148e-13,\n",
              "        4.13682796e-02, 9.29068089e-01, 2.66358136e-09, 2.19651467e-11,\n",
              "        2.38895950e-06, 1.07012254e-14, 2.00505856e-05, 1.67289489e-08,\n",
              "        1.35081209e-22, 1.00002067e-21, 4.76208477e-29, 3.73973884e-03,\n",
              "        6.03723731e-25, 2.57688090e-02, 1.78245805e-13, 4.71559179e-06,\n",
              "        6.34764303e-24, 5.52992111e-26, 4.31111221e-26, 1.33494094e-09,\n",
              "        8.96971827e-26, 4.00609146e-08],\n",
              "       [9.44505513e-01, 1.33092944e-02, 3.71454621e-06, 2.64224975e-04,\n",
              "        1.42950989e-14, 1.06290740e-10, 3.44223827e-02, 1.38706613e-12,\n",
              "        8.10838618e-10, 4.13538003e-03, 1.20997166e-14, 2.17567771e-16,\n",
              "        3.35223014e-08, 5.92720792e-15, 1.23309765e-10, 2.69562639e-09,\n",
              "        6.65208546e-08, 1.91470849e-06, 3.35682929e-03, 1.46814271e-18,\n",
              "        5.67804648e-08, 1.02209644e-10, 7.43810395e-14, 4.91130891e-07,\n",
              "        1.06570604e-15, 4.07295898e-12],\n",
              "       [1.06284931e-25, 6.88061630e-12, 0.00000000e+00, 1.00660520e-08,\n",
              "        3.80871953e-19, 2.31876365e-10, 1.03089551e-03, 1.20311218e-18,\n",
              "        9.57301617e-01, 1.56554038e-21, 1.52466172e-28, 1.93610776e-05,\n",
              "        2.36421593e-30, 9.26583253e-29, 1.21382029e-34, 4.16247435e-02,\n",
              "        2.54305772e-32, 1.08395043e-14, 3.02203716e-20, 2.34059153e-05,\n",
              "        5.51462236e-33, 1.02258146e-31, 1.59387506e-35, 2.77059019e-25,\n",
              "        0.00000000e+00, 9.77987767e-29],\n",
              "       [7.51811980e-32, 1.48598717e-13, 0.00000000e+00, 4.08348615e-06,\n",
              "        1.62551511e-19, 7.33792096e-15, 8.84932572e-09, 3.29489093e-16,\n",
              "        1.64187588e-02, 3.69440308e-21, 3.60120312e-29, 9.83563900e-01,\n",
              "        2.33557659e-27, 1.88086982e-24, 1.80289180e-30, 9.34209493e-06,\n",
              "        4.12699986e-31, 1.55090058e-18, 5.89380646e-21, 3.86224610e-06,\n",
              "        4.49095755e-30, 5.92367633e-27, 3.21613315e-32, 5.07967759e-27,\n",
              "        0.00000000e+00, 2.65600340e-33],\n",
              "       [5.27293778e-05, 3.62376204e-06, 3.71983857e-03, 6.30352603e-10,\n",
              "        1.17205452e-15, 2.23246524e-15, 1.30481737e-09, 6.43653967e-13,\n",
              "        2.05535408e-14, 7.86106940e-03, 6.69111930e-16, 2.82349883e-16,\n",
              "        1.12951411e-05, 3.60636152e-12, 2.29093803e-06, 4.53845547e-15,\n",
              "        1.14923351e-08, 9.22414853e-11, 9.88268733e-01, 3.77689496e-21,\n",
              "        1.31339193e-05, 2.07096722e-08, 2.77275121e-12, 6.72192837e-05,\n",
              "        7.03332859e-09, 1.62389592e-13],\n",
              "       [1.17357892e-12, 8.80897646e-11, 2.03416190e-12, 2.75056419e-11,\n",
              "        9.37857317e-26, 2.07779746e-28, 6.83686684e-20, 1.62888840e-14,\n",
              "        1.36538225e-24, 9.86736715e-01, 4.63710702e-24, 3.86930672e-17,\n",
              "        3.90460686e-04, 3.05906758e-11, 5.17050270e-03, 6.93141410e-25,\n",
              "        1.76698407e-08, 6.79503593e-19, 7.43057765e-03, 4.75768154e-30,\n",
              "        2.69346841e-04, 2.07354492e-06, 4.99623953e-12, 7.04489667e-09,\n",
              "        1.84940504e-07, 6.26010038e-25],\n",
              "       [2.41095900e-35, 3.31224055e-20, 6.23080883e-36, 4.83458530e-19,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.50499677e-10,\n",
              "        0.00000000e+00, 2.57208339e-05, 2.03057662e-38, 4.91971445e-21,\n",
              "        3.42354964e-04, 2.14205306e-06, 2.90862117e-05, 0.00000000e+00,\n",
              "        2.11496954e-03, 6.33342136e-36, 7.77103669e-14, 0.00000000e+00,\n",
              "        9.92804229e-01, 4.59942920e-03, 7.79406546e-05, 1.65259273e-15,\n",
              "        4.10055191e-06, 0.00000000e+00],\n",
              "       [1.22749013e-28, 1.48168655e-13, 1.91894408e-29, 2.82418227e-07,\n",
              "        3.15101958e-32, 0.00000000e+00, 1.57068732e-32, 5.74524211e-06,\n",
              "        1.29795001e-32, 8.09087127e-04, 6.13305157e-24, 1.03862658e-13,\n",
              "        9.66832161e-01, 2.48122658e-03, 4.67133441e-06, 2.10300485e-34,\n",
              "        1.25503371e-04, 6.40246808e-25, 1.56329283e-10, 6.23448443e-31,\n",
              "        3.05551942e-03, 1.70521366e-06, 3.76636322e-08, 2.68526007e-10,\n",
              "        2.66840179e-02, 3.33054313e-38],\n",
              "       [2.77071392e-17, 3.67622164e-08, 1.11321616e-28, 5.58450389e-15,\n",
              "        1.35540140e-05, 1.62491605e-01, 3.62308121e-08, 2.43874020e-15,\n",
              "        1.59175499e-04, 2.61598309e-18, 1.97129704e-12, 1.02346238e-08,\n",
              "        4.99670224e-30, 3.99460259e-28, 7.41739877e-37, 8.35784853e-01,\n",
              "        2.28942806e-31, 1.61140997e-05, 2.87726610e-16, 1.53467502e-03,\n",
              "        7.68148036e-31, 1.20012656e-33, 2.10948798e-33, 3.40651918e-15,\n",
              "        7.21513209e-35, 4.80808681e-15],\n",
              "       [1.26082172e-32, 1.87152433e-16, 3.96228730e-32, 2.54969199e-16,\n",
              "        4.90937989e-37, 0.00000000e+00, 1.28503102e-34, 3.32549191e-03,\n",
              "        3.90762458e-37, 3.57996366e-09, 2.33510864e-25, 3.06499534e-18,\n",
              "        6.58943372e-06, 7.85832759e-04, 1.10543814e-11, 0.00000000e+00,\n",
              "        9.70712364e-01, 1.53902341e-29, 1.95160203e-15, 2.32916377e-34,\n",
              "        7.38247298e-03, 6.32443218e-08, 7.84555729e-03, 2.03264974e-11,\n",
              "        9.94165335e-03, 0.00000000e+00],\n",
              "       [7.84417649e-30, 2.37950454e-16, 1.76680149e-30, 1.36514894e-14,\n",
              "        5.98584145e-38, 0.00000000e+00, 5.12159583e-33, 6.08483560e-07,\n",
              "        2.06499307e-38, 3.85822277e-05, 5.28089615e-30, 4.51826273e-16,\n",
              "        3.97188682e-03, 5.43569971e-04, 1.05065374e-04, 0.00000000e+00,\n",
              "        2.04122961e-02, 1.27730804e-28, 3.56012897e-12, 0.00000000e+00,\n",
              "        9.57914770e-01, 1.38142062e-02, 3.12793348e-03, 5.70154872e-13,\n",
              "        7.11002795e-05, 0.00000000e+00],\n",
              "       [0.00000000e+00, 2.11737516e-18, 0.00000000e+00, 8.83649301e-14,\n",
              "        2.73201982e-25, 2.83578753e-15, 2.21621299e-09, 1.72419430e-25,\n",
              "        9.99248207e-01, 2.27604647e-31, 0.00000000e+00, 2.52804341e-04,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.36150571e-04,\n",
              "        0.00000000e+00, 8.05566329e-23, 6.22201821e-29, 3.62830731e-04,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.29118042e-36,\n",
              "        0.00000000e+00, 0.00000000e+00],\n",
              "       [1.16013773e-02, 5.93890181e-05, 9.82294559e-01, 3.68600062e-09,\n",
              "        3.64232863e-12, 7.27265970e-10, 4.82846553e-06, 6.87602481e-13,\n",
              "        1.74635750e-10, 3.94779418e-05, 5.75060746e-12, 1.72147673e-15,\n",
              "        8.10586940e-08, 3.26541528e-13, 1.32439992e-09, 1.80343615e-10,\n",
              "        5.87290172e-09, 3.33897425e-08, 5.91419172e-03, 5.70454356e-18,\n",
              "        5.40060228e-07, 1.56703789e-10, 9.95419242e-13, 8.54949030e-05,\n",
              "        4.16403856e-11, 2.04473172e-09],\n",
              "       [1.26329485e-11, 1.08816994e-05, 4.62088236e-17, 1.03687315e-15,\n",
              "        2.37380862e-02, 3.30702351e-05, 1.08665797e-16, 3.79427323e-11,\n",
              "        1.10706054e-14, 8.04660918e-16, 9.66582596e-01, 7.02419622e-14,\n",
              "        4.64904352e-19, 3.70588805e-20, 2.29400116e-27, 1.60787772e-11,\n",
              "        1.36385909e-23, 9.29499324e-03, 9.85721357e-17, 7.60652845e-16,\n",
              "        3.01169231e-21, 3.71018260e-23, 2.35972929e-24, 5.67400562e-08,\n",
              "        7.24285534e-22, 3.40255880e-04],\n",
              "       [8.69456596e-37, 3.09614903e-14, 0.00000000e+00, 2.27225780e-10,\n",
              "        9.03095822e-14, 7.39430879e-15, 1.57734179e-19, 2.44381440e-14,\n",
              "        2.16809397e-08, 4.28792887e-23, 1.13475496e-22, 9.99688387e-01,\n",
              "        1.82478718e-32, 3.03991787e-27, 1.61626003e-38, 6.95899605e-09,\n",
              "        1.73960644e-37, 1.20701688e-18, 1.34307892e-23, 3.11663578e-04,\n",
              "        6.21956586e-35, 2.77220965e-34, 1.21072725e-36, 1.07396872e-26,\n",
              "        0.00000000e+00, 6.89271316e-34],\n",
              "       [5.97788420e-14, 2.97461099e-07, 1.70004608e-13, 2.49893731e-07,\n",
              "        3.12524823e-16, 9.29702622e-21, 3.54167934e-17, 2.08195066e-03,\n",
              "        1.10302417e-16, 1.17757998e-03, 1.86150470e-11, 2.71257961e-10,\n",
              "        3.01547162e-03, 4.08574590e-04, 8.52938399e-07, 2.50877324e-18,\n",
              "        3.12793329e-02, 6.06982251e-14, 3.64414404e-06, 4.80509024e-13,\n",
              "        8.04533809e-03, 3.63061190e-06, 1.05874409e-04, 3.47976456e-04,\n",
              "        9.53529298e-01, 1.23285637e-17]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "predicted_message = model3.predict(MESSAGE_2D)\n",
        "predicted_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG-Us71Z6pha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc8d03a-c919-4e86-c58b-14c5ae2250fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENEFIVPBOXINGWIFAILSJUMPQUICKLY\n"
          ]
        }
      ],
      "source": [
        "message_list = \"\"\n",
        "for num in range(len(predicted_message)):\n",
        "    max_val = 0\n",
        "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    message_list += predict\n",
        "print(message_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKPnYAN1Smjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de13c2d-8773-48be-a3af-3f15a70833ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy between predicted and actual message: 80.64516129032258%\n",
            "Here is the list of letters that are a mismatch between predicted and actual message:\n",
            "Actual    Predicted\n",
            "T         E\n",
            "H         N\n",
            "E         P\n",
            "Z         F\n",
            "R         I\n",
            "D         L\n"
          ]
        }
      ],
      "source": [
        "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
        "list_mismatch = []\n",
        "num_total, num_correct = len(actual_message), len(message_list)\n",
        "for i in range(len(actual_message)):\n",
        "    if message_list[i] != actual_message[i]:\n",
        "        num_correct -= 1\n",
        "        list_mismatch.append([actual_message[i], message_list[i]])\n",
        "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_mismatch) == 0:\n",
        "    print(\"The message appears to be decoded correctly.\")\n",
        "else:\n",
        "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_mismatch)):\n",
        "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Fgx3s06qmg"
      },
      "source": [
        "## A: As more hidden layers are added, the categorical accuracy obtained will gradually worsen. Although research sources stated that additional layers may decrease accuracy due to a vanishing gradient, those same sources indicate that using ReLU activation for the hidden layers solves the issue of the vanishing gradient, so our model with ReLU for the hidden layers is unlikely to experience vanishing gradient effects, and the decreased accuracy with additional layers is most likely due to overfitting. As a result, no matter how many hidden layers are added, all letters in the message will never be decoded correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsXHxW4a0kl1"
      },
      "source": [
        "# 9. Load the EMNIST Letters dataset, and use plt.imshow() to verify that the image data has been loaded correctly and that the corresponding labels are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRZJslkA89H7"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt # Needed to do matplotlib operations\n",
        "emnist_data = np.load('emnist_letters.npz')\n",
        "\n",
        "train_img = emnist_data['train_images']\n",
        "train_label = emnist_data['train_labels']\n",
        "\n",
        "test_img = emnist_data['test_images']\n",
        "test_label = emnist_data['test_labels']\n",
        "\n",
        "validate_img = emnist_data['validate_images']\n",
        "validate_label = emnist_data['validate_labels']\n",
        "\n",
        "\n",
        "train_img = train_img.reshape((104000, 28, 28))\n",
        "train_img = train_img.astype(\"float32\") / 255\n",
        "test_img = test_img.reshape((20800, 28, 28))\n",
        "test_img = test_img.astype(\"float32\") / 255\n",
        "validate_img = validate_img.reshape((20800, 28, 28))\n",
        "validate_img = validate_img.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toUOFe1X8_yS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c378aa6f-8e56-4fc5-e580-78c1b32cd5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<numpy.lib.npyio.NpzFile object at 0x7fb6f641d390>\n",
            "Train_img shape:  (104000, 28, 28)\n",
            "Train_label shape:  (104000, 27)\n",
            "Test_img shape:  (20800, 28, 28)\n",
            "Test_label shape:  (20800, 27)\n",
            "Validate_img shape:  (20800, 28, 28)\n",
            "Validate_label shape:  (20800, 27)\n"
          ]
        }
      ],
      "source": [
        "print(emnist_data)\n",
        "print(\"Train_img shape: \", train_img.shape)\n",
        "print(\"Train_label shape: \", train_label.shape)\n",
        "print(\"Test_img shape: \", test_img.shape)\n",
        "print(\"Test_label shape: \", test_label.shape)\n",
        "print(\"Validate_img shape: \", validate_img.shape)\n",
        "print(\"Validate_label shape: \", validate_label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBrBavugSwy9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "b7617abd-f6b5-43e5-e193-e8b535490264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label of the image:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0.]\n",
            "This is letter:  W\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Print out the first 3 letters.\n",
        "plt.imshow(train_img[0].reshape(28, 28))\n",
        "print(\"Label of the image: \",train_label[0])\n",
        "print(\"This is letter: \",chr(np.argmax(train_label[0])-1+ord('A')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKtfxZWXSxuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "28b17831-f076-4a60-8cd4-d3ff5c901030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label of the image:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "This is letter:  G\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(train_img[1].reshape(28, 28))\n",
        "print(\"Label of the image: \",train_label[1])\n",
        "print(\"This is letter: \",chr(np.argmax(train_label[1])-1+ord('A')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etH3aPdVSyli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "e17caab2-f4cc-4ea7-e538-88561b9f51f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label of the image:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "This is letter:  P\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(train_img[2].reshape(28, 28))\n",
        "print(\"Label of the image: \",train_label[2])\n",
        "print(\"This is letter: \",chr(np.argmax(train_label[2])-1+ord('A')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN-IAng3ZLBq"
      },
      "source": [
        "#10.\tApply the network architecture from Cholletâ€™s MNIST notebook to the EMNIST Letters data. (You will need to modify the numbers of inputs and outputs, but should leave the dense layer intact.) What accuracy do you achieve? How does this compare with the accuracy for MNIST?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmq9wPE2ZFi3"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(27, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxYW7FbAZUr_"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBI8lFbQZWzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156e9d52-46a6-4732-b12d-f31122f2ff09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "813/813 [==============================] - 8s 10ms/step - loss: 2.3158 - accuracy: 0.4198\n",
            "Epoch 2/5\n",
            "813/813 [==============================] - 7s 9ms/step - loss: 1.3933 - accuracy: 0.5997\n",
            "Epoch 3/5\n",
            "813/813 [==============================] - 7s 9ms/step - loss: 1.2017 - accuracy: 0.6493\n",
            "Epoch 4/5\n",
            "813/813 [==============================] - 7s 9ms/step - loss: 1.1179 - accuracy: 0.6725\n",
            "Epoch 5/5\n",
            "813/813 [==============================] - 8s 9ms/step - loss: 1.0619 - accuracy: 0.6902\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6f642ef10>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "model.fit(train_img, train_label, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_img, test_label, verbose=1)"
      ],
      "metadata": {
        "id": "8RZUVN934VGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834b9031-7638-4a1d-f878-864ba051337b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "650/650 [==============================] - 2s 2ms/step - loss: 1.0466 - accuracy: 0.6953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0466054677963257, 0.6952884793281555]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oahb36Taqp13"
      },
      "source": [
        "## A: The accuracy we obtained for the EMNIST Letters data was 70.17%, which is lower than the 98.89% accuracy that Chollet's notebook obtained for the MNIST data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bIgW6jorVct"
      },
      "source": [
        "# 11.\tKeeping the same number of layers in the network (i.e. an MLP with a single hidden layer), modify the architecture to improve the accuracy. You will need to decide on an appropriate number of neurons in the hidden layer. Keep in mind that:\n",
        "\n",
        ">a.\tThere are 27 classes rather than 10, so you will need a larger hidden layer than the MNIST network.\n",
        "\n",
        ">b.\tIn addition to having more classes, EMNIST Letters mixes upper- and lowercase letters within each class, so even with enough neurons in the hidden layer, your accuracy is likely to be lower.  See the details in the EMNIST paper for the kind of performance you might reasonably expect.\n",
        "\n",
        ">c.\tThe Keras fit() method can take a validation_data parameter in order to evaluate metrics on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rm2rLsJroSP"
      },
      "outputs": [],
      "source": [
        "model2 = keras.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1024, activation=\"relu\"),     #doubled the dense layer 512 (acc:69%)->1024 (acc:71%)\n",
        "    layers.Dense(27, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbMhL5Vkrvd9"
      },
      "outputs": [],
      "source": [
        "model2.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc5bB4TftF05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5be72e9-d75e-44ac-a57e-f4f763968bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "813/813 [==============================] - 15s 19ms/step - loss: 2.1125 - accuracy: 0.4536 - val_loss: 1.4399 - val_accuracy: 0.5840\n",
            "Epoch 2/5\n",
            "813/813 [==============================] - 15s 18ms/step - loss: 1.2774 - accuracy: 0.6278 - val_loss: 1.1883 - val_accuracy: 0.6498\n",
            "Epoch 3/5\n",
            "813/813 [==============================] - 14s 18ms/step - loss: 1.1262 - accuracy: 0.6697 - val_loss: 1.0848 - val_accuracy: 0.6825\n",
            "Epoch 4/5\n",
            "813/813 [==============================] - 14s 18ms/step - loss: 1.0449 - accuracy: 0.6952 - val_loss: 1.0089 - val_accuracy: 0.7082\n",
            "Epoch 5/5\n",
            "813/813 [==============================] - 15s 18ms/step - loss: 0.9728 - accuracy: 0.7180 - val_loss: 0.9418 - val_accuracy: 0.7287\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6f62be290>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "model2.fit(train_img, train_label, validation_data=(validate_img, validate_label), epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(test_img, test_label, verbose=1)"
      ],
      "metadata": {
        "id": "JCLtxtCd4TFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d977f9-ef85-4309-9e5c-af50ee3e69cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "650/650 [==============================] - 3s 4ms/step - loss: 0.9492 - accuracy: 0.7263\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9491686224937439, 0.7262980937957764]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The accuracy is 72.53%, which is a slight improvement over the accuracy from the previous problem. We also tried increasing the neuron count in the hidden layer to 10,000 as that was the number of neurons used in the network that the EMNIST paper ran on the EMNIST Letters dataset. This increased the accuracy to 84.59%, which is very close to the 85.15% accuracy obtained by the EMNIST paper network on the same dataset."
      ],
      "metadata": {
        "id": "FzYQmhQwqSFL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO9i14YSjls1"
      },
      "source": [
        "# 12. The Keras examples include a Simple MNIST convnet. Note the accuracy obtained by that code compared to the previous example from Chollet.\n",
        "# Rather than building a deeper MLP, letâ€™s apply this architecture to the EMNIST Letters data. What accuracy do you achieve? How does this compare with the accuracy for the MNIST dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evQuab40tZEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abd3133-2e4c-424f-81a1-7bd96ac77bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(104000, 28, 28, 1)\n",
            "(28, 28, 1)\n",
            "(20800, 28, 28, 1)\n",
            "(28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 27\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "train_img = train_img.reshape(104000, 28, 28)\n",
        "test_img = test_img.reshape(20800, 28, 28)\n",
        "\n",
        "train_img_convnet = np.expand_dims(train_img, -1)\n",
        "test_img_convnet = np.expand_dims(test_img, -1)\n",
        "\n",
        "\n",
        "print(train_img_convnet.shape)\n",
        "print(train_img_convnet[0].shape)\n",
        "print(test_img_convnet.shape)\n",
        "print(test_img_convnet[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mszni8H8v7GL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b37f25-4e50-40d5-d362-dfc275f1b2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 27)                43227     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,043\n",
            "Trainable params: 62,043\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model3 = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "a2sg1TGS7yv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.fit(train_img_convnet, train_label, epochs=5, batch_size=128, validation_split=0.1)"
      ],
      "metadata": {
        "id": "KzYHlAeQ1MeJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9ab11f-04cf-4ec4-8bcc-9bbb9acf8c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "732/732 [==============================] - 68s 92ms/step - loss: 2.0930 - accuracy: 0.3938 - val_loss: 1.2943 - val_accuracy: 0.6130\n",
            "Epoch 2/5\n",
            "732/732 [==============================] - 67s 92ms/step - loss: 1.3100 - accuracy: 0.6090 - val_loss: 1.0664 - val_accuracy: 0.6870\n",
            "Epoch 3/5\n",
            "732/732 [==============================] - 67s 92ms/step - loss: 1.0786 - accuracy: 0.6746 - val_loss: 0.8243 - val_accuracy: 0.7541\n",
            "Epoch 4/5\n",
            "732/732 [==============================] - 67s 91ms/step - loss: 0.8723 - accuracy: 0.7376 - val_loss: 0.6606 - val_accuracy: 0.7998\n",
            "Epoch 5/5\n",
            "732/732 [==============================] - 67s 91ms/step - loss: 0.7543 - accuracy: 0.7708 - val_loss: 0.5702 - val_accuracy: 0.8293\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6f60d8150>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.evaluate(test_img_convnet, test_label, verbose=1)"
      ],
      "metadata": {
        "id": "jSZx3SS7kXCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9fa3de-af5d-4161-a6fb-34c8b4e303f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "650/650 [==============================] - 6s 9ms/step - loss: 0.5656 - accuracy: 0.8324\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5656337141990662, 0.832403838634491]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The accuracy obtained is 82.10%, which is not better than the accuracy for MNIST, although it is a significant improvement over the previous models we used on the EMNIST Letters data set."
      ],
      "metadata": {
        "id": "7iPgf_SbqYPK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PaJ8VTIjoy0"
      },
      "source": [
        "# 13. Use plt.imshow() to view some of the misclassified images and examine their labels. Describe what you think might have gone wrong."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only plt.imshow() will be used to view the misclassified images.\n",
        "misclassified_images = []\n",
        "for i in range(200):\n",
        "    predict_image = model.predict(test_img[i].reshape(1, 28, 28))\n",
        "    predict_letter = chr(np.argmax(predict_image)-1+ord('A'))\n",
        "    actual_letter = chr(np.argmax(test_label[i])-1+ord('A'))\n",
        "    if predict_letter != actual_letter:\n",
        "        misclassified_images.append(i)\n",
        "print(\"List of images that appears to be misclassified:\")\n",
        "print(misclassified_images)"
      ],
      "metadata": {
        "id": "x_2wpfcX1bN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269ab80f-2136-452e-d82e-16b660a5e9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of images that appears to be misclassified:\n",
            "[0, 1, 3, 4, 8, 10, 12, 13, 14, 16, 18, 19, 21, 22, 27, 33, 34, 36, 37, 41, 44, 45, 47, 48, 51, 52, 56, 57, 58, 59, 66, 69, 76, 78, 80, 83, 84, 88, 90, 91, 94, 95, 96, 98, 103, 106, 108, 111, 117, 118, 119, 121, 123, 127, 128, 129, 131, 132, 136, 137, 138, 139, 142, 143, 144, 145, 149, 152, 157, 159, 160, 161, 164, 166, 169, 170, 173, 178, 179, 180, 184, 185, 186, 187, 190, 191, 192]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[0]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "4PM0EQ3DAI2R",
        "outputId": "e96ba7aa-55d1-499d-ad00-f16be91f41e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186c9055d0>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[1]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "B3dQ4w5BAJmJ",
        "outputId": "3a6029bf-6812-40c8-ef17-9c4ff407b0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186cb949d0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[2]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "idkEHM8NAJyG",
        "outputId": "222a5200-b860-44ce-9e6f-05209f1aaa7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186cae3bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[3]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "W1vciU-bAJ71",
        "outputId": "8b6c77ab-cc7c-4a1b-fef3-e66b38bc316c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186c935990>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[4]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "fU2lJehhAKEa",
        "outputId": "889b592c-6bf0-4c28-b890-262644c6cfc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186ca6e850>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[5]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "p3g6EZ-sAbLS",
        "outputId": "34edcef2-da93-47d3-9ff4-91007eaa3939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1867a4ca50>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[6]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "hFo96yipAbhD",
        "outputId": "131b54bd-73a8-43dd-b102-3216f3016162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1867a32690>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[7]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "JiXtTXThAb5M",
        "outputId": "17e81db5-79cf-4fca-ae0f-47aaa783e75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186799e2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[-1]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Bjf3mkUqBMsG",
        "outputId": "07926af3-0ed3-4152-d415-dc1180a2b452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186b3e2210>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_img[misclassified_images[-2]].reshape(28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "xDSrs2fvBUt6",
        "outputId": "8f88e0e4-d7b4-4124-ad55-5f68f0c6a142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f186c965450>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A: Many of the letters in the misclassified images resemble other letters. Although all the letters shown belong to the class label 'A', some closely resemble other letters such as lowercase 'q' or uppercase 'G' or 'D'. The confusion matrix in the EMNIST paper corroborates the idea that certain letter samples are more easily mistaken for other letter samples by the neural network, such as samples of A being more easily confused for G or Q. The misclassified samples we have here resemble the letters that were most prone to confusion with A."
      ],
      "metadata": {
        "id": "f3FQb6GSBbnU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "afSkHPK3on6v",
        "r-GVfQg0rJyH",
        "L8Fgx3s06qmg",
        "FzYQmhQwqSFL",
        "7iPgf_SbqYPK"
      ],
      "name": "Project 2 Keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}