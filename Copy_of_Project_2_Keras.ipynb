{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Project 2 Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raymec/keras/blob/main/Copy_of_Project_2_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spring 2022\n",
        "# CPSC 585 Project 2\n",
        "## Raymond Carpio\n",
        "## Yu Pan\n",
        "## Sijie Shang\n",
        "## John Tu"
      ],
      "metadata": {
        "id": "hmSTcGw2lmqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwUguxm0px6t",
        "outputId": "4b6d683f-f594-4295-a26d-14796e8cd37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import * # Import the entire dataset\n",
        "import random # Needed to generate random numbers\n",
        "import numpy as np # Needed to do NumPy functions\n",
        "from matplotlib import pyplot as plt # Needed to do matplotlib graphs\n",
        "\n",
        "print(\"TRAINING_SET: \", TRAINING_SET[0])\n",
        "print(\"TEST_SET: \", TEST_SET[0])\n",
        "print(\"MESSAGE: \", MESSAGE[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oB4W5pNn319",
        "outputId": "49c3b979-804c-477f-f9b8-8c722dd3676c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING_SET:  ('A', [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "TEST_SET:  ('A', [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "MESSAGE:  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HyfA6D9norA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. As with Project 1, convert the images in TRAINING_SET, TEST_SET, and MESSAGE into two-dimensional NumPy arrays of size (# examples Ã— # features)."
      ],
      "metadata": {
        "id": "Dth6n0lcl37G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show(image):\n",
        "    letter_len = len(image)\n",
        "    counter = 0\n",
        "    for x in range(letter_len):\n",
        "        if image[x] == 1:\n",
        "            print('#', end='')\n",
        "        else:\n",
        "            print(' ', end='')\n",
        "        counter += 1\n",
        "        if counter == 16:\n",
        "            counter = 0\n",
        "            print('\\n', end='')"
      ],
      "metadata": {
        "id": "awAfKGbfpiYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEtinAfAkx_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecaf5a6e-16d3-4c00-a685-a514d43d6b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(52, 256)\n",
            "(26, 256)\n",
            "(31, 256)\n",
            "\n",
            "The image of the first letter: \n",
            "\n",
            "    ####        \n",
            "    ####        \n",
            "  ########      \n",
            "  ########      \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "############    \n",
            "############    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "####    ####    \n",
            "                \n",
            "                \n",
            "The letter list:  <function letter_list at 0x7f394e09d680>\n"
          ]
        }
      ],
      "source": [
        "def convert_2d_array(input_data):\n",
        "    if len(input_data) == 0:\n",
        "        return None\n",
        "    output_data = []\n",
        "    if len(input_data[0]) == 2:\n",
        "        for x, y in input_data:\n",
        "            output_data.append(np.array(y))\n",
        "    else:\n",
        "        for x in input_data:\n",
        "            output_data.append(np.array(x))\n",
        "    return np.array(output_data)\n",
        "\n",
        "def letter_list(input_data):\n",
        "    if len(input_data) == 0:\n",
        "        return None\n",
        "    output_data = []\n",
        "    if len(input_data[0]) == 2:\n",
        "        for x, y in input_data:\n",
        "            output_data.append(np.array(x))\n",
        "    return np.array(output_data)\n",
        "\n",
        "TRAINING_SET_2D = convert_2d_array(TRAINING_SET)\n",
        "TEST_SET_2D = convert_2d_array(TEST_SET)\n",
        "MESSAGE_2D = convert_2d_array(MESSAGE)\n",
        "\n",
        "# Verify that each 2-dimensional NumPy array contains the same number of elements from the original arrays.\n",
        "print(TRAINING_SET_2D.shape)\n",
        "print(TEST_SET_2D.shape)\n",
        "print(MESSAGE_2D.shape)\n",
        "\n",
        "print(\"\\nThe image of the first letter: \\n\")\n",
        "show(TRAINING_SET_2D[0])\n",
        "\n",
        "letter_list_train=letter_list(TRAINING_SET)\n",
        "letter_list_test=letter_list(TEST_SET)\n",
        "print('The letter list: ',letter_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Rather than training 26 different perceptrons as you did in Project 1, this time you will use a single network with 26 possible outputs.\n",
        "# In order to use the character labels in TRAINING_SET and TEST_SET, convert them into integer class vectors using ord(), then into 26 one-hot encoded categorical features."
      ],
      "metadata": {
        "id": "ieAOi1CcmZXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "KpwNzPB9uj9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unicode_letters_train, unicode_letters_test = [], []\n",
        "for i in letter_list_train:\n",
        "  unicode_char=ord(i)-ord('A')\n",
        "  unicode_letters_train.append(unicode_char)\n",
        "\n",
        "print(unicode_letters_train)\n",
        "\n",
        "unicode_cat_train = tf.keras.utils.to_categorical(unicode_letters_train, num_classes=None, dtype=\"float32\")\n",
        "\n",
        "unicode_cat_train"
      ],
      "metadata": {
        "id": "HT8OYTrerfqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0471968c-6fe2-4106-b606-cdb1e3879220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in letter_list_test:\n",
        "  unicode_char=ord(i)-ord('A')\n",
        "  unicode_letters_test.append(unicode_char)\n",
        "\n",
        "print(unicode_letters_test)\n",
        "\n",
        "unicode_cat_test = tf.keras.utils.to_categorical(unicode_letters_test, num_classes=None, dtype=\"float32\")\n",
        "\n",
        "unicode_cat_test"
      ],
      "metadata": {
        "id": "t7SRbOcSnOpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221f4551-5f25-4712-e2c8-377505d112c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Create a Sequential Keras model with a Dense hidden layer and a Dense output layer with softmax activation and categorical cross-entropy loss."
      ],
      "metadata": {
        "id": "WMjg_gFmmjpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sequential model with two Dense layers:\n",
        "# One as a hidden layer and the other as an output layer\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(256,)))\n",
        "model.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer\"))\n",
        "model.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yWI6X0D7NK0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87a2125-3a94-447e-d2fa-428951d30bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Hidden_layer (Dense)        (None, 32)                8224      \n",
            "                                                                 \n",
            " Output_layer (Dense)        (None, 26)                858       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,082\n",
            "Trainable params: 9,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. compile and fit the model to the training set. Train the model until the accuracy is as high as possible. You may wish to use an EarlyStopping callback."
      ],
      "metadata": {
        "id": "eZoyNBQygS1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "             metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "fn4Aze_nhvdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
        "model.fit(x=TRAINING_SET_2D, y=unicode_cat_train, epochs=9999, verbose=2, callbacks=[callback])"
      ],
      "metadata": {
        "id": "V3FRJkvMiNv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b9aabc-5239-4731-ab26-2e594757b726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9999\n",
            "2/2 - 1s - loss: 3.4614 - categorical_accuracy: 0.0385 - 1s/epoch - 517ms/step\n",
            "Epoch 2/9999\n",
            "2/2 - 0s - loss: 3.3627 - categorical_accuracy: 0.0769 - 11ms/epoch - 5ms/step\n",
            "Epoch 3/9999\n",
            "2/2 - 0s - loss: 3.2845 - categorical_accuracy: 0.0962 - 10ms/epoch - 5ms/step\n",
            "Epoch 4/9999\n",
            "2/2 - 0s - loss: 3.2251 - categorical_accuracy: 0.1154 - 13ms/epoch - 7ms/step\n",
            "Epoch 5/9999\n",
            "2/2 - 0s - loss: 3.1735 - categorical_accuracy: 0.0962 - 12ms/epoch - 6ms/step\n",
            "Epoch 6/9999\n",
            "2/2 - 0s - loss: 3.1309 - categorical_accuracy: 0.1154 - 19ms/epoch - 9ms/step\n",
            "Epoch 7/9999\n",
            "2/2 - 0s - loss: 3.0911 - categorical_accuracy: 0.1154 - 21ms/epoch - 10ms/step\n",
            "Epoch 8/9999\n",
            "2/2 - 0s - loss: 3.0491 - categorical_accuracy: 0.1731 - 9ms/epoch - 5ms/step\n",
            "Epoch 9/9999\n",
            "2/2 - 0s - loss: 3.0097 - categorical_accuracy: 0.1538 - 17ms/epoch - 9ms/step\n",
            "Epoch 10/9999\n",
            "2/2 - 0s - loss: 2.9697 - categorical_accuracy: 0.2115 - 9ms/epoch - 4ms/step\n",
            "Epoch 11/9999\n",
            "2/2 - 0s - loss: 2.9287 - categorical_accuracy: 0.2115 - 9ms/epoch - 5ms/step\n",
            "Epoch 12/9999\n",
            "2/2 - 0s - loss: 2.8907 - categorical_accuracy: 0.2500 - 9ms/epoch - 4ms/step\n",
            "Epoch 13/9999\n",
            "2/2 - 0s - loss: 2.8462 - categorical_accuracy: 0.2692 - 9ms/epoch - 5ms/step\n",
            "Epoch 14/9999\n",
            "2/2 - 0s - loss: 2.8030 - categorical_accuracy: 0.2885 - 10ms/epoch - 5ms/step\n",
            "Epoch 15/9999\n",
            "2/2 - 0s - loss: 2.7594 - categorical_accuracy: 0.3462 - 12ms/epoch - 6ms/step\n",
            "Epoch 16/9999\n",
            "2/2 - 0s - loss: 2.7149 - categorical_accuracy: 0.3846 - 13ms/epoch - 6ms/step\n",
            "Epoch 17/9999\n",
            "2/2 - 0s - loss: 2.6700 - categorical_accuracy: 0.4038 - 13ms/epoch - 7ms/step\n",
            "Epoch 18/9999\n",
            "2/2 - 0s - loss: 2.6253 - categorical_accuracy: 0.4038 - 20ms/epoch - 10ms/step\n",
            "Epoch 19/9999\n",
            "2/2 - 0s - loss: 2.5769 - categorical_accuracy: 0.4038 - 13ms/epoch - 7ms/step\n",
            "Epoch 20/9999\n",
            "2/2 - 0s - loss: 2.5318 - categorical_accuracy: 0.4038 - 18ms/epoch - 9ms/step\n",
            "Epoch 21/9999\n",
            "2/2 - 0s - loss: 2.4837 - categorical_accuracy: 0.4038 - 14ms/epoch - 7ms/step\n",
            "Epoch 22/9999\n",
            "2/2 - 0s - loss: 2.4362 - categorical_accuracy: 0.4038 - 16ms/epoch - 8ms/step\n",
            "Epoch 23/9999\n",
            "2/2 - 0s - loss: 2.3885 - categorical_accuracy: 0.4038 - 12ms/epoch - 6ms/step\n",
            "Epoch 24/9999\n",
            "2/2 - 0s - loss: 2.3411 - categorical_accuracy: 0.4038 - 10ms/epoch - 5ms/step\n",
            "Epoch 25/9999\n",
            "2/2 - 0s - loss: 2.2932 - categorical_accuracy: 0.4615 - 14ms/epoch - 7ms/step\n",
            "Epoch 26/9999\n",
            "2/2 - 0s - loss: 2.2444 - categorical_accuracy: 0.4808 - 8ms/epoch - 4ms/step\n",
            "Epoch 27/9999\n",
            "2/2 - 0s - loss: 2.1971 - categorical_accuracy: 0.5192 - 8ms/epoch - 4ms/step\n",
            "Epoch 28/9999\n",
            "2/2 - 0s - loss: 2.1498 - categorical_accuracy: 0.5000 - 9ms/epoch - 4ms/step\n",
            "Epoch 29/9999\n",
            "2/2 - 0s - loss: 2.1034 - categorical_accuracy: 0.5000 - 15ms/epoch - 8ms/step\n",
            "Epoch 30/9999\n",
            "2/2 - 0s - loss: 2.0581 - categorical_accuracy: 0.5385 - 9ms/epoch - 4ms/step\n",
            "Epoch 31/9999\n",
            "2/2 - 0s - loss: 2.0125 - categorical_accuracy: 0.5577 - 9ms/epoch - 5ms/step\n",
            "Epoch 32/9999\n",
            "2/2 - 0s - loss: 1.9654 - categorical_accuracy: 0.5769 - 16ms/epoch - 8ms/step\n",
            "Epoch 33/9999\n",
            "2/2 - 0s - loss: 1.9216 - categorical_accuracy: 0.6154 - 9ms/epoch - 4ms/step\n",
            "Epoch 34/9999\n",
            "2/2 - 0s - loss: 1.8765 - categorical_accuracy: 0.6346 - 9ms/epoch - 5ms/step\n",
            "Epoch 35/9999\n",
            "2/2 - 0s - loss: 1.8341 - categorical_accuracy: 0.6346 - 10ms/epoch - 5ms/step\n",
            "Epoch 36/9999\n",
            "2/2 - 0s - loss: 1.7901 - categorical_accuracy: 0.6731 - 10ms/epoch - 5ms/step\n",
            "Epoch 37/9999\n",
            "2/2 - 0s - loss: 1.7453 - categorical_accuracy: 0.7115 - 12ms/epoch - 6ms/step\n",
            "Epoch 38/9999\n",
            "2/2 - 0s - loss: 1.7023 - categorical_accuracy: 0.7308 - 9ms/epoch - 5ms/step\n",
            "Epoch 39/9999\n",
            "2/2 - 0s - loss: 1.6598 - categorical_accuracy: 0.7692 - 9ms/epoch - 5ms/step\n",
            "Epoch 40/9999\n",
            "2/2 - 0s - loss: 1.6180 - categorical_accuracy: 0.7885 - 18ms/epoch - 9ms/step\n",
            "Epoch 41/9999\n",
            "2/2 - 0s - loss: 1.5784 - categorical_accuracy: 0.8269 - 9ms/epoch - 5ms/step\n",
            "Epoch 42/9999\n",
            "2/2 - 0s - loss: 1.5390 - categorical_accuracy: 0.8269 - 9ms/epoch - 4ms/step\n",
            "Epoch 43/9999\n",
            "2/2 - 0s - loss: 1.4996 - categorical_accuracy: 0.8269 - 9ms/epoch - 4ms/step\n",
            "Epoch 44/9999\n",
            "2/2 - 0s - loss: 1.4597 - categorical_accuracy: 0.8462 - 9ms/epoch - 4ms/step\n",
            "Epoch 45/9999\n",
            "2/2 - 0s - loss: 1.4242 - categorical_accuracy: 0.8654 - 9ms/epoch - 4ms/step\n",
            "Epoch 46/9999\n",
            "2/2 - 0s - loss: 1.3870 - categorical_accuracy: 0.8846 - 10ms/epoch - 5ms/step\n",
            "Epoch 47/9999\n",
            "2/2 - 0s - loss: 1.3498 - categorical_accuracy: 0.9038 - 18ms/epoch - 9ms/step\n",
            "Epoch 48/9999\n",
            "2/2 - 0s - loss: 1.3148 - categorical_accuracy: 0.9038 - 19ms/epoch - 9ms/step\n",
            "Epoch 49/9999\n",
            "2/2 - 0s - loss: 1.2798 - categorical_accuracy: 0.8846 - 15ms/epoch - 8ms/step\n",
            "Epoch 50/9999\n",
            "2/2 - 0s - loss: 1.2454 - categorical_accuracy: 0.8846 - 12ms/epoch - 6ms/step\n",
            "Epoch 51/9999\n",
            "2/2 - 0s - loss: 1.2133 - categorical_accuracy: 0.9038 - 9ms/epoch - 4ms/step\n",
            "Epoch 52/9999\n",
            "2/2 - 0s - loss: 1.1814 - categorical_accuracy: 0.9038 - 9ms/epoch - 4ms/step\n",
            "Epoch 53/9999\n",
            "2/2 - 0s - loss: 1.1500 - categorical_accuracy: 0.9038 - 9ms/epoch - 5ms/step\n",
            "Epoch 54/9999\n",
            "2/2 - 0s - loss: 1.1181 - categorical_accuracy: 0.9231 - 9ms/epoch - 4ms/step\n",
            "Epoch 55/9999\n",
            "2/2 - 0s - loss: 1.0900 - categorical_accuracy: 0.9423 - 10ms/epoch - 5ms/step\n",
            "Epoch 56/9999\n",
            "2/2 - 0s - loss: 1.0609 - categorical_accuracy: 0.9423 - 9ms/epoch - 5ms/step\n",
            "Epoch 57/9999\n",
            "2/2 - 0s - loss: 1.0324 - categorical_accuracy: 0.9615 - 9ms/epoch - 5ms/step\n",
            "Epoch 58/9999\n",
            "2/2 - 0s - loss: 1.0050 - categorical_accuracy: 0.9615 - 10ms/epoch - 5ms/step\n",
            "Epoch 59/9999\n",
            "2/2 - 0s - loss: 0.9803 - categorical_accuracy: 0.9615 - 9ms/epoch - 5ms/step\n",
            "Epoch 60/9999\n",
            "2/2 - 0s - loss: 0.9530 - categorical_accuracy: 0.9615 - 14ms/epoch - 7ms/step\n",
            "Epoch 61/9999\n",
            "2/2 - 0s - loss: 0.9279 - categorical_accuracy: 0.9615 - 33ms/epoch - 16ms/step\n",
            "Epoch 62/9999\n",
            "2/2 - 0s - loss: 0.9026 - categorical_accuracy: 0.9615 - 25ms/epoch - 12ms/step\n",
            "Epoch 63/9999\n",
            "2/2 - 0s - loss: 0.8781 - categorical_accuracy: 0.9615 - 11ms/epoch - 5ms/step\n",
            "Epoch 64/9999\n",
            "2/2 - 0s - loss: 0.8569 - categorical_accuracy: 0.9808 - 12ms/epoch - 6ms/step\n",
            "Epoch 65/9999\n",
            "2/2 - 0s - loss: 0.8349 - categorical_accuracy: 0.9808 - 12ms/epoch - 6ms/step\n",
            "Epoch 66/9999\n",
            "2/2 - 0s - loss: 0.8118 - categorical_accuracy: 0.9808 - 12ms/epoch - 6ms/step\n",
            "Epoch 67/9999\n",
            "2/2 - 0s - loss: 0.7905 - categorical_accuracy: 0.9808 - 11ms/epoch - 6ms/step\n",
            "Epoch 68/9999\n",
            "2/2 - 0s - loss: 0.7692 - categorical_accuracy: 0.9808 - 25ms/epoch - 12ms/step\n",
            "Epoch 69/9999\n",
            "2/2 - 0s - loss: 0.7500 - categorical_accuracy: 0.9808 - 12ms/epoch - 6ms/step\n",
            "Epoch 70/9999\n",
            "2/2 - 0s - loss: 0.7301 - categorical_accuracy: 0.9808 - 11ms/epoch - 5ms/step\n",
            "Epoch 71/9999\n",
            "2/2 - 0s - loss: 0.7125 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 72/9999\n",
            "2/2 - 0s - loss: 0.6933 - categorical_accuracy: 0.9808 - 18ms/epoch - 9ms/step\n",
            "Epoch 73/9999\n",
            "2/2 - 0s - loss: 0.6773 - categorical_accuracy: 0.9808 - 27ms/epoch - 13ms/step\n",
            "Epoch 74/9999\n",
            "2/2 - 0s - loss: 0.6591 - categorical_accuracy: 0.9808 - 16ms/epoch - 8ms/step\n",
            "Epoch 75/9999\n",
            "2/2 - 0s - loss: 0.6426 - categorical_accuracy: 0.9808 - 29ms/epoch - 15ms/step\n",
            "Epoch 76/9999\n",
            "2/2 - 0s - loss: 0.6269 - categorical_accuracy: 0.9808 - 11ms/epoch - 5ms/step\n",
            "Epoch 77/9999\n",
            "2/2 - 0s - loss: 0.6116 - categorical_accuracy: 0.9808 - 17ms/epoch - 9ms/step\n",
            "Epoch 78/9999\n",
            "2/2 - 0s - loss: 0.5960 - categorical_accuracy: 0.9808 - 11ms/epoch - 6ms/step\n",
            "Epoch 79/9999\n",
            "2/2 - 0s - loss: 0.5819 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 80/9999\n",
            "2/2 - 0s - loss: 0.5669 - categorical_accuracy: 0.9808 - 16ms/epoch - 8ms/step\n",
            "Epoch 81/9999\n",
            "2/2 - 0s - loss: 0.5529 - categorical_accuracy: 0.9808 - 14ms/epoch - 7ms/step\n",
            "Epoch 82/9999\n",
            "2/2 - 0s - loss: 0.5402 - categorical_accuracy: 0.9808 - 9ms/epoch - 5ms/step\n",
            "Epoch 83/9999\n",
            "2/2 - 0s - loss: 0.5282 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 84/9999\n",
            "2/2 - 0s - loss: 0.5154 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 85/9999\n",
            "2/2 - 0s - loss: 0.5039 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 86/9999\n",
            "2/2 - 0s - loss: 0.4927 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 87/9999\n",
            "2/2 - 0s - loss: 0.4807 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 88/9999\n",
            "2/2 - 0s - loss: 0.4709 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 89/9999\n",
            "2/2 - 0s - loss: 0.4605 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 90/9999\n",
            "2/2 - 0s - loss: 0.4505 - categorical_accuracy: 1.0000 - 17ms/epoch - 9ms/step\n",
            "Epoch 91/9999\n",
            "2/2 - 0s - loss: 0.4400 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 92/9999\n",
            "2/2 - 0s - loss: 0.4305 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 93/9999\n",
            "2/2 - 0s - loss: 0.4217 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 94/9999\n",
            "2/2 - 0s - loss: 0.4128 - categorical_accuracy: 1.0000 - 15ms/epoch - 8ms/step\n",
            "Epoch 95/9999\n",
            "2/2 - 0s - loss: 0.4039 - categorical_accuracy: 1.0000 - 21ms/epoch - 11ms/step\n",
            "Epoch 96/9999\n",
            "2/2 - 0s - loss: 0.3953 - categorical_accuracy: 1.0000 - 13ms/epoch - 6ms/step\n",
            "Epoch 97/9999\n",
            "2/2 - 0s - loss: 0.3872 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 98/9999\n",
            "2/2 - 0s - loss: 0.3803 - categorical_accuracy: 1.0000 - 15ms/epoch - 7ms/step\n",
            "Epoch 99/9999\n",
            "2/2 - 0s - loss: 0.3724 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 100/9999\n",
            "2/2 - 0s - loss: 0.3644 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 101/9999\n",
            "2/2 - 0s - loss: 0.3569 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 102/9999\n",
            "2/2 - 0s - loss: 0.3502 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 103/9999\n",
            "2/2 - 0s - loss: 0.3431 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 104/9999\n",
            "2/2 - 0s - loss: 0.3362 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 105/9999\n",
            "2/2 - 0s - loss: 0.3305 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 106/9999\n",
            "2/2 - 0s - loss: 0.3232 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 107/9999\n",
            "2/2 - 0s - loss: 0.3169 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 108/9999\n",
            "2/2 - 0s - loss: 0.3106 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 109/9999\n",
            "2/2 - 0s - loss: 0.3049 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 110/9999\n",
            "2/2 - 0s - loss: 0.2989 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 111/9999\n",
            "2/2 - 0s - loss: 0.2938 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 112/9999\n",
            "2/2 - 0s - loss: 0.2882 - categorical_accuracy: 1.0000 - 20ms/epoch - 10ms/step\n",
            "Epoch 113/9999\n",
            "2/2 - 0s - loss: 0.2830 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 114/9999\n",
            "2/2 - 0s - loss: 0.2776 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 115/9999\n",
            "2/2 - 0s - loss: 0.2726 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 116/9999\n",
            "2/2 - 0s - loss: 0.2679 - categorical_accuracy: 1.0000 - 18ms/epoch - 9ms/step\n",
            "Epoch 117/9999\n",
            "2/2 - 0s - loss: 0.2627 - categorical_accuracy: 1.0000 - 18ms/epoch - 9ms/step\n",
            "Epoch 118/9999\n",
            "2/2 - 0s - loss: 0.2581 - categorical_accuracy: 1.0000 - 15ms/epoch - 8ms/step\n",
            "Epoch 119/9999\n",
            "2/2 - 0s - loss: 0.2540 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 120/9999\n",
            "2/2 - 0s - loss: 0.2492 - categorical_accuracy: 1.0000 - 13ms/epoch - 7ms/step\n",
            "Epoch 121/9999\n",
            "2/2 - 0s - loss: 0.2450 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 122/9999\n",
            "2/2 - 0s - loss: 0.2408 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 123/9999\n",
            "2/2 - 0s - loss: 0.2368 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 124/9999\n",
            "2/2 - 0s - loss: 0.2323 - categorical_accuracy: 1.0000 - 16ms/epoch - 8ms/step\n",
            "Epoch 125/9999\n",
            "2/2 - 0s - loss: 0.2283 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 126/9999\n",
            "2/2 - 0s - loss: 0.2245 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 127/9999\n",
            "2/2 - 0s - loss: 0.2208 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 128/9999\n",
            "2/2 - 0s - loss: 0.2172 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 129/9999\n",
            "2/2 - 0s - loss: 0.2133 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 130/9999\n",
            "2/2 - 0s - loss: 0.2099 - categorical_accuracy: 1.0000 - 19ms/epoch - 9ms/step\n",
            "Epoch 131/9999\n",
            "2/2 - 0s - loss: 0.2070 - categorical_accuracy: 1.0000 - 15ms/epoch - 8ms/step\n",
            "Epoch 132/9999\n",
            "2/2 - 0s - loss: 0.2033 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 133/9999\n",
            "2/2 - 0s - loss: 0.2002 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 134/9999\n",
            "2/2 - 0s - loss: 0.1966 - categorical_accuracy: 1.0000 - 15ms/epoch - 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f38ca91b6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. evaluate the model on TEST_SET. What accuracy do you obtain? If the accuracy is less than 100%, which test images are misclassified? (You may wish to use the show(image) function you defined in the previous project.)"
      ],
      "metadata": {
        "id": "nO1B3uswgWya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x=TEST_SET_2D, y=unicode_cat_test, verbose=1)\n"
      ],
      "metadata": {
        "id": "7ATf3Nujn4nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7b0bbb-0790-41b2-b9b9-5efb42ba60c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 147ms/step - loss: 1.9708 - categorical_accuracy: 0.5385\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9707945585250854, 0.5384615659713745]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(TEST_SET_2D)  \n",
        "#result = np.absolute(unicode_cat_test-predicted)\n",
        "print(predicted)"
      ],
      "metadata": {
        "id": "WzuPg2m4q5fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e5533c-261d-4a92-a6e6-30375263c010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.62024147e-03 1.88086275e-02 6.74042804e-03 2.00573921e-01\n",
            "  3.67955118e-02 3.56977270e-03 2.08981067e-01 9.34394193e-05\n",
            "  2.16928963e-02 6.66480744e-03 2.62727309e-02 2.33973369e-01\n",
            "  1.08842747e-02 1.88416243e-02 6.11918839e-03 2.41639209e-03\n",
            "  1.13936570e-02 8.83201286e-02 1.11407191e-02 1.50103904e-02\n",
            "  4.71429812e-04 4.12722147e-04 2.62856688e-02 8.10895363e-05\n",
            "  3.77611315e-04 3.94582152e-02]\n",
            " [8.60818662e-04 2.22570688e-01 1.37335202e-02 1.87894925e-02\n",
            "  1.12249956e-01 1.09424330e-02 1.43462734e-03 3.78836594e-05\n",
            "  5.84094934e-02 5.91929711e-04 4.45196591e-02 3.23829241e-02\n",
            "  4.40667709e-03 4.63963603e-04 3.47702357e-04 1.06456257e-01\n",
            "  2.33279192e-03 8.32202658e-02 1.43073471e-02 7.84569010e-02\n",
            "  1.04584331e-04 1.70530460e-04 2.57590960e-04 3.20350664e-04\n",
            "  1.80317927e-02 1.74599826e-01]\n",
            " [7.03377370e-03 9.21904668e-03 5.73405802e-01 6.56894036e-03\n",
            "  1.16929561e-01 1.16385790e-02 8.62679183e-02 2.41287038e-04\n",
            "  4.23466526e-02 4.18441952e-04 8.08461383e-03 7.13957772e-02\n",
            "  3.66315665e-03 3.62658384e-03 5.35306474e-03 1.29150483e-03\n",
            "  7.34205777e-03 6.29734481e-04 1.96281192e-03 1.53243458e-02\n",
            "  4.41191113e-03 1.29140937e-03 3.15198209e-04 7.63037242e-05\n",
            "  8.01012013e-03 1.31514631e-02]\n",
            " [3.17446043e-04 2.09064726e-02 2.01797746e-02 6.99866235e-01\n",
            "  8.56656116e-03 2.71721720e-03 2.29453016e-02 8.63345758e-06\n",
            "  1.25053814e-02 3.58713878e-04 1.63356832e-03 1.33868530e-01\n",
            "  6.03576191e-04 1.01496305e-04 6.46914914e-03 8.83667357e-03\n",
            "  2.15678313e-03 8.71118810e-03 2.00000964e-02 7.90584926e-03\n",
            "  7.32646658e-05 1.79638027e-05 5.21829315e-05 2.21400223e-05\n",
            "  2.36435735e-04 2.09393911e-02]\n",
            " [2.63934693e-04 4.97427024e-03 1.19872764e-03 3.75824049e-04\n",
            "  6.98985338e-01 1.47498166e-02 8.06391414e-04 1.43309180e-05\n",
            "  2.44046804e-02 2.03783580e-04 6.30941093e-02 4.39809589e-03\n",
            "  5.09687408e-04 7.34485802e-04 6.84184579e-06 8.10819771e-03\n",
            "  2.18425394e-05 1.26352198e-02 1.01386930e-03 1.33501425e-01\n",
            "  9.96686322e-06 1.42049263e-04 9.47745411e-06 5.24670351e-04\n",
            "  1.12685990e-02 1.80444196e-02]\n",
            " [1.67348320e-04 4.82516922e-03 3.03520355e-04 8.01589165e-04\n",
            "  9.61775258e-02 4.05343115e-01 2.58095271e-04 8.08368932e-05\n",
            "  2.43592635e-02 2.43019909e-04 2.06651352e-02 1.28114037e-03\n",
            "  2.30625828e-05 9.77821473e-05 3.49195361e-05 1.79565355e-01\n",
            "  7.65848745e-06 2.67588850e-02 1.79431145e-03 1.08586811e-01\n",
            "  7.32157832e-06 1.62390317e-03 3.87747377e-06 3.70976399e-03\n",
            "  1.20094456e-01 3.18618049e-03]\n",
            " [1.42530038e-03 3.97903472e-03 1.09112509e-01 1.22781629e-02\n",
            "  1.26092970e-01 1.20839034e-03 6.05371237e-01 6.34453827e-05\n",
            "  4.17206669e-03 1.22017437e-03 1.35993650e-02 5.48565835e-02\n",
            "  4.03934065e-03 2.29880717e-02 1.65306521e-03 5.28651035e-05\n",
            "  3.58730089e-03 1.15233904e-03 4.87505225e-04 7.81292934e-03\n",
            "  3.57688149e-03 4.95299464e-04 2.31877994e-03 1.96634228e-05\n",
            "  9.44426924e-04 1.74924321e-02]\n",
            " [2.52254307e-03 5.56063615e-02 2.20429152e-04 6.13841927e-03\n",
            "  1.32292900e-02 4.62424941e-03 2.62394547e-03 1.41482688e-02\n",
            "  4.03525279e-04 1.52226421e-03 1.48736671e-01 3.12739685e-02\n",
            "  2.22086430e-01 1.35088414e-01 1.70735910e-03 4.36013285e-03\n",
            "  8.02071241e-04 5.66608757e-02 5.81663148e-03 9.83646489e-04\n",
            "  1.25543273e-03 7.01529533e-03 2.24559575e-01 3.54397967e-02\n",
            "  5.32285718e-04 2.26420015e-02]\n",
            " [6.87617285e-05 3.98350821e-05 5.14910975e-03 2.42565711e-05\n",
            "  1.31134479e-03 8.82488821e-05 9.18598744e-06 3.40281929e-07\n",
            "  9.33155954e-01 1.22331758e-05 1.37962861e-05 2.27481301e-04\n",
            "  1.60999698e-05 4.38420784e-07 2.17656998e-06 1.24233204e-03\n",
            "  5.32288759e-05 9.15691453e-06 4.08269651e-03 4.80703115e-02\n",
            "  9.88086867e-07 9.05233901e-06 7.89208983e-08 1.80409245e-06\n",
            "  4.55073221e-03 1.86046248e-03]\n",
            " [1.39852287e-02 2.01754551e-02 4.91703302e-03 1.07112184e-01\n",
            "  8.92668380e-04 3.53521114e-04 2.66260076e-02 8.34624004e-03\n",
            "  4.55106318e-04 4.55199867e-01 8.26755259e-03 3.27840768e-04\n",
            "  1.16058579e-02 1.18505750e-02 2.07087517e-01 1.02749118e-03\n",
            "  1.25344396e-02 1.33774281e-02 2.18847990e-02 4.15075896e-03\n",
            "  1.89852100e-02 1.16845863e-02 5.07974392e-03 1.60395391e-02\n",
            "  7.39963027e-03 1.06337536e-02]\n",
            " [8.77410173e-04 2.44990923e-02 1.99288392e-04 2.94633815e-03\n",
            "  1.23408705e-01 9.67640355e-02 5.51663153e-03 5.83607005e-04\n",
            "  2.53268704e-03 2.19640904e-03 4.36016917e-01 1.25633655e-02\n",
            "  1.96782150e-03 8.07526987e-03 3.24434106e-04 1.85276680e-02\n",
            "  4.86587342e-05 1.92263767e-01 3.93458270e-03 1.75964888e-02\n",
            "  6.77666467e-05 1.71531050e-03 6.65339525e-04 1.86311547e-02\n",
            "  7.97046814e-03 2.01067813e-02]\n",
            " [5.70557371e-04 2.16112398e-02 2.67838053e-02 1.85160115e-02\n",
            "  3.03510487e-01 1.59503855e-02 1.46074528e-02 2.90711869e-05\n",
            "  8.68515968e-02 1.70642874e-04 2.12131087e-02 3.19270551e-01\n",
            "  1.46586890e-03 1.39067229e-03 1.69241888e-04 1.78103074e-02\n",
            "  9.06504923e-04 2.47708466e-02 1.33989127e-02 7.26448968e-02\n",
            "  2.30648191e-04 5.53963328e-05 9.51507245e-05 1.17933458e-04\n",
            "  2.12779944e-03 3.57308760e-02]\n",
            " [5.08501902e-02 9.67857148e-03 1.32518914e-02 2.84927664e-03\n",
            "  1.68153822e-01 2.24552527e-01 9.79695655e-03 1.01991091e-02\n",
            "  2.39744298e-02 1.22655020e-03 8.39907825e-02 5.08952923e-02\n",
            "  1.14891799e-02 2.73982771e-02 5.49567211e-03 3.90801243e-02\n",
            "  7.15902075e-04 1.92158949e-02 5.84494732e-02 2.76215244e-02\n",
            "  7.82213546e-03 1.39515912e-02 1.08307099e-03 1.00564137e-01\n",
            "  1.31912353e-02 2.45023388e-02]\n",
            " [8.70642439e-03 4.39662971e-02 5.60827029e-04 2.73208669e-03\n",
            "  8.17148015e-02 3.10836197e-03 2.20000464e-02 2.90752552e-03\n",
            "  8.29943339e-04 1.05146843e-03 2.02983245e-01 2.72590220e-02\n",
            "  1.71550378e-01 2.55845189e-01 2.40774453e-03 1.03752618e-03\n",
            "  6.51974464e-04 9.24483314e-02 2.02060696e-02 5.42221777e-03\n",
            "  1.66476076e-03 2.70710373e-03 1.87587328e-02 4.55622282e-03\n",
            "  1.46404869e-04 2.47771535e-02]\n",
            " [9.57036857e-03 1.17524182e-02 1.59714013e-01 2.34877616e-01\n",
            "  7.95950275e-03 4.16919356e-03 1.56755105e-01 1.52780427e-04\n",
            "  8.82565044e-03 7.15190172e-03 3.01710796e-03 4.53159586e-02\n",
            "  1.34272324e-02 4.55144560e-03 2.91464806e-01 7.06606545e-04\n",
            "  9.04379133e-03 3.01988889e-03 4.53627855e-03 2.56321859e-03\n",
            "  3.66128259e-03 1.13876455e-03 1.33491319e-03 1.50268985e-04\n",
            "  6.79713208e-04 1.44601576e-02]\n",
            " [2.30635647e-04 3.17957066e-02 4.39595489e-04 6.90609636e-03\n",
            "  8.33062828e-03 9.61204097e-02 1.28682223e-04 9.44310596e-05\n",
            "  1.03205889e-02 2.22820338e-04 5.61844092e-03 1.94352458e-03\n",
            "  8.16245883e-05 3.33692733e-05 3.99746583e-04 6.62354529e-01\n",
            "  2.53303704e-04 8.95579308e-02 8.36520735e-03 3.09541784e-02\n",
            "  1.30640929e-05 8.98503116e-04 2.09746668e-05 1.30583823e-03\n",
            "  3.94876488e-02 4.12258552e-03]\n",
            " [9.23050288e-03 5.55899227e-03 2.32804418e-02 2.50485867e-01\n",
            "  1.27172498e-02 2.20524590e-03 3.48562419e-01 4.48996609e-04\n",
            "  9.20477672e-04 9.45307780e-03 1.61160238e-03 4.00716998e-02\n",
            "  5.29007986e-03 6.34231977e-03 6.43386990e-02 8.16684158e-04\n",
            "  1.54855222e-01 4.47755959e-03 3.84074100e-03 3.02088866e-03\n",
            "  7.79795088e-03 1.02989515e-02 1.73415057e-02 1.84259232e-04\n",
            "  2.44571036e-03 1.44028692e-02]\n",
            " [1.80513877e-03 7.65546933e-02 7.57103262e-04 6.82464913e-02\n",
            "  4.56216112e-02 9.81865898e-02 3.84945003e-03 5.13291801e-04\n",
            "  9.59741231e-03 2.41469056e-03 3.44405361e-02 2.48834454e-02\n",
            "  1.22693426e-03 7.80959381e-04 3.03035323e-03 2.14738816e-01\n",
            "  1.50647608e-03 3.29467922e-01 1.46786775e-02 3.33052240e-02\n",
            "  6.00829298e-05 2.14685337e-03 6.64520136e-04 2.81645474e-03\n",
            "  1.52615430e-02 1.34446202e-02]\n",
            " [5.09048440e-03 1.40019402e-01 1.97109714e-01 1.11836856e-02\n",
            "  4.93614078e-02 9.19181202e-03 1.21876933e-02 1.55918428e-03\n",
            "  3.44259813e-02 9.77791264e-04 3.38474400e-02 3.20883095e-02\n",
            "  9.05105658e-03 1.42136181e-03 2.76401802e-03 3.37795727e-02\n",
            "  2.28963085e-02 1.25373648e-02 7.50773177e-02 9.95704830e-02\n",
            "  3.67668131e-03 1.90481031e-03 7.51924352e-04 5.43818763e-03\n",
            "  5.86933419e-02 1.45394728e-01]\n",
            " [1.33805424e-02 6.97216019e-03 3.02688070e-02 2.85674520e-02\n",
            "  2.75046080e-01 5.36817452e-03 1.49730071e-02 1.29017020e-02\n",
            "  1.02632632e-02 1.99978426e-02 4.68303682e-03 3.20792315e-03\n",
            "  5.01740677e-03 6.84431940e-03 1.83958421e-03 8.27699807e-03\n",
            "  1.37441121e-02 1.97881646e-03 7.48408586e-02 2.66466469e-01\n",
            "  2.76604686e-02 2.32736375e-02 4.78144415e-04 3.50317284e-02\n",
            "  8.12557340e-02 2.76618470e-02]\n",
            " [3.02111963e-03 6.10777996e-02 3.60160053e-01 1.31101245e-02\n",
            "  2.80872360e-02 1.59128965e-03 5.59829585e-02 1.95157155e-03\n",
            "  8.64878821e-04 7.63122152e-05 1.08814267e-02 1.72488034e-01\n",
            "  3.46593894e-02 2.01743301e-02 3.74992341e-02 2.28074452e-04\n",
            "  3.91321350e-03 7.04710372e-04 8.01262911e-03 6.32550102e-04\n",
            "  1.16346277e-01 1.77865359e-03 1.26255406e-02 6.67990767e-04\n",
            "  2.67613126e-04 5.31968847e-02]\n",
            " [1.11080585e-02 1.80391837e-02 4.84204173e-01 4.33706753e-02\n",
            "  1.68712027e-02 7.85530638e-03 7.20332041e-02 9.24404524e-03\n",
            "  1.14586502e-02 1.23822328e-03 2.89670797e-03 8.35664645e-02\n",
            "  6.36206986e-03 4.42144461e-03 4.07872051e-02 2.19815271e-03\n",
            "  2.11315472e-02 4.31710097e-04 1.09228604e-02 4.72178124e-03\n",
            "  8.86912793e-02 1.08870994e-02 7.99248926e-03 1.94718142e-03\n",
            "  1.05657307e-02 2.70535648e-02]\n",
            " [8.84528458e-03 1.15402276e-02 1.10292539e-01 3.24830115e-02\n",
            "  1.12416089e-01 1.26481550e-02 3.58709283e-02 6.59339502e-03\n",
            "  3.14377691e-03 4.35189623e-03 1.59589685e-02 1.24459624e-01\n",
            "  2.00773608e-02 2.57338211e-02 5.66905877e-03 1.73515789e-02\n",
            "  9.27027389e-02 6.73321402e-03 4.90226038e-02 4.85701440e-03\n",
            "  1.34750292e-01 4.50846963e-02 2.25869101e-02 3.47193778e-02\n",
            "  2.47591343e-02 3.73482481e-02]\n",
            " [3.68536562e-02 2.46288143e-02 5.48815494e-03 2.27081100e-03\n",
            "  5.86792566e-02 4.75812564e-03 2.47291569e-03 9.98119707e-04\n",
            "  9.11785737e-02 3.52628976e-02 8.43934193e-02 3.56134563e-03\n",
            "  9.10530314e-02 3.33869047e-02 5.06766140e-04 4.48920019e-02\n",
            "  8.25473107e-03 6.26436025e-02 6.21351041e-03 9.39205959e-02\n",
            "  1.37726788e-03 5.89270703e-03 8.14122334e-03 3.92503198e-03\n",
            "  1.50919780e-01 1.38326794e-01]\n",
            " [1.46089911e-01 1.92070436e-02 3.98425683e-02 1.85189638e-02\n",
            "  6.62748516e-02 1.49877078e-03 8.85114074e-03 2.36171521e-02\n",
            "  1.20931929e-02 6.88335970e-02 1.42601049e-02 4.28407500e-03\n",
            "  3.52639146e-02 5.39505258e-02 3.65770259e-03 1.95488594e-02\n",
            "  7.99306557e-02 6.48445776e-03 3.02099157e-02 3.20478007e-02\n",
            "  8.52513835e-02 1.00405905e-02 1.49093615e-02 1.89014636e-02\n",
            "  9.76721048e-02 8.87598172e-02]\n",
            " [6.00212626e-03 8.39182362e-03 6.87455162e-02 1.57144088e-02\n",
            "  8.25657099e-02 2.77457014e-03 2.14865804e-03 3.30166513e-04\n",
            "  4.80591431e-02 7.84990191e-03 4.85159736e-03 2.62593967e-03\n",
            "  8.64602160e-03 9.75279079e-04 6.69263420e-04 3.73777226e-02\n",
            "  2.78230868e-02 3.86042451e-03 3.32880355e-02 2.55931646e-01\n",
            "  1.87987729e-03 9.57357977e-03 3.85680876e-04 4.59742779e-03\n",
            "  2.16662958e-01 1.48269430e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the actual and predicted letters from TEST_SET_2D.\n",
        "result_list = []\n",
        "for num in range(len(predicted)):\n",
        "    max_val = 0\n",
        "    actual = chr(num+ord('A'))\n",
        "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    result_list.append([actual, predict])\n",
        "result_list"
      ],
      "metadata": {
        "id": "NGiUIDJTz3v0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb17b9b-c9ef-4dd1-a6df-0921f739f1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A', 'L'],\n",
              " ['B', 'B'],\n",
              " ['C', 'C'],\n",
              " ['D', 'D'],\n",
              " ['E', 'E'],\n",
              " ['F', 'F'],\n",
              " ['G', 'G'],\n",
              " ['H', 'W'],\n",
              " ['I', 'I'],\n",
              " ['J', 'J'],\n",
              " ['K', 'K'],\n",
              " ['L', 'L'],\n",
              " ['M', 'F'],\n",
              " ['N', 'N'],\n",
              " ['O', 'O'],\n",
              " ['P', 'P'],\n",
              " ['Q', 'G'],\n",
              " ['R', 'R'],\n",
              " ['S', 'C'],\n",
              " ['T', 'E'],\n",
              " ['U', 'C'],\n",
              " ['V', 'C'],\n",
              " ['W', 'U'],\n",
              " ['X', 'Y'],\n",
              " ['Y', 'A'],\n",
              " ['Z', 'T']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the accuracy obtained for evaluate().\n",
        "list_misclassified = []\n",
        "num_total, num_correct = 26, 26\n",
        "for i in range(len(result_list)):\n",
        "    if result_list[i][0] != result_list[i][1]:\n",
        "        num_correct -= 1\n",
        "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
        "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_misclassified) == 0:\n",
        "    print(\"All test images are classified correctly.\")\n",
        "else:\n",
        "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_misclassified)):\n",
        "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
      ],
      "metadata": {
        "id": "-hxWTm_Qz5bE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56b93c2-3522-4e0c-c21e-f42bd4b20506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images correctly classified: 53.84615384615385%\n",
            "Here are the list of test images that are misclassified and how they appear:\n",
            "Actual    Predicted\n",
            "A         L\n",
            "H         W\n",
            "M         F\n",
            "Q         G\n",
            "S         C\n",
            "T         E\n",
            "U         C\n",
            "V         C\n",
            "W         U\n",
            "X         Y\n",
            "Y         A\n",
            "Z         T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict how the message will appear via Keras Sequential model.\n",
        "predicted_message = model.predict(MESSAGE_2D)\n",
        "predicted_message"
      ],
      "metadata": {
        "id": "N-pRa2Pq4lg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f6ca7f-4821-4fdd-d6aa-b595c6036c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.06684529e-03, 6.00511348e-03, 3.12305391e-02, 3.28231528e-02,\n",
              "        2.40790457e-01, 3.69007443e-03, 1.48184570e-02, 8.07532854e-03,\n",
              "        9.65750590e-03, 2.12320052e-02, 4.02373355e-03, 3.69393337e-03,\n",
              "        5.93178626e-03, 4.28860076e-03, 1.88718224e-03, 7.22557493e-03,\n",
              "        1.87982991e-02, 1.98706752e-03, 8.14012215e-02, 3.12274575e-01,\n",
              "        1.81998461e-02, 2.19776202e-02, 5.09078032e-04, 2.88584437e-02,\n",
              "        7.75834993e-02, 3.39700915e-02],\n",
              "       [3.70929576e-03, 6.15910254e-02, 2.70335702e-04, 5.27712330e-03,\n",
              "        1.21542374e-02, 4.89033479e-03, 2.42968276e-03, 2.29687225e-02,\n",
              "        4.83892247e-04, 1.39035471e-03, 1.00816987e-01, 2.83076130e-02,\n",
              "        2.28362605e-01, 1.31448150e-01, 2.02286337e-03, 4.42923699e-03,\n",
              "        8.24472285e-04, 4.16094474e-02, 5.67501225e-03, 7.56212568e-04,\n",
              "        2.28757714e-03, 9.29713063e-03, 2.73916155e-01, 3.43987681e-02,\n",
              "        6.20838313e-04, 2.00619139e-02],\n",
              "       [6.91725756e-04, 1.15129743e-02, 9.94155766e-04, 5.79104200e-03,\n",
              "        6.80851281e-01, 1.02410175e-01, 3.78287653e-03, 2.93906633e-04,\n",
              "        5.99108217e-03, 5.48646203e-05, 5.96196018e-02, 7.01303631e-02,\n",
              "        8.43351707e-04, 8.67396768e-04, 2.10251208e-04, 1.70607977e-02,\n",
              "        4.08347114e-06, 1.77461579e-02, 1.66345562e-03, 4.27710777e-03,\n",
              "        1.64696961e-04, 4.23938123e-04, 7.60277762e-05, 1.07912265e-03,\n",
              "        2.66796118e-03, 1.07916333e-02],\n",
              "       [2.03840900e-04, 4.20460198e-03, 4.33326612e-04, 1.00009586e-03,\n",
              "        2.41434336e-01, 6.11284316e-01, 5.68723946e-04, 2.59074324e-04,\n",
              "        2.29344331e-03, 1.04276251e-05, 2.68539768e-02, 7.42473546e-03,\n",
              "        5.91386379e-05, 1.75911657e-04, 9.57042794e-05, 7.07504973e-02,\n",
              "        1.83309464e-06, 1.21318111e-02, 4.16860828e-04, 3.99518525e-03,\n",
              "        3.98197626e-05, 1.88393437e-03, 6.19220236e-06, 2.96142185e-03,\n",
              "        1.04298592e-02, 1.08098518e-03],\n",
              "       [1.03763363e-03, 3.11974523e-04, 1.48832667e-04, 2.24582018e-05,\n",
              "        1.01006040e-02, 6.41773222e-04, 3.50902083e-05, 3.44788095e-05,\n",
              "        6.06867909e-01, 1.73644698e-03, 5.13467763e-04, 6.20992141e-05,\n",
              "        1.92898617e-04, 5.74433943e-05, 5.61123716e-06, 4.03978257e-03,\n",
              "        1.86400193e-05, 1.28716812e-03, 1.14565268e-02, 3.32800597e-01,\n",
              "        1.19870892e-05, 3.30072042e-04, 3.50787127e-06, 3.16819700e-04,\n",
              "        2.41075009e-02, 3.85866268e-03],\n",
              "       [2.45474570e-04, 3.81744409e-04, 4.41063821e-05, 5.75013517e-04,\n",
              "        1.18514516e-04, 7.92306400e-06, 2.15222244e-03, 3.59463319e-03,\n",
              "        4.88813612e-07, 1.28996978e-02, 3.05692010e-05, 2.08777962e-07,\n",
              "        4.37381503e-04, 3.66160908e-04, 6.82116486e-04, 1.08799686e-05,\n",
              "        1.34690739e-02, 4.31488043e-05, 1.35920069e-03, 4.01514299e-05,\n",
              "        2.94730105e-02, 9.21163857e-01, 5.81565255e-04, 1.31498254e-03,\n",
              "        1.09210154e-02, 8.67878116e-05],\n",
              "       [2.17724941e-04, 3.69550963e-03, 8.96256650e-04, 2.30027625e-04,\n",
              "        7.47355461e-01, 8.57284106e-03, 5.34332416e-04, 1.08225850e-05,\n",
              "        2.04660334e-02, 1.21282079e-04, 6.43783733e-02, 4.37582564e-03,\n",
              "        4.95986431e-04, 6.93765003e-04, 3.62079618e-06, 5.36521524e-03,\n",
              "        1.39023459e-05, 1.06184874e-02, 1.13799691e-03, 1.11735009e-01,\n",
              "        7.24382016e-06, 6.08353985e-05, 6.43182284e-06, 4.12401714e-04,\n",
              "        4.85995412e-03, 1.37346350e-02],\n",
              "       [3.50689952e-04, 8.24074328e-01, 5.65450464e-04, 3.10456902e-02,\n",
              "        6.50324486e-03, 2.76547414e-03, 4.08048101e-04, 2.20315720e-04,\n",
              "        1.74940506e-04, 9.79773104e-05, 1.52282650e-02, 8.76085926e-03,\n",
              "        9.75297880e-04, 3.61766142e-04, 4.72275424e-04, 1.76944099e-02,\n",
              "        9.34384239e-04, 5.12855873e-02, 6.46515284e-03, 5.64032933e-04,\n",
              "        5.49936143e-04, 5.35785985e-05, 1.43096840e-03, 5.02143288e-04,\n",
              "        8.14364408e-04, 2.77007502e-02],\n",
              "       [1.20235961e-02, 2.30752658e-02, 1.17342854e-02, 1.50286527e-02,\n",
              "        8.49418793e-05, 2.05514662e-05, 1.54114170e-02, 2.37897318e-02,\n",
              "        9.38141511e-06, 7.76526053e-04, 8.79235959e-05, 4.30946908e-04,\n",
              "        9.56822652e-04, 1.53501763e-03, 6.10447884e-01, 3.89623201e-05,\n",
              "        1.02516741e-01, 1.87835438e-04, 2.02413648e-02, 1.22539073e-04,\n",
              "        1.37009218e-01, 7.32248602e-03, 1.53697589e-02, 1.82560034e-04,\n",
              "        1.81781201e-04, 1.41387177e-03],\n",
              "       [3.97000909e-02, 1.67517662e-02, 5.15598152e-03, 2.36375001e-03,\n",
              "        5.60627393e-02, 2.20088428e-03, 2.14009406e-03, 1.24261517e-03,\n",
              "        9.30956528e-02, 3.87077294e-02, 6.20358326e-02, 2.35044910e-03,\n",
              "        1.03626348e-01, 3.30398865e-02, 3.37319419e-04, 3.27911302e-02,\n",
              "        8.01428594e-03, 4.00904566e-02, 8.20507482e-03, 1.26065865e-01,\n",
              "        1.47832045e-03, 4.92993463e-03, 7.82064721e-03, 3.70964152e-03,\n",
              "        1.49469718e-01, 1.58613726e-01],\n",
              "       [2.53443704e-05, 4.03413687e-05, 1.08882505e-03, 4.19383068e-05,\n",
              "        1.61923841e-03, 3.97429510e-04, 3.12265774e-06, 1.06530763e-07,\n",
              "        8.99991393e-01, 9.18561568e-07, 5.56995792e-06, 3.75623116e-04,\n",
              "        1.00163334e-06, 4.47152466e-08, 6.01836007e-07, 4.05254774e-03,\n",
              "        9.06879995e-06, 2.30518090e-05, 3.30224936e-03, 8.75160024e-02,\n",
              "        6.31045722e-08, 6.54112227e-07, 5.14955811e-09, 5.34537207e-07,\n",
              "        1.15010561e-03, 3.54247837e-04],\n",
              "       [1.97920594e-02, 2.21524225e-03, 7.52778433e-05, 6.36030571e-04,\n",
              "        1.79337710e-03, 4.14540373e-05, 4.62232297e-03, 2.72288527e-02,\n",
              "        5.31164505e-06, 2.29824660e-03, 5.93870180e-03, 2.58913224e-05,\n",
              "        1.89301193e-01, 6.81991398e-01, 3.96763161e-03, 2.24831037e-05,\n",
              "        7.82719362e-05, 1.30326394e-03, 6.98989723e-04, 1.22533980e-04,\n",
              "        1.48589583e-02, 1.98292695e-02, 1.57711357e-02, 5.29325148e-03,\n",
              "        1.35323964e-04, 1.95350847e-03],\n",
              "       [7.96193723e-04, 7.10330380e-04, 1.09843940e-01, 2.41630152e-03,\n",
              "        2.95045064e-03, 4.50724183e-05, 8.51896286e-01, 4.27765117e-05,\n",
              "        4.23038800e-05, 9.38114172e-05, 1.15950336e-03, 5.87746175e-03,\n",
              "        8.25868454e-04, 5.37526654e-03, 6.37035491e-03, 6.79309380e-07,\n",
              "        2.92819669e-03, 1.83782377e-05, 3.28866678e-04, 1.93681743e-04,\n",
              "        2.13584770e-03, 9.08492148e-05, 4.24661528e-04, 4.16653984e-06,\n",
              "        6.97658033e-05, 5.35892835e-03],\n",
              "       [3.86872794e-03, 1.22245832e-03, 1.92118587e-05, 8.44124355e-04,\n",
              "        4.60203555e-05, 1.31679326e-06, 1.12568378e-03, 6.00030646e-03,\n",
              "        1.07206347e-06, 1.96516412e-04, 1.51445667e-04, 8.52063094e-05,\n",
              "        2.82037016e-02, 4.50814776e-02, 4.39132506e-04, 1.81100177e-06,\n",
              "        2.75848294e-03, 9.64510837e-05, 2.31473430e-04, 3.68004362e-06,\n",
              "        7.46555440e-03, 1.05111790e-03, 8.97455394e-01, 6.98853924e-04,\n",
              "        4.62722801e-06, 2.94613442e-03],\n",
              "       [1.43849850e-03, 2.08976388e-04, 1.97430578e-04, 1.38265505e-05,\n",
              "        7.16627948e-03, 4.23880585e-04, 3.11349977e-05, 2.16666285e-05,\n",
              "        7.76472986e-01, 6.48916815e-04, 2.26311240e-04, 1.25972714e-04,\n",
              "        1.59095362e-04, 3.02404260e-05, 4.94323331e-06, 1.77233375e-03,\n",
              "        3.25904148e-05, 4.50442516e-04, 8.32240935e-03, 1.92546919e-01,\n",
              "        7.47471586e-06, 1.04446997e-04, 3.33418211e-06, 1.18881326e-04,\n",
              "        6.40393235e-03, 3.06706876e-03],\n",
              "       [9.75815486e-03, 1.01244673e-02, 9.86646414e-02, 2.38991156e-02,\n",
              "        1.27038032e-01, 5.22285234e-03, 3.58231715e-03, 5.53693972e-04,\n",
              "        4.72329296e-02, 7.69140618e-03, 4.44778102e-03, 4.69970796e-03,\n",
              "        9.86527000e-03, 1.59036042e-03, 1.46229367e-03, 3.73781100e-02,\n",
              "        2.96843220e-02, 3.08268680e-03, 4.78802286e-02, 1.93695858e-01,\n",
              "        3.17015173e-03, 1.17261857e-02, 3.86185478e-04, 5.20163635e-03,\n",
              "        1.78830341e-01, 1.33131295e-01],\n",
              "       [8.24831009e-01, 6.53743511e-04, 4.59752628e-04, 2.11447291e-03,\n",
              "        2.01382631e-04, 1.44797683e-04, 1.34818209e-02, 5.37322881e-03,\n",
              "        3.66268621e-04, 8.63855053e-03, 1.98421083e-04, 3.16735997e-04,\n",
              "        7.31587932e-02, 1.13147907e-02, 2.29731146e-02, 1.44789941e-04,\n",
              "        4.61490033e-03, 6.67903514e-04, 4.27430496e-03, 3.73207804e-05,\n",
              "        8.42611026e-03, 2.51572905e-03, 1.25294030e-02, 8.48313561e-04,\n",
              "        1.27979249e-04, 1.58637960e-03],\n",
              "       [1.89358881e-03, 9.15024728e-02, 6.52336224e-04, 8.46081153e-02,\n",
              "        4.28902358e-02, 7.61517957e-02, 4.67995740e-03, 5.17836888e-04,\n",
              "        9.46816616e-03, 2.67156237e-03, 3.35981697e-02, 3.39431912e-02,\n",
              "        1.69125851e-03, 8.42949259e-04, 4.13021166e-03, 1.66033641e-01,\n",
              "        2.10967055e-03, 3.59478503e-01, 2.13657953e-02, 3.35702673e-02,\n",
              "        5.11850958e-05, 1.94565614e-03, 8.97687161e-04, 2.30895309e-03,\n",
              "        8.88104551e-03, 1.41156744e-02],\n",
              "       [3.47638153e-04, 2.37908550e-02, 2.51390990e-02, 7.14345038e-01,\n",
              "        5.45312185e-03, 2.37970497e-03, 2.87985243e-02, 9.77058426e-06,\n",
              "        1.02356430e-02, 5.28813980e-04, 2.13741278e-03, 1.10884726e-01,\n",
              "        4.06019128e-04, 9.41742837e-05, 7.37936469e-03, 9.06277448e-03,\n",
              "        3.32778227e-03, 7.83314928e-03, 1.78515762e-02, 6.76731486e-03,\n",
              "        7.21743927e-05, 2.03160434e-05, 6.78762954e-05, 2.30753431e-05,\n",
              "        3.47979803e-04, 2.26959437e-02],\n",
              "       [2.73714238e-03, 2.58198846e-02, 1.07372901e-03, 5.18208882e-03,\n",
              "        1.66638347e-03, 3.07430426e-04, 7.57920649e-03, 3.51674622e-03,\n",
              "        1.94743800e-03, 3.32929962e-03, 1.50480820e-03, 1.13810680e-03,\n",
              "        7.39416515e-04, 1.79177721e-03, 5.70413982e-03, 1.90339098e-03,\n",
              "        3.16303363e-03, 6.26587728e-03, 8.81368935e-01, 1.90549139e-02,\n",
              "        5.05407434e-03, 2.92171724e-03, 6.31469302e-04, 1.17641094e-03,\n",
              "        2.28693103e-03, 1.21356165e-02],\n",
              "       [7.54105672e-03, 3.62809282e-04, 4.18869859e-05, 2.98102712e-03,\n",
              "        3.58128280e-04, 8.35157607e-06, 6.56178803e-04, 8.41671659e-04,\n",
              "        8.43492089e-05, 9.65354681e-01, 3.09005380e-04, 2.52971267e-06,\n",
              "        1.82692963e-03, 2.04836787e-03, 1.11888675e-03, 6.99356897e-05,\n",
              "        1.01435999e-03, 1.09379983e-03, 3.49345640e-03, 4.47333645e-04,\n",
              "        2.46995874e-03, 1.18043169e-03, 3.52143164e-04, 3.03683686e-03,\n",
              "        2.21528928e-03, 1.09042425e-03],\n",
              "       [9.41969361e-03, 2.33124429e-03, 3.37996840e-04, 1.25777384e-03,\n",
              "        3.95680981e-04, 3.88389162e-06, 1.26487389e-03, 1.30063713e-01,\n",
              "        1.65933307e-06, 2.56260741e-03, 2.70566543e-05, 8.38856522e-06,\n",
              "        2.53004953e-03, 3.47078848e-03, 8.60211381e-04, 2.15259365e-06,\n",
              "        6.67356467e-03, 1.03306365e-05, 3.92028224e-03, 2.92972927e-05,\n",
              "        8.18831265e-01, 4.23613004e-03, 7.66998297e-03, 3.31395213e-03,\n",
              "        3.28901893e-04, 4.48554754e-04],\n",
              "       [2.69756396e-03, 1.77739875e-03, 2.71995134e-07, 2.29954894e-05,\n",
              "        1.40288589e-03, 2.07997027e-06, 4.54086585e-05, 1.03973178e-02,\n",
              "        1.24598364e-06, 1.58793773e-04, 3.83345550e-03, 7.02921852e-06,\n",
              "        8.64719629e-01, 7.34890401e-02, 2.31381796e-06, 4.40079475e-06,\n",
              "        1.02792774e-05, 2.94151017e-04, 1.68906976e-04, 2.05723518e-05,\n",
              "        1.13223716e-04, 2.33274186e-04, 2.74728239e-02, 7.90477265e-03,\n",
              "        8.93075685e-06, 5.21130441e-03],\n",
              "       [6.69942587e-04, 2.60327663e-02, 6.94810820e-04, 7.09299976e-03,\n",
              "        6.48040846e-02, 1.62212387e-01, 5.34764491e-04, 1.00516703e-03,\n",
              "        1.35674386e-03, 7.23676712e-05, 3.82164456e-02, 6.87909964e-03,\n",
              "        1.12890571e-04, 2.33804079e-04, 2.83020345e-04, 5.57316601e-01,\n",
              "        5.57021085e-05, 5.54679297e-02, 4.39447351e-03, 5.08506875e-03,\n",
              "        1.84074059e-04, 2.25000456e-03, 5.77732390e-05, 1.20957503e-02,\n",
              "        4.59437408e-02, 6.94756489e-03],\n",
              "       [1.43653294e-03, 2.53464724e-03, 4.50122030e-03, 1.10916188e-02,\n",
              "        1.78834816e-05, 3.27347698e-06, 7.28095975e-03, 4.93238261e-03,\n",
              "        1.68314966e-06, 6.72412280e-04, 2.09022473e-05, 8.72387536e-05,\n",
              "        1.24965896e-04, 3.71688162e-04, 4.95053418e-02, 1.37379984e-05,\n",
              "        8.73179197e-01, 6.96244751e-05, 6.59218291e-03, 1.25913706e-04,\n",
              "        2.17676740e-02, 3.21141956e-03, 1.15578370e-02, 2.49047764e-04,\n",
              "        1.92162173e-04, 4.58368246e-04],\n",
              "       [1.60716325e-02, 1.51653662e-02, 3.93420411e-03, 6.69853576e-03,\n",
              "        9.49968744e-05, 1.47675464e-05, 8.18527211e-03, 1.10718250e-01,\n",
              "        5.44034492e-06, 1.03141239e-03, 2.24047282e-04, 2.45192874e-04,\n",
              "        5.27418777e-03, 1.35472436e-02, 1.15218028e-01, 1.64634239e-05,\n",
              "        2.50645019e-02, 1.05098217e-04, 3.11347190e-03, 4.70139494e-05,\n",
              "        4.95889574e-01, 1.75229535e-02, 1.59022257e-01, 9.63128929e-04,\n",
              "        2.15409411e-04, 1.61151367e-03],\n",
              "       [6.36909817e-06, 1.54928948e-05, 7.47471116e-04, 1.06696207e-05,\n",
              "        6.85276987e-04, 1.99307280e-04, 1.01080616e-06, 2.31006503e-08,\n",
              "        9.06828701e-01, 2.17674554e-07, 2.71932504e-06, 1.73830776e-04,\n",
              "        2.75675177e-07, 1.75397918e-08, 2.11801321e-07, 2.01683305e-03,\n",
              "        2.21649975e-06, 8.44818351e-06, 2.20122235e-03, 8.62173885e-02,\n",
              "        1.85070288e-08, 2.34692791e-07, 1.06714004e-09, 1.92727427e-07,\n",
              "        6.18851162e-04, 2.62967456e-04],\n",
              "       [7.34810252e-04, 1.02811772e-03, 8.93645167e-01, 3.19522968e-03,\n",
              "        1.95103872e-03, 2.46346899e-04, 5.39778247e-02, 9.80767873e-05,\n",
              "        8.48725205e-04, 1.64180437e-05, 5.08992583e-04, 1.45699540e-02,\n",
              "        1.93605301e-04, 4.12216206e-04, 1.48604531e-02, 6.13759403e-05,\n",
              "        3.09130014e-03, 2.47556836e-05, 9.21229715e-04, 2.91649980e-04,\n",
              "        4.95343423e-03, 7.34970788e-04, 2.93244171e-04, 9.57910834e-06,\n",
              "        5.07839140e-04, 2.82343850e-03],\n",
              "       [4.20874450e-04, 2.17727348e-02, 1.95383418e-05, 5.22902294e-04,\n",
              "        4.68767434e-02, 1.33944778e-02, 6.18865772e-04, 7.42965145e-04,\n",
              "        9.00033119e-05, 2.25529162e-04, 7.78506875e-01, 7.65021250e-04,\n",
              "        2.72576488e-03, 7.47112883e-03, 2.48187716e-05, 1.25512118e-02,\n",
              "        4.46103542e-07, 7.00884610e-02, 4.53446177e-04, 5.25352021e-04,\n",
              "        1.12234848e-04, 6.39056496e-04, 2.85823800e-04, 1.92826465e-02,\n",
              "        4.77263378e-03, 1.71102621e-02],\n",
              "       [1.63209224e-05, 5.80160553e-03, 1.97723834e-03, 4.77590365e-03,\n",
              "        4.67592031e-02, 8.35404918e-03, 2.19693454e-03, 2.01060857e-06,\n",
              "        8.45169765e-04, 5.63916274e-07, 2.29576370e-03, 9.18990612e-01,\n",
              "        1.83285680e-04, 1.89913451e-04, 4.59246221e-05, 8.74276098e-04,\n",
              "        1.59264218e-05, 1.87225535e-03, 5.23133611e-04, 1.86439240e-04,\n",
              "        1.03954480e-04, 4.43963745e-06, 4.19137359e-05, 1.64941212e-05,\n",
              "        1.00068055e-05, 3.91662354e-03],\n",
              "       [9.06608941e-04, 5.00301411e-03, 3.23447341e-04, 1.36471208e-04,\n",
              "        1.21465521e-02, 5.77751035e-03, 1.38162606e-04, 4.66820697e-04,\n",
              "        2.00977572e-03, 3.98020260e-03, 4.53710705e-02, 3.81357131e-05,\n",
              "        1.11623063e-04, 1.16940099e-03, 2.69989505e-05, 3.08270268e-02,\n",
              "        5.87426257e-05, 7.18095945e-03, 2.58865603e-03, 4.08432446e-02,\n",
              "        1.62688084e-04, 4.56563383e-03, 5.54175895e-05, 2.16115322e-02,\n",
              "        7.88110256e-01, 2.63900030e-02]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_list = \"\"\n",
        "for num in range(len(predicted_message)):\n",
        "    max_val = 0\n",
        "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    message_list += predict\n",
        "print(message_list)"
      ],
      "metadata": {
        "id": "O4oXJz4r4mp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b09ce39-0515-4d5f-b36c-d88a926389de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWEFIVEBOZINGWITARDSJUMPQUICKLY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
        "list_mismatch = []\n",
        "num_total, num_correct = len(actual_message), len(message_list)\n",
        "for i in range(len(actual_message)):\n",
        "    if message_list[i] != actual_message[i]:\n",
        "        num_correct -= 1\n",
        "        list_mismatch.append([actual_message[i], message_list[i]])\n",
        "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_mismatch) == 0:\n",
        "    print(\"The message appears to be decoded correctly.\")\n",
        "else:\n",
        "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_mismatch)):\n",
        "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YdutdB8SX6T",
        "outputId": "88eba13f-5f1c-4236-a901-ea92a649d49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy between predicted and actual message: 90.32258064516128%\n",
            "Here is the list of letters that are a mismatch between predicted and actual message:\n",
            "Actual    Predicted\n",
            "H         W\n",
            "X         Z\n",
            "Z         T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. How does this model compare with the performance of your perceptron models in Project 1?"
      ],
      "metadata": {
        "id": "QGOHSW6sgZlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A: The accuracy obtained for the Keras Sequential model ended up being different from the perceptrons model for Project 1 as there are hidden layers involved for the former."
      ],
      "metadata": {
        "id": "afSkHPK3on6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. All of the letters in MESSAGE were likely not decoded correctly, so letâ€™s try to improve the performance of the model by adding additional hidden layers. Add two additional hidden layers of the same size as your original hidden layer, then repeat experiments (4) and (5)."
      ],
      "metadata": {
        "id": "EThYHOgrgbVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MESSAGE_2D.shape)\n",
        "show(MESSAGE_2D[0])\n",
        "print(MESSAGE)\n",
        "message_list=['']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CeNdBjX4E2f",
        "outputId": "85adab83-690e-4e9f-a0ae-68a2f1277d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31, 256)\n",
            "  ##############\n",
            "  ############# \n",
            "  ##    ##    ##\n",
            "  ##    ##    ##\n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "        ##      \n",
            "   #    ##    # \n",
            "        ##      \n",
            "        ##      \n",
            "      ######    \n",
            "      ######    \n",
            "  #             \n",
            "                \n",
            "[[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do experiments 4 and 5 again, but this time, add two more hidden layers.\n",
        "model2 = tf.keras.models.Sequential()\n",
        "model2.add(tf.keras.Input(shape=(256,)))\n",
        "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer1\"))\n",
        "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer2\"))\n",
        "model2.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer3\"))\n",
        "model2.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "ATyBdSQfqf5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10173e88-75ee-4e1a-dc6f-2dac9f37a0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Hidden_layer1 (Dense)       (None, 32)                8224      \n",
            "                                                                 \n",
            " Hidden_layer2 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer3 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Output_layer (Dense)        (None, 26)                858       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,194\n",
            "Trainable params: 11,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "             metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
        "model2.fit(x=TRAINING_SET_2D, y=unicode_cat_train, epochs=9999, verbose=2, callbacks=[callback])"
      ],
      "metadata": {
        "id": "Mhz_tgqGqpZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454536bf-7093-4f80-e724-3ad39bc8890a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9999\n",
            "2/2 - 0s - loss: 3.2850 - categorical_accuracy: 0.0962 - 444ms/epoch - 222ms/step\n",
            "Epoch 2/9999\n",
            "2/2 - 0s - loss: 3.2437 - categorical_accuracy: 0.1154 - 13ms/epoch - 6ms/step\n",
            "Epoch 3/9999\n",
            "2/2 - 0s - loss: 3.2170 - categorical_accuracy: 0.0962 - 9ms/epoch - 4ms/step\n",
            "Epoch 4/9999\n",
            "2/2 - 0s - loss: 3.1977 - categorical_accuracy: 0.0962 - 9ms/epoch - 5ms/step\n",
            "Epoch 5/9999\n",
            "2/2 - 0s - loss: 3.1771 - categorical_accuracy: 0.0962 - 8ms/epoch - 4ms/step\n",
            "Epoch 6/9999\n",
            "2/2 - 0s - loss: 3.1576 - categorical_accuracy: 0.1154 - 9ms/epoch - 4ms/step\n",
            "Epoch 7/9999\n",
            "2/2 - 0s - loss: 3.1381 - categorical_accuracy: 0.1346 - 7ms/epoch - 4ms/step\n",
            "Epoch 8/9999\n",
            "2/2 - 0s - loss: 3.1202 - categorical_accuracy: 0.1346 - 8ms/epoch - 4ms/step\n",
            "Epoch 9/9999\n",
            "2/2 - 0s - loss: 3.1018 - categorical_accuracy: 0.1731 - 8ms/epoch - 4ms/step\n",
            "Epoch 10/9999\n",
            "2/2 - 0s - loss: 3.0855 - categorical_accuracy: 0.1731 - 9ms/epoch - 4ms/step\n",
            "Epoch 11/9999\n",
            "2/2 - 0s - loss: 3.0656 - categorical_accuracy: 0.1923 - 7ms/epoch - 4ms/step\n",
            "Epoch 12/9999\n",
            "2/2 - 0s - loss: 3.0457 - categorical_accuracy: 0.2115 - 11ms/epoch - 5ms/step\n",
            "Epoch 13/9999\n",
            "2/2 - 0s - loss: 3.0257 - categorical_accuracy: 0.2115 - 10ms/epoch - 5ms/step\n",
            "Epoch 14/9999\n",
            "2/2 - 0s - loss: 3.0029 - categorical_accuracy: 0.2308 - 8ms/epoch - 4ms/step\n",
            "Epoch 15/9999\n",
            "2/2 - 0s - loss: 2.9798 - categorical_accuracy: 0.2500 - 8ms/epoch - 4ms/step\n",
            "Epoch 16/9999\n",
            "2/2 - 0s - loss: 2.9563 - categorical_accuracy: 0.2500 - 10ms/epoch - 5ms/step\n",
            "Epoch 17/9999\n",
            "2/2 - 0s - loss: 2.9290 - categorical_accuracy: 0.2500 - 7ms/epoch - 4ms/step\n",
            "Epoch 18/9999\n",
            "2/2 - 0s - loss: 2.9064 - categorical_accuracy: 0.2500 - 8ms/epoch - 4ms/step\n",
            "Epoch 19/9999\n",
            "2/2 - 0s - loss: 2.8770 - categorical_accuracy: 0.2500 - 7ms/epoch - 4ms/step\n",
            "Epoch 20/9999\n",
            "2/2 - 0s - loss: 2.8501 - categorical_accuracy: 0.2692 - 9ms/epoch - 5ms/step\n",
            "Epoch 21/9999\n",
            "2/2 - 0s - loss: 2.8169 - categorical_accuracy: 0.2692 - 8ms/epoch - 4ms/step\n",
            "Epoch 22/9999\n",
            "2/2 - 0s - loss: 2.7884 - categorical_accuracy: 0.2692 - 8ms/epoch - 4ms/step\n",
            "Epoch 23/9999\n",
            "2/2 - 0s - loss: 2.7538 - categorical_accuracy: 0.2885 - 10ms/epoch - 5ms/step\n",
            "Epoch 24/9999\n",
            "2/2 - 0s - loss: 2.7197 - categorical_accuracy: 0.2885 - 8ms/epoch - 4ms/step\n",
            "Epoch 25/9999\n",
            "2/2 - 0s - loss: 2.6833 - categorical_accuracy: 0.2885 - 14ms/epoch - 7ms/step\n",
            "Epoch 26/9999\n",
            "2/2 - 0s - loss: 2.6455 - categorical_accuracy: 0.2885 - 9ms/epoch - 4ms/step\n",
            "Epoch 27/9999\n",
            "2/2 - 0s - loss: 2.6053 - categorical_accuracy: 0.3462 - 14ms/epoch - 7ms/step\n",
            "Epoch 28/9999\n",
            "2/2 - 0s - loss: 2.5634 - categorical_accuracy: 0.3462 - 8ms/epoch - 4ms/step\n",
            "Epoch 29/9999\n",
            "2/2 - 0s - loss: 2.5196 - categorical_accuracy: 0.3462 - 10ms/epoch - 5ms/step\n",
            "Epoch 30/9999\n",
            "2/2 - 0s - loss: 2.4745 - categorical_accuracy: 0.3462 - 14ms/epoch - 7ms/step\n",
            "Epoch 31/9999\n",
            "2/2 - 0s - loss: 2.4266 - categorical_accuracy: 0.3654 - 7ms/epoch - 4ms/step\n",
            "Epoch 32/9999\n",
            "2/2 - 0s - loss: 2.3774 - categorical_accuracy: 0.3846 - 7ms/epoch - 4ms/step\n",
            "Epoch 33/9999\n",
            "2/2 - 0s - loss: 2.3330 - categorical_accuracy: 0.4038 - 8ms/epoch - 4ms/step\n",
            "Epoch 34/9999\n",
            "2/2 - 0s - loss: 2.2766 - categorical_accuracy: 0.4423 - 11ms/epoch - 6ms/step\n",
            "Epoch 35/9999\n",
            "2/2 - 0s - loss: 2.2238 - categorical_accuracy: 0.4423 - 10ms/epoch - 5ms/step\n",
            "Epoch 36/9999\n",
            "2/2 - 0s - loss: 2.1693 - categorical_accuracy: 0.4231 - 8ms/epoch - 4ms/step\n",
            "Epoch 37/9999\n",
            "2/2 - 0s - loss: 2.1135 - categorical_accuracy: 0.4423 - 11ms/epoch - 5ms/step\n",
            "Epoch 38/9999\n",
            "2/2 - 0s - loss: 2.0554 - categorical_accuracy: 0.4808 - 12ms/epoch - 6ms/step\n",
            "Epoch 39/9999\n",
            "2/2 - 0s - loss: 2.0004 - categorical_accuracy: 0.5000 - 9ms/epoch - 5ms/step\n",
            "Epoch 40/9999\n",
            "2/2 - 0s - loss: 1.9367 - categorical_accuracy: 0.5577 - 8ms/epoch - 4ms/step\n",
            "Epoch 41/9999\n",
            "2/2 - 0s - loss: 1.8800 - categorical_accuracy: 0.5192 - 8ms/epoch - 4ms/step\n",
            "Epoch 42/9999\n",
            "2/2 - 0s - loss: 1.8152 - categorical_accuracy: 0.5385 - 8ms/epoch - 4ms/step\n",
            "Epoch 43/9999\n",
            "2/2 - 0s - loss: 1.7587 - categorical_accuracy: 0.5962 - 8ms/epoch - 4ms/step\n",
            "Epoch 44/9999\n",
            "2/2 - 0s - loss: 1.6926 - categorical_accuracy: 0.6346 - 10ms/epoch - 5ms/step\n",
            "Epoch 45/9999\n",
            "2/2 - 0s - loss: 1.6355 - categorical_accuracy: 0.6346 - 9ms/epoch - 5ms/step\n",
            "Epoch 46/9999\n",
            "2/2 - 0s - loss: 1.5771 - categorical_accuracy: 0.6731 - 9ms/epoch - 5ms/step\n",
            "Epoch 47/9999\n",
            "2/2 - 0s - loss: 1.5203 - categorical_accuracy: 0.7115 - 9ms/epoch - 5ms/step\n",
            "Epoch 48/9999\n",
            "2/2 - 0s - loss: 1.4574 - categorical_accuracy: 0.7308 - 11ms/epoch - 6ms/step\n",
            "Epoch 49/9999\n",
            "2/2 - 0s - loss: 1.4030 - categorical_accuracy: 0.7692 - 8ms/epoch - 4ms/step\n",
            "Epoch 50/9999\n",
            "2/2 - 0s - loss: 1.3503 - categorical_accuracy: 0.7692 - 13ms/epoch - 7ms/step\n",
            "Epoch 51/9999\n",
            "2/2 - 0s - loss: 1.2960 - categorical_accuracy: 0.7692 - 8ms/epoch - 4ms/step\n",
            "Epoch 52/9999\n",
            "2/2 - 0s - loss: 1.2451 - categorical_accuracy: 0.7885 - 10ms/epoch - 5ms/step\n",
            "Epoch 53/9999\n",
            "2/2 - 0s - loss: 1.1919 - categorical_accuracy: 0.8269 - 10ms/epoch - 5ms/step\n",
            "Epoch 54/9999\n",
            "2/2 - 0s - loss: 1.1411 - categorical_accuracy: 0.8269 - 8ms/epoch - 4ms/step\n",
            "Epoch 55/9999\n",
            "2/2 - 0s - loss: 1.0939 - categorical_accuracy: 0.8269 - 11ms/epoch - 6ms/step\n",
            "Epoch 56/9999\n",
            "2/2 - 0s - loss: 1.0468 - categorical_accuracy: 0.8269 - 9ms/epoch - 5ms/step\n",
            "Epoch 57/9999\n",
            "2/2 - 0s - loss: 1.0008 - categorical_accuracy: 0.8269 - 8ms/epoch - 4ms/step\n",
            "Epoch 58/9999\n",
            "2/2 - 0s - loss: 0.9567 - categorical_accuracy: 0.8077 - 7ms/epoch - 4ms/step\n",
            "Epoch 59/9999\n",
            "2/2 - 0s - loss: 0.9145 - categorical_accuracy: 0.8077 - 9ms/epoch - 4ms/step\n",
            "Epoch 60/9999\n",
            "2/2 - 0s - loss: 0.8787 - categorical_accuracy: 0.8462 - 9ms/epoch - 5ms/step\n",
            "Epoch 61/9999\n",
            "2/2 - 0s - loss: 0.8334 - categorical_accuracy: 0.8654 - 8ms/epoch - 4ms/step\n",
            "Epoch 62/9999\n",
            "2/2 - 0s - loss: 0.7953 - categorical_accuracy: 0.8654 - 9ms/epoch - 5ms/step\n",
            "Epoch 63/9999\n",
            "2/2 - 0s - loss: 0.7618 - categorical_accuracy: 0.8654 - 8ms/epoch - 4ms/step\n",
            "Epoch 64/9999\n",
            "2/2 - 0s - loss: 0.7264 - categorical_accuracy: 0.8846 - 9ms/epoch - 5ms/step\n",
            "Epoch 65/9999\n",
            "2/2 - 0s - loss: 0.6922 - categorical_accuracy: 0.9231 - 9ms/epoch - 4ms/step\n",
            "Epoch 66/9999\n",
            "2/2 - 0s - loss: 0.6587 - categorical_accuracy: 0.9231 - 9ms/epoch - 5ms/step\n",
            "Epoch 67/9999\n",
            "2/2 - 0s - loss: 0.6282 - categorical_accuracy: 0.9231 - 8ms/epoch - 4ms/step\n",
            "Epoch 68/9999\n",
            "2/2 - 0s - loss: 0.5993 - categorical_accuracy: 0.9231 - 7ms/epoch - 3ms/step\n",
            "Epoch 69/9999\n",
            "2/2 - 0s - loss: 0.5737 - categorical_accuracy: 0.9231 - 13ms/epoch - 7ms/step\n",
            "Epoch 70/9999\n",
            "2/2 - 0s - loss: 0.5479 - categorical_accuracy: 0.9231 - 9ms/epoch - 4ms/step\n",
            "Epoch 71/9999\n",
            "2/2 - 0s - loss: 0.5229 - categorical_accuracy: 0.9423 - 9ms/epoch - 4ms/step\n",
            "Epoch 72/9999\n",
            "2/2 - 0s - loss: 0.4991 - categorical_accuracy: 0.9423 - 11ms/epoch - 5ms/step\n",
            "Epoch 73/9999\n",
            "2/2 - 0s - loss: 0.4795 - categorical_accuracy: 0.9231 - 10ms/epoch - 5ms/step\n",
            "Epoch 74/9999\n",
            "2/2 - 0s - loss: 0.4544 - categorical_accuracy: 0.9423 - 10ms/epoch - 5ms/step\n",
            "Epoch 75/9999\n",
            "2/2 - 0s - loss: 0.4339 - categorical_accuracy: 0.9423 - 9ms/epoch - 4ms/step\n",
            "Epoch 76/9999\n",
            "2/2 - 0s - loss: 0.4158 - categorical_accuracy: 0.9615 - 10ms/epoch - 5ms/step\n",
            "Epoch 77/9999\n",
            "2/2 - 0s - loss: 0.4011 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 78/9999\n",
            "2/2 - 0s - loss: 0.3826 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 79/9999\n",
            "2/2 - 0s - loss: 0.3669 - categorical_accuracy: 0.9808 - 10ms/epoch - 5ms/step\n",
            "Epoch 80/9999\n",
            "2/2 - 0s - loss: 0.3524 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 81/9999\n",
            "2/2 - 0s - loss: 0.3385 - categorical_accuracy: 0.9808 - 7ms/epoch - 4ms/step\n",
            "Epoch 82/9999\n",
            "2/2 - 0s - loss: 0.3259 - categorical_accuracy: 0.9808 - 11ms/epoch - 5ms/step\n",
            "Epoch 83/9999\n",
            "2/2 - 0s - loss: 0.3149 - categorical_accuracy: 0.9808 - 11ms/epoch - 6ms/step\n",
            "Epoch 84/9999\n",
            "2/2 - 0s - loss: 0.3032 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 85/9999\n",
            "2/2 - 0s - loss: 0.2935 - categorical_accuracy: 0.9808 - 9ms/epoch - 4ms/step\n",
            "Epoch 86/9999\n",
            "2/2 - 0s - loss: 0.2819 - categorical_accuracy: 0.9808 - 7ms/epoch - 3ms/step\n",
            "Epoch 87/9999\n",
            "2/2 - 0s - loss: 0.2710 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 88/9999\n",
            "2/2 - 0s - loss: 0.2614 - categorical_accuracy: 0.9808 - 9ms/epoch - 4ms/step\n",
            "Epoch 89/9999\n",
            "2/2 - 0s - loss: 0.2532 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 90/9999\n",
            "2/2 - 0s - loss: 0.2451 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 91/9999\n",
            "2/2 - 0s - loss: 0.2350 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 92/9999\n",
            "2/2 - 0s - loss: 0.2282 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 93/9999\n",
            "2/2 - 0s - loss: 0.2195 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 94/9999\n",
            "2/2 - 0s - loss: 0.2130 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 95/9999\n",
            "2/2 - 0s - loss: 0.2066 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 96/9999\n",
            "2/2 - 0s - loss: 0.1982 - categorical_accuracy: 1.0000 - 13ms/epoch - 6ms/step\n",
            "Epoch 97/9999\n",
            "2/2 - 0s - loss: 0.1937 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 98/9999\n",
            "2/2 - 0s - loss: 0.1873 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 99/9999\n",
            "2/2 - 0s - loss: 0.1828 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 100/9999\n",
            "2/2 - 0s - loss: 0.1761 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 101/9999\n",
            "2/2 - 0s - loss: 0.1699 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 102/9999\n",
            "2/2 - 0s - loss: 0.1643 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 103/9999\n",
            "2/2 - 0s - loss: 0.1604 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 104/9999\n",
            "2/2 - 0s - loss: 0.1569 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 105/9999\n",
            "2/2 - 0s - loss: 0.1506 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 106/9999\n",
            "2/2 - 0s - loss: 0.1469 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 107/9999\n",
            "2/2 - 0s - loss: 0.1411 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 108/9999\n",
            "2/2 - 0s - loss: 0.1371 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 109/9999\n",
            "2/2 - 0s - loss: 0.1324 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 110/9999\n",
            "2/2 - 0s - loss: 0.1294 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 111/9999\n",
            "2/2 - 0s - loss: 0.1266 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 112/9999\n",
            "2/2 - 0s - loss: 0.1219 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 113/9999\n",
            "2/2 - 0s - loss: 0.1188 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 114/9999\n",
            "2/2 - 0s - loss: 0.1144 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 115/9999\n",
            "2/2 - 0s - loss: 0.1109 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 116/9999\n",
            "2/2 - 0s - loss: 0.1077 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 117/9999\n",
            "2/2 - 0s - loss: 0.1051 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 118/9999\n",
            "2/2 - 0s - loss: 0.1014 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 119/9999\n",
            "2/2 - 0s - loss: 0.0991 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 120/9999\n",
            "2/2 - 0s - loss: 0.0964 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 121/9999\n",
            "2/2 - 0s - loss: 0.0933 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 122/9999\n",
            "2/2 - 0s - loss: 0.0909 - categorical_accuracy: 1.0000 - 13ms/epoch - 7ms/step\n",
            "Epoch 123/9999\n",
            "2/2 - 0s - loss: 0.0884 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 124/9999\n",
            "2/2 - 0s - loss: 0.0866 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 125/9999\n",
            "2/2 - 0s - loss: 0.0834 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 126/9999\n",
            "2/2 - 0s - loss: 0.0819 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 127/9999\n",
            "2/2 - 0s - loss: 0.0808 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 128/9999\n",
            "2/2 - 0s - loss: 0.0784 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 129/9999\n",
            "2/2 - 0s - loss: 0.0757 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 130/9999\n",
            "2/2 - 0s - loss: 0.0734 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 131/9999\n",
            "2/2 - 0s - loss: 0.0721 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 132/9999\n",
            "2/2 - 0s - loss: 0.0694 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 133/9999\n",
            "2/2 - 0s - loss: 0.0676 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 134/9999\n",
            "2/2 - 0s - loss: 0.0655 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 135/9999\n",
            "2/2 - 0s - loss: 0.0645 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 136/9999\n",
            "2/2 - 0s - loss: 0.0627 - categorical_accuracy: 1.0000 - 7ms/epoch - 4ms/step\n",
            "Epoch 137/9999\n",
            "2/2 - 0s - loss: 0.0610 - categorical_accuracy: 1.0000 - 13ms/epoch - 6ms/step\n",
            "Epoch 138/9999\n",
            "2/2 - 0s - loss: 0.0597 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 139/9999\n",
            "2/2 - 0s - loss: 0.0583 - categorical_accuracy: 1.0000 - 16ms/epoch - 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f38ca7da2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(x=TEST_SET_2D, y=unicode_cat_test, verbose=1)"
      ],
      "metadata": {
        "id": "5hZ322rMqwXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ba31a0-f30a-43ac-87e9-32076caa20d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 149ms/step - loss: 3.8667 - categorical_accuracy: 0.2308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.86668062210083, 0.23076923191547394]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model2.predict(TEST_SET_2D)\n",
        "predicted"
      ],
      "metadata": {
        "id": "b_ZZkjrOz-tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946fe81c-1bf6-4348-aa1f-fb9e54a32e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.20660368e-04, 4.73795272e-03, 1.15000410e-02, 4.46463078e-02,\n",
              "        8.55714316e-04, 1.67479215e-04, 9.23344731e-01, 8.38396517e-08,\n",
              "        1.01507387e-04, 5.11058781e-04, 1.71859472e-04, 1.15249529e-02,\n",
              "        2.49221870e-07, 3.32758973e-05, 1.40075659e-04, 7.75189721e-04,\n",
              "        1.24870930e-05, 7.14075519e-04, 1.18279377e-06, 8.43057205e-06,\n",
              "        2.28763520e-06, 2.43847381e-07, 1.04134808e-04, 4.50736252e-07,\n",
              "        2.61178286e-07, 4.25239064e-04],\n",
              "       [4.73899000e-08, 7.70191327e-02, 1.15649693e-03, 1.32197456e-03,\n",
              "        3.17274660e-01, 5.99241909e-03, 2.30653778e-01, 4.87832494e-07,\n",
              "        3.40629891e-02, 7.09449407e-03, 6.88360655e-04, 1.81064755e-02,\n",
              "        3.36593484e-06, 8.54739264e-05, 2.21239225e-05, 5.89696137e-05,\n",
              "        1.50002562e-03, 1.39382808e-03, 5.03552321e-04, 6.59840414e-03,\n",
              "        7.46226704e-07, 6.47098830e-07, 7.68835616e-06, 3.30069656e-06,\n",
              "        1.38372525e-06, 2.96449184e-01],\n",
              "       [5.34180544e-05, 4.12741341e-02, 1.27887398e-01, 5.51829673e-03,\n",
              "        1.20972112e-01, 1.86239951e-03, 5.30578375e-01, 4.40503391e-05,\n",
              "        1.49832079e-02, 2.21871436e-02, 1.56447571e-03, 3.86595167e-02,\n",
              "        8.02304639e-05, 2.91775377e-03, 1.94659704e-04, 2.84114474e-04,\n",
              "        4.38754866e-03, 3.04198172e-03, 2.47694319e-03, 6.80251000e-03,\n",
              "        1.75172943e-04, 2.16713070e-05, 9.11948970e-04, 7.90300037e-05,\n",
              "        1.45031290e-05, 7.30273575e-02],\n",
              "       [1.39966141e-06, 1.08736400e-02, 8.78502149e-04, 4.56604958e-02,\n",
              "        1.32459655e-01, 8.62857029e-02, 1.48529395e-01, 1.82220916e-07,\n",
              "        1.02890441e-02, 6.45711145e-04, 5.66603267e-04, 5.39759696e-01,\n",
              "        2.20964444e-06, 6.32375213e-06, 5.11142134e-04, 9.23276821e-04,\n",
              "        2.51684047e-04, 2.21208506e-03, 5.22944727e-04, 9.90467612e-04,\n",
              "        8.99866677e-07, 4.31452008e-07, 1.85917379e-05, 4.77400818e-07,\n",
              "        3.19527953e-06, 1.86062716e-02],\n",
              "       [3.46693355e-12, 5.12805173e-07, 1.04152796e-05, 4.08629885e-09,\n",
              "        2.52175421e-01, 5.58009744e-03, 1.57852046e-05, 7.50164642e-09,\n",
              "        6.00133598e-01, 2.46205877e-06, 6.22229322e-07, 2.18555695e-04,\n",
              "        1.03092340e-07, 1.58604792e-06, 2.46465000e-08, 1.86397159e-07,\n",
              "        7.93450226e-07, 6.91963864e-09, 8.36457592e-03, 6.35509863e-02,\n",
              "        1.71203413e-10, 4.59457500e-10, 3.51315577e-09, 3.53599390e-08,\n",
              "        3.45709213e-06, 6.99407607e-02],\n",
              "       [2.37978609e-10, 9.81869164e-08, 8.80410482e-08, 6.21496465e-10,\n",
              "        7.83366803e-03, 4.58186179e-01, 1.67107302e-07, 1.60018772e-08,\n",
              "        3.32047790e-01, 1.21935770e-07, 5.28233215e-07, 9.00615305e-06,\n",
              "        1.43246808e-07, 2.31179160e-06, 2.00126024e-07, 1.81368188e-04,\n",
              "        7.85274423e-08, 6.12568973e-09, 2.45049614e-02, 1.62186548e-01,\n",
              "        2.17091414e-11, 1.44577870e-08, 6.14912565e-09, 1.85125782e-06,\n",
              "        1.43393008e-02, 7.05671147e-04],\n",
              "       [1.28768320e-07, 2.76715774e-02, 7.67695252e-03, 2.34399014e-03,\n",
              "        9.26538929e-03, 2.40600039e-05, 8.54166985e-01, 6.43569891e-08,\n",
              "        1.36190502e-03, 3.71371470e-02, 4.02469886e-05, 2.72997562e-02,\n",
              "        5.97188077e-08, 7.50574100e-06, 3.59302419e-07, 1.98592534e-06,\n",
              "        7.82310904e-04, 8.20170331e-04, 1.96827059e-05, 3.55186086e-04,\n",
              "        2.74592603e-05, 6.32952109e-08, 1.15302964e-05, 2.59684640e-07,\n",
              "        4.46307435e-09, 3.09852753e-02],\n",
              "       [2.18076584e-05, 4.78533655e-01, 2.23877374e-02, 8.54723796e-04,\n",
              "        4.68377024e-02, 1.38530310e-03, 3.51229683e-03, 2.49539991e-03,\n",
              "        2.99420822e-06, 3.05850517e-05, 2.25151256e-01, 1.00660855e-02,\n",
              "        2.88810562e-02, 1.15241949e-02, 8.49621966e-02, 4.47408855e-03,\n",
              "        3.26028932e-03, 2.08451003e-02, 1.43995443e-02, 5.08763827e-04,\n",
              "        8.11324164e-04, 1.49177108e-03, 3.09719015e-02, 2.26245564e-03,\n",
              "        2.64994669e-05, 4.30116104e-03],\n",
              "       [6.16979179e-08, 1.25015713e-05, 7.53588620e-06, 1.32179338e-07,\n",
              "        2.44555622e-03, 5.00486279e-03, 1.08148242e-05, 6.51410801e-06,\n",
              "        5.62497020e-01, 1.88726161e-04, 8.39629593e-06, 2.56941603e-05,\n",
              "        9.68616678e-06, 6.74433322e-05, 6.15544309e-07, 3.48716785e-05,\n",
              "        6.36250843e-05, 2.77295982e-07, 8.03903025e-03, 4.00724441e-01,\n",
              "        7.48826636e-08, 1.31865897e-06, 1.08470761e-06, 6.56115881e-05,\n",
              "        5.46061341e-03, 1.53234275e-02],\n",
              "       [1.09506516e-04, 1.13910824e-01, 1.64712570e-03, 4.42771940e-04,\n",
              "        5.60852175e-04, 3.14711142e-05, 1.15891369e-02, 2.11437442e-03,\n",
              "        3.86914564e-03, 6.58284426e-01, 2.41015339e-03, 2.74267630e-04,\n",
              "        4.40554344e-04, 2.94092903e-03, 3.36216995e-04, 5.41500267e-05,\n",
              "        2.80943941e-02, 5.19428961e-03, 2.27899058e-04, 6.25931541e-04,\n",
              "        9.15423781e-02, 1.54481234e-03, 2.05453415e-03, 2.09054165e-03,\n",
              "        2.08730125e-05, 6.95883259e-02],\n",
              "       [9.52379821e-07, 6.16519992e-06, 1.64559060e-05, 4.56406087e-06,\n",
              "        4.03004587e-02, 7.63386190e-01, 1.45821004e-05, 1.83004420e-06,\n",
              "        6.28159642e-02, 1.63849131e-06, 2.16631393e-04, 9.99874668e-04,\n",
              "        3.03271281e-06, 2.00820614e-05, 2.18248417e-04, 2.70001963e-02,\n",
              "        2.88181036e-06, 1.76509238e-05, 3.06534977e-03, 3.04901823e-02,\n",
              "        5.15437684e-08, 4.35858510e-06, 9.23941172e-07, 2.22093622e-05,\n",
              "        7.06999153e-02, 6.89571491e-04],\n",
              "       [2.24435972e-08, 5.38809763e-05, 1.98855400e-04, 3.37023404e-04,\n",
              "        1.16361491e-01, 2.42604576e-02, 1.77090557e-03, 7.40011075e-09,\n",
              "        2.39431038e-02, 4.24675127e-05, 1.67554310e-06, 8.28915536e-01,\n",
              "        4.97112111e-08, 9.18806222e-08, 1.28484571e-05, 4.93852212e-06,\n",
              "        3.58272846e-05, 9.36955894e-06, 1.54415471e-03, 5.38513879e-04,\n",
              "        7.13799352e-07, 6.63163258e-09, 1.08560278e-06, 2.39002329e-09,\n",
              "        2.33309180e-07, 1.96665316e-03],\n",
              "       [7.15303991e-04, 1.06231216e-03, 1.75023929e-03, 6.97847936e-05,\n",
              "        1.74969528e-02, 2.32617091e-02, 7.01203244e-04, 1.53491425e-03,\n",
              "        2.38617440e-03, 8.21519643e-05, 1.10747717e-01, 5.34508086e-04,\n",
              "        4.02877061e-03, 1.05356751e-02, 5.78273740e-03, 3.86531532e-01,\n",
              "        1.76382615e-04, 2.49457266e-03, 4.60579013e-03, 1.99324097e-02,\n",
              "        3.66678578e-05, 5.34976693e-03, 7.05078419e-04, 5.57122491e-02,\n",
              "        3.40536505e-01, 3.22891865e-03],\n",
              "       [5.35536637e-05, 3.57774317e-01, 8.36212113e-02, 2.01353170e-02,\n",
              "        1.91202819e-01, 1.58417015e-03, 1.79902971e-01, 3.14326280e-05,\n",
              "        1.55699789e-04, 4.11801157e-04, 2.88177300e-02, 2.83014663e-02,\n",
              "        8.35691113e-04, 1.38836522e-02, 1.13475984e-02, 5.22590941e-03,\n",
              "        4.16173047e-04, 6.73053786e-02, 6.80106343e-04, 5.88450523e-04,\n",
              "        3.20254148e-05, 3.65759879e-05, 1.45322399e-03, 8.62616434e-05,\n",
              "        2.08215351e-05, 6.09559985e-03],\n",
              "       [4.84510278e-03, 1.98498908e-02, 4.82398719e-02, 5.05661786e-01,\n",
              "        2.23075552e-03, 2.22050166e-03, 3.23646188e-01, 4.37627532e-05,\n",
              "        6.90812012e-04, 2.79268390e-03, 1.50008849e-03, 4.68018614e-02,\n",
              "        3.87013497e-05, 4.92819061e-04, 2.64222119e-02, 4.41793259e-03,\n",
              "        5.10955520e-04, 3.63529334e-03, 8.09247431e-05, 1.45616214e-04,\n",
              "        1.02661339e-04, 8.55604503e-06, 4.21661790e-03, 2.07699177e-05,\n",
              "        1.73312583e-05, 1.36641017e-03],\n",
              "       [1.20676013e-05, 5.72227640e-04, 2.53603448e-05, 1.26230792e-04,\n",
              "        2.73501333e-02, 7.76547074e-01, 5.77005092e-04, 1.78022594e-06,\n",
              "        3.90007272e-02, 2.29407669e-05, 6.37713820e-04, 1.25451316e-03,\n",
              "        7.51105972e-06, 1.46582563e-04, 3.72949842e-04, 9.09360275e-02,\n",
              "        2.31945232e-05, 2.48510420e-04, 1.44256570e-03, 2.90440898e-02,\n",
              "        1.35358263e-07, 2.28518747e-05, 9.07978847e-06, 6.34400276e-05,\n",
              "        3.03700250e-02, 1.18523778e-03],\n",
              "       [2.18830508e-04, 6.14070185e-02, 2.45384593e-03, 4.47258651e-01,\n",
              "        1.13952933e-02, 5.23637887e-03, 2.25268990e-01, 2.15898672e-05,\n",
              "        5.61254456e-05, 3.66777391e-03, 1.00945728e-02, 9.48282927e-02,\n",
              "        6.95498284e-06, 3.89664165e-05, 1.69481151e-02, 7.53518427e-04,\n",
              "        5.41506661e-03, 1.12927780e-01, 2.93710127e-05, 1.20389108e-04,\n",
              "        4.97968751e-04, 2.38932025e-05, 7.25755002e-04, 3.54724057e-06,\n",
              "        1.04040328e-06, 6.00264757e-04],\n",
              "       [1.34984375e-05, 5.08431485e-03, 3.75079107e-04, 2.76923203e-03,\n",
              "        2.66902447e-01, 4.26950634e-01, 2.12056581e-02, 2.92408163e-06,\n",
              "        1.53560072e-01, 2.66594347e-04, 3.85613460e-03, 1.70750562e-02,\n",
              "        1.72684686e-05, 3.80789948e-04, 8.71312281e-04, 3.27344015e-02,\n",
              "        1.95883433e-04, 2.25263159e-03, 8.82634136e-04, 3.56971696e-02,\n",
              "        1.17566992e-06, 2.75286966e-05, 1.82663971e-05, 3.32061682e-05,\n",
              "        2.91825528e-03, 2.59077381e-02],\n",
              "       [1.01942994e-07, 7.86998346e-02, 9.53473523e-03, 5.05922537e-04,\n",
              "        1.27738357e-01, 9.67590720e-04, 3.28671932e-01, 2.51194092e-06,\n",
              "        1.24751460e-02, 2.17696447e-02, 4.64088633e-04, 3.77377532e-02,\n",
              "        9.01520434e-06, 9.36434444e-05, 2.34075796e-05, 2.46623840e-05,\n",
              "        1.38676993e-03, 1.67800381e-03, 1.46195432e-03, 3.73568642e-03,\n",
              "        2.78237621e-05, 1.28445265e-06, 8.85602785e-05, 1.14830291e-05,\n",
              "        5.63399738e-07, 3.72889549e-01],\n",
              "       [1.36078384e-07, 2.10024984e-04, 7.00166784e-05, 1.14193449e-06,\n",
              "        4.86033857e-02, 2.86272215e-03, 1.72102838e-04, 9.92623027e-05,\n",
              "        3.59778643e-01, 8.74688663e-03, 2.41197835e-04, 2.89037998e-04,\n",
              "        7.47342128e-05, 3.87494598e-04, 2.22680228e-06, 1.83697721e-05,\n",
              "        7.05914339e-03, 1.11732943e-05, 7.77126243e-03, 2.61316538e-01,\n",
              "        6.15501540e-06, 4.73346445e-05, 5.82357370e-06, 4.39109193e-04,\n",
              "        2.25886772e-03, 2.99527198e-01],\n",
              "       [3.82086500e-06, 2.17735581e-02, 1.61553711e-01, 4.00914252e-03,\n",
              "        3.21707577e-02, 2.03508581e-03, 2.29561016e-01, 2.35151329e-05,\n",
              "        1.67007078e-04, 1.08839571e-03, 5.32060687e-04, 5.13017595e-01,\n",
              "        2.47024727e-05, 5.02963921e-05, 2.24925787e-03, 1.50159147e-04,\n",
              "        6.81612757e-04, 6.63673389e-04, 3.55310971e-03, 3.80453945e-04,\n",
              "        2.06968602e-04, 1.82946712e-06, 4.34625288e-03, 5.61441766e-06,\n",
              "        1.35885557e-07, 2.17502769e-02],\n",
              "       [2.41770549e-03, 2.59393435e-02, 1.81324277e-02, 5.16182125e-01,\n",
              "        2.78568384e-03, 4.77556838e-03, 1.82091787e-01, 1.30207496e-04,\n",
              "        3.12821037e-04, 1.12324646e-02, 6.67574676e-03, 1.65057257e-01,\n",
              "        1.30871722e-05, 3.58626967e-05, 1.75791979e-02, 5.43113379e-03,\n",
              "        6.91126613e-03, 1.38633819e-02, 7.22179029e-05, 1.41855548e-04,\n",
              "        1.22014275e-02, 1.08870481e-04, 4.75844508e-03, 5.65648152e-05,\n",
              "        1.30959115e-05, 3.08049074e-03],\n",
              "       [1.17888709e-03, 8.25305469e-03, 3.85448001e-02, 5.70732951e-02,\n",
              "        1.79764442e-02, 5.18066203e-03, 1.32271767e-01, 2.19939597e-04,\n",
              "        5.79010975e-03, 3.12680379e-02, 6.76135579e-03, 5.51329255e-01,\n",
              "        2.50489829e-05, 3.92876536e-05, 6.36754185e-03, 4.35419846e-03,\n",
              "        1.46125257e-02, 9.80721973e-03, 4.38712857e-04, 3.61571379e-04,\n",
              "        8.64381567e-02, 5.24807721e-04, 2.77681509e-03, 1.95886591e-04,\n",
              "        6.50352304e-05, 1.81456804e-02],\n",
              "       [6.14414224e-04, 2.54507940e-02, 1.84206967e-03, 4.50382904e-05,\n",
              "        1.58458538e-02, 9.76000156e-04, 4.87563713e-03, 8.94187018e-03,\n",
              "        5.78721194e-03, 2.78517921e-02, 7.33306482e-02, 1.22640035e-04,\n",
              "        3.67293730e-02, 3.03895742e-01, 9.72498732e-04, 1.19439289e-02,\n",
              "        4.86658979e-03, 9.52712167e-03, 8.02121032e-03, 7.93897286e-02,\n",
              "        8.24040966e-04, 2.66995933e-02, 3.05212592e-03, 2.78785914e-01,\n",
              "        3.17663886e-02, 3.78417410e-02],\n",
              "       [1.65185716e-03, 1.46684228e-02, 1.93011935e-03, 4.60225332e-04,\n",
              "        2.72583356e-03, 9.57268203e-05, 1.68909971e-02, 1.32431043e-03,\n",
              "        8.91541541e-02, 7.36986160e-01, 3.74814100e-03, 1.60012132e-04,\n",
              "        5.04322525e-04, 2.93050110e-02, 1.02641439e-04, 4.49582003e-04,\n",
              "        1.38119813e-02, 5.48416702e-03, 1.21788988e-04, 1.80987287e-02,\n",
              "        6.18957216e-03, 2.90706474e-03, 7.00302364e-04, 5.85977826e-03,\n",
              "        3.45094455e-03, 4.32180502e-02],\n",
              "       [4.58999203e-08, 4.03939521e-05, 3.85015999e-04, 5.06434787e-07,\n",
              "        1.66816831e-01, 4.45459783e-03, 1.73231092e-04, 2.69626835e-05,\n",
              "        2.84059703e-01, 5.37897169e-04, 2.23370385e-04, 1.13024330e-03,\n",
              "        3.16178921e-05, 1.09607550e-04, 3.80878396e-06, 4.79392766e-05,\n",
              "        5.86467970e-04, 3.95501365e-06, 1.00326883e-02, 1.73744082e-01,\n",
              "        2.27600617e-06, 9.96236031e-06, 3.78508071e-06, 1.66722180e-04,\n",
              "        1.33581110e-03, 3.56072456e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_list = []\n",
        "for num in range(len(predicted)):\n",
        "    max_val = 0\n",
        "    actual = chr(num+ord('A'))\n",
        "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    result_list.append([actual, predict])\n",
        "result_list"
      ],
      "metadata": {
        "id": "Gd_I7bri0DMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d39725-71c3-44d1-8fe7-81f85747befc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A', 'G'],\n",
              " ['B', 'E'],\n",
              " ['C', 'G'],\n",
              " ['D', 'L'],\n",
              " ['E', 'I'],\n",
              " ['F', 'F'],\n",
              " ['G', 'G'],\n",
              " ['H', 'B'],\n",
              " ['I', 'I'],\n",
              " ['J', 'J'],\n",
              " ['K', 'F'],\n",
              " ['L', 'L'],\n",
              " ['M', 'P'],\n",
              " ['N', 'B'],\n",
              " ['O', 'D'],\n",
              " ['P', 'F'],\n",
              " ['Q', 'D'],\n",
              " ['R', 'F'],\n",
              " ['S', 'Z'],\n",
              " ['T', 'I'],\n",
              " ['U', 'L'],\n",
              " ['V', 'D'],\n",
              " ['W', 'L'],\n",
              " ['X', 'N'],\n",
              " ['Y', 'J'],\n",
              " ['Z', 'Z']]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_misclassified = []\n",
        "num_total, num_correct = 26, 26\n",
        "for i in range(len(result_list)):\n",
        "    if result_list[i][0] != result_list[i][1]:\n",
        "        num_correct -= 1\n",
        "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
        "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_misclassified) == 0:\n",
        "    print(\"All test images are classified correctly.\")\n",
        "else:\n",
        "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_misclassified)):\n",
        "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
      ],
      "metadata": {
        "id": "YHEgSKw70ERA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7bfd1a-6ff4-4d9f-b913-b869cd434160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images correctly classified: 23.076923076923077%\n",
            "Here are the list of test images that are misclassified and how they appear:\n",
            "Actual    Predicted\n",
            "A         G\n",
            "B         E\n",
            "C         G\n",
            "D         L\n",
            "E         I\n",
            "H         B\n",
            "K         F\n",
            "M         P\n",
            "N         B\n",
            "O         D\n",
            "P         F\n",
            "Q         D\n",
            "R         F\n",
            "S         Z\n",
            "T         I\n",
            "U         L\n",
            "V         D\n",
            "W         L\n",
            "X         N\n",
            "Y         J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_message = model2.predict(MESSAGE_2D)\n",
        "predicted_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEEngNBxSkHY",
        "outputId": "4cf8a080-c149-4e61-ad94-ecf51f010717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.36314412e-07, 3.40612693e-04, 1.02531274e-04, 1.96385326e-06,\n",
              "        6.97136298e-02, 3.50540597e-03, 2.61299429e-04, 1.18831551e-04,\n",
              "        3.03108215e-01, 9.46780294e-03, 3.37022997e-04, 4.41452983e-04,\n",
              "        9.62184131e-05, 4.68309212e-04, 3.27150519e-06, 1.67010658e-05,\n",
              "        1.10881934e-02, 1.75135683e-05, 9.93007887e-03, 2.16546983e-01,\n",
              "        9.31563409e-06, 5.37548040e-05, 7.64139077e-06, 3.88597196e-04,\n",
              "        1.39493728e-03, 3.72579575e-01],\n",
              "       [3.94673480e-05, 3.87245119e-01, 2.36702431e-02, 1.34132232e-03,\n",
              "        3.88824269e-02, 2.00200290e-03, 3.47455451e-03, 2.70248740e-03,\n",
              "        3.36231210e-06, 2.14022930e-05, 2.37007275e-01, 1.40808439e-02,\n",
              "        2.82621849e-02, 7.96324294e-03, 1.60261035e-01, 7.79844727e-03,\n",
              "        3.27412458e-03, 1.85500011e-02, 1.43539878e-02, 4.96965658e-04,\n",
              "        6.53366733e-04, 1.30964536e-03, 4.08940278e-02, 2.09422782e-03,\n",
              "        3.84143568e-05, 3.57981492e-03],\n",
              "       [1.33519784e-09, 1.54557161e-03, 4.74080618e-04, 1.43770703e-05,\n",
              "        9.07906175e-01, 4.10414860e-02, 1.12148817e-03, 3.12415239e-07,\n",
              "        9.00197483e-05, 8.84216718e-07, 9.24611173e-04, 3.61755397e-03,\n",
              "        5.78280342e-05, 1.74930392e-04, 6.86508138e-05, 1.91362211e-04,\n",
              "        1.23003274e-05, 3.03350953e-05, 3.03688869e-02, 1.73593336e-03,\n",
              "        1.52446376e-08, 2.27648982e-07, 2.82353085e-06, 6.50915308e-06,\n",
              "        2.46353534e-06, 1.06112901e-02],\n",
              "       [5.03085129e-09, 1.30065598e-06, 3.44542400e-06, 2.58874962e-07,\n",
              "        2.75853518e-02, 9.53316152e-01, 1.78547918e-06, 6.93809810e-08,\n",
              "        2.98356696e-04, 5.22790655e-09, 3.70220587e-05, 2.34628824e-04,\n",
              "        1.31513696e-06, 3.70804105e-06, 4.79296177e-05, 6.04921812e-03,\n",
              "        1.21755107e-07, 5.75131196e-07, 1.05205849e-02, 9.70808498e-04,\n",
              "        6.34589714e-10, 1.98263081e-07, 9.96068295e-08, 3.59911178e-06,\n",
              "        8.38981359e-04, 8.43539092e-05],\n",
              "       [2.37870630e-08, 2.95096896e-07, 1.72646011e-07, 2.64328032e-10,\n",
              "        1.14817958e-04, 1.26062092e-04, 6.30064335e-07, 6.62646585e-07,\n",
              "        9.01351511e-01, 9.87916021e-04, 1.10033085e-07, 3.28910318e-07,\n",
              "        1.01550336e-06, 6.20960200e-05, 1.22002142e-08, 3.14694944e-06,\n",
              "        7.66967423e-06, 1.40842333e-08, 2.87302479e-04, 7.82020763e-02,\n",
              "        1.16396386e-08, 1.49010896e-06, 4.28712639e-08, 5.07955374e-05,\n",
              "        1.74022708e-02, 1.39958307e-03],\n",
              "       [8.98353534e-08, 2.29722809e-06, 2.40321434e-08, 8.70042094e-11,\n",
              "        3.76077855e-06, 7.49062181e-08, 1.49558532e-09, 9.76585504e-03,\n",
              "        7.28441947e-08, 3.83594634e-05, 1.30880062e-05, 4.47236602e-07,\n",
              "        1.34397030e-03, 7.52423693e-06, 4.37777908e-06, 3.86278458e-07,\n",
              "        1.06910532e-02, 1.58220189e-06, 2.26511074e-05, 1.88054446e-06,\n",
              "        6.01130596e-04, 9.74604726e-01, 2.39132060e-05, 2.80712522e-03,\n",
              "        3.83027291e-05, 2.73902042e-05],\n",
              "       [2.31635045e-12, 8.50570018e-07, 1.09404518e-05, 4.03161460e-09,\n",
              "        2.58598089e-01, 4.21884144e-03, 2.51859310e-05, 8.75400907e-09,\n",
              "        5.04217803e-01, 3.98842258e-06, 7.88034868e-07, 1.48405743e-04,\n",
              "        1.61560379e-07, 2.62939693e-06, 1.63695919e-08, 1.64854058e-07,\n",
              "        1.06390144e-06, 5.83725823e-09, 8.72965902e-03, 8.16885456e-02,\n",
              "        1.10718351e-10, 5.04383812e-10, 3.39319883e-09, 6.68076439e-08,\n",
              "        2.46965647e-06, 1.42350271e-01],\n",
              "       [4.53938220e-08, 9.15487766e-01, 7.64684446e-05, 1.04380073e-03,\n",
              "        1.72226384e-04, 3.56712434e-07, 1.00281583e-02, 9.07182738e-08,\n",
              "        8.17650871e-08, 1.77810740e-04, 3.87144624e-03, 2.22882700e-05,\n",
              "        1.51465926e-07, 9.78156822e-06, 2.88595620e-05, 1.34981128e-05,\n",
              "        4.26854858e-05, 6.86695725e-02, 1.69061352e-08, 3.92907197e-07,\n",
              "        1.31899196e-05, 7.88255591e-07, 1.21365192e-05, 9.91322622e-07,\n",
              "        2.40076514e-09, 3.27409420e-04],\n",
              "       [4.40838601e-04, 4.77140682e-04, 1.64305116e-03, 2.81344728e-05,\n",
              "        1.18656331e-04, 9.34905547e-05, 4.92580693e-05, 5.86303882e-02,\n",
              "        3.54637308e-07, 1.83261858e-04, 1.31759990e-03, 3.88807995e-04,\n",
              "        1.28240034e-03, 1.42190416e-04, 6.35755479e-01, 1.85599256e-05,\n",
              "        5.52154966e-02, 1.90908413e-05, 4.00059740e-04, 1.35449236e-05,\n",
              "        1.26484245e-01, 1.82319735e-03, 1.15296811e-01, 7.50280451e-05,\n",
              "        1.28779948e-06, 1.01584606e-04],\n",
              "       [5.65601920e-04, 3.35975438e-02, 3.41514661e-03, 5.30051875e-05,\n",
              "        1.66127738e-02, 5.67576615e-04, 6.69599604e-03, 1.08481916e-02,\n",
              "        9.22501087e-03, 4.55332175e-02, 4.78452109e-02, 1.36602015e-04,\n",
              "        4.14093398e-02, 4.21208531e-01, 8.45253700e-04, 5.05137490e-03,\n",
              "        6.51632482e-03, 6.76680962e-03, 8.41668155e-03, 6.65604547e-02,\n",
              "        1.34007749e-03, 2.02747546e-02, 3.55095556e-03, 1.68412060e-01,\n",
              "        1.34276869e-02, 6.11238517e-02],\n",
              "       [2.27662160e-12, 1.00092867e-09, 2.04875725e-08, 2.15837972e-11,\n",
              "        1.24765013e-03, 1.03808520e-02, 5.33671995e-09, 8.74378570e-10,\n",
              "        9.43420887e-01, 2.54803272e-08, 2.45809018e-09, 3.69270128e-06,\n",
              "        7.32105487e-09, 3.75742566e-08, 4.33830127e-09, 3.00362387e-07,\n",
              "        6.61004851e-09, 2.32228230e-11, 7.38387834e-03, 3.70656028e-02,\n",
              "        3.43013270e-12, 9.02868891e-11, 2.04918901e-10, 8.57539018e-09,\n",
              "        2.56292871e-04, 2.40815833e-04],\n",
              "       [3.04374930e-06, 5.36802283e-04, 9.09107039e-05, 3.74551679e-09,\n",
              "        2.63094589e-05, 1.54386584e-08, 4.67532800e-07, 3.40265548e-03,\n",
              "        9.33315789e-08, 1.23990721e-05, 7.78857386e-04, 1.03540636e-07,\n",
              "        1.46752611e-01, 8.40562999e-01, 4.20438009e-05, 3.99386954e-06,\n",
              "        3.91733265e-05, 6.24512404e-06, 3.10113857e-04, 9.15443379e-05,\n",
              "        1.35729410e-04, 1.93618145e-03, 4.15297487e-04, 4.81604924e-03,\n",
              "        7.65468576e-06, 2.86886934e-05],\n",
              "       [2.05071188e-07, 8.00821185e-03, 4.14100848e-02, 9.42338083e-04,\n",
              "        1.14201172e-03, 5.50199002e-06, 9.29134190e-01, 3.17542188e-08,\n",
              "        1.31230991e-04, 7.56856473e-03, 5.56847590e-06, 4.90561547e-03,\n",
              "        1.86825169e-08, 1.24677581e-05, 1.06553068e-06, 1.28870965e-06,\n",
              "        2.32004568e-05, 8.54426762e-05, 4.57741180e-06, 2.30406204e-05,\n",
              "        7.02878833e-06, 4.45054038e-09, 2.84765611e-05, 6.75991316e-08,\n",
              "        5.25170241e-10, 6.55973889e-03],\n",
              "       [1.85842655e-04, 9.42225195e-03, 1.28215663e-02, 2.16015906e-06,\n",
              "        2.77674258e-06, 5.98417671e-08, 9.99779877e-05, 1.06687788e-02,\n",
              "        3.54491263e-08, 8.83316796e-04, 5.63941758e-05, 7.99006193e-06,\n",
              "        6.01743290e-04, 4.48812265e-03, 2.40309536e-02, 1.15974771e-07,\n",
              "        4.30408819e-03, 8.03253351e-06, 2.41471189e-05, 2.12121586e-06,\n",
              "        1.69454385e-02, 3.00572792e-05, 9.15305078e-01, 2.30202681e-06,\n",
              "        3.11391246e-09, 1.06636115e-04],\n",
              "       [1.76877339e-08, 1.01394721e-06, 5.15321574e-07, 2.10147522e-09,\n",
              "        2.98307394e-04, 5.94126468e-04, 1.79067672e-06, 9.46928765e-07,\n",
              "        8.97748351e-01, 9.61006910e-04, 1.95616195e-07, 1.78800451e-06,\n",
              "        1.60772754e-06, 4.61265263e-05, 4.46465336e-08, 3.06124502e-06,\n",
              "        1.90254323e-05, 2.09297610e-08, 1.06672896e-03, 9.00328234e-02,\n",
              "        2.03109458e-08, 7.66088988e-07, 1.06621570e-07, 2.53747075e-05,\n",
              "        5.35918726e-03, 3.83699196e-03],\n",
              "       [6.35873008e-08, 3.38489772e-05, 4.44792182e-04, 6.87309694e-07,\n",
              "        1.18167490e-01, 1.89133058e-03, 2.52934289e-04, 1.50894821e-05,\n",
              "        4.29792970e-01, 6.50694827e-04, 1.89225073e-04, 1.17645843e-03,\n",
              "        1.56310762e-05, 6.54143296e-05, 3.04299328e-06, 3.96361247e-05,\n",
              "        3.66092310e-04, 4.87481793e-06, 2.89696641e-03, 1.14252910e-01,\n",
              "        2.31151489e-06, 6.58073122e-06, 3.08238259e-06, 8.98958315e-05,\n",
              "        1.14390324e-03, 3.28494161e-01],\n",
              "       [9.66390729e-01, 3.41099076e-05, 3.71569564e-04, 5.17378235e-03,\n",
              "        1.34998089e-07, 1.82439919e-06, 3.57198529e-04, 6.74368584e-06,\n",
              "        2.20271247e-07, 6.00631529e-06, 1.47239034e-05, 5.80134702e-06,\n",
              "        4.48418831e-07, 1.72959626e-04, 2.58828923e-02, 4.13394475e-04,\n",
              "        3.33968674e-05, 9.17817579e-06, 4.75470685e-09, 3.12323820e-08,\n",
              "        4.95158274e-05, 2.80843710e-06, 1.07022643e-03, 2.15886018e-07,\n",
              "        2.06916911e-06, 2.49579202e-08],\n",
              "       [3.36942066e-05, 9.41151939e-03, 5.79192943e-04, 6.50550891e-03,\n",
              "        2.69547701e-01, 4.23793346e-01, 3.65644805e-02, 4.27619898e-06,\n",
              "        9.21948627e-02, 2.99586594e-04, 8.22502933e-03, 2.13236194e-02,\n",
              "        2.65603248e-05, 5.80988417e-04, 1.56385463e-03, 6.65934607e-02,\n",
              "        2.50650803e-04, 5.44009358e-03, 7.10197899e-04, 2.96777878e-02,\n",
              "        2.05519905e-06, 5.38517124e-05, 3.34966244e-05, 5.69934236e-05,\n",
              "        3.63386446e-03, 2.28932872e-02],\n",
              "       [2.39841302e-06, 2.20419057e-02, 6.28179580e-04, 1.28755257e-01,\n",
              "        4.70486470e-02, 2.86868960e-02, 2.26670101e-01, 1.64541532e-07,\n",
              "        6.88440865e-03, 2.24584737e-03, 6.02217915e-04, 5.15142381e-01,\n",
              "        1.01564092e-06, 3.83712904e-06, 2.67584488e-04, 5.03822521e-04,\n",
              "        6.00423547e-04, 5.02167130e-03, 1.20227945e-04, 4.93908126e-04,\n",
              "        3.01024670e-06, 6.14423868e-07, 1.95497632e-05, 3.95795666e-07,\n",
              "        1.68687802e-06, 1.42539125e-02],\n",
              "       [3.04104075e-09, 3.58950533e-03, 9.49803944e-05, 2.02663372e-07,\n",
              "        2.40925420e-02, 2.96479650e-03, 1.21887701e-04, 1.64790341e-04,\n",
              "        9.09286973e-06, 1.01562782e-05, 1.27167837e-03, 1.22645157e-04,\n",
              "        5.79393283e-03, 3.08467541e-04, 1.30024768e-04, 4.68878807e-06,\n",
              "        9.92482645e-04, 4.44737043e-06, 9.38706279e-01, 1.83837349e-03,\n",
              "        1.41410419e-06, 1.08622926e-05, 3.45636974e-04, 3.78425611e-04,\n",
              "        9.11396171e-07, 1.90417096e-02],\n",
              "       [4.11086802e-07, 1.59516887e-04, 4.61736772e-06, 4.46782394e-07,\n",
              "        2.49241816e-06, 1.69201382e-08, 1.60784402e-04, 1.55067414e-06,\n",
              "        3.55533557e-03, 9.93305504e-01, 1.32114053e-06, 1.27608928e-07,\n",
              "        3.06357691e-07, 1.13169444e-04, 3.41273747e-08, 5.81455595e-09,\n",
              "        2.40428402e-04, 2.12484292e-06, 1.16325616e-07, 3.27958078e-05,\n",
              "        4.29866486e-04, 8.34899993e-07, 1.34405104e-06, 2.71830140e-06,\n",
              "        4.49590544e-07, 1.98362535e-03],\n",
              "       [5.10901145e-06, 1.29965701e-05, 1.68581645e-03, 1.04258957e-08,\n",
              "        9.81087851e-06, 2.47060807e-08, 7.06944491e-07, 1.29010126e-01,\n",
              "        1.01564619e-05, 1.09651615e-03, 5.62569585e-05, 1.11360268e-05,\n",
              "        1.42503018e-03, 1.63652396e-04, 2.97954015e-04, 2.53161147e-07,\n",
              "        5.74390450e-03, 4.95157764e-08, 3.43251188e-04, 2.46226068e-06,\n",
              "        8.46648455e-01, 3.65001033e-03, 3.70695745e-03, 2.51400145e-03,\n",
              "        1.09618782e-06, 3.60433105e-03],\n",
              "       [4.97498154e-11, 1.45034928e-05, 1.36659455e-05, 4.11231684e-14,\n",
              "        2.16435865e-06, 3.93790729e-11, 2.87476737e-10, 6.63670525e-03,\n",
              "        2.37441816e-10, 2.46818610e-09, 3.67590401e-04, 1.07271980e-09,\n",
              "        9.57648933e-01, 2.59433407e-02, 3.74528713e-06, 3.63889150e-08,\n",
              "        2.13456886e-07, 3.18244298e-09, 2.77614733e-03, 2.82634937e-06,\n",
              "        9.69492817e-07, 5.44536779e-05, 7.15134665e-05, 6.36667851e-03,\n",
              "        1.93547045e-08, 9.65434083e-05],\n",
              "       [8.53503570e-06, 3.60502017e-05, 2.01142921e-05, 2.16141725e-05,\n",
              "        1.10264553e-03, 4.94872918e-03, 1.54389400e-05, 5.11816950e-07,\n",
              "        6.73707200e-06, 1.68551466e-08, 2.01791874e-03, 1.18558673e-04,\n",
              "        5.38403719e-06, 2.37643217e-05, 1.32873224e-03, 9.77703035e-01,\n",
              "        1.27936431e-07, 2.99824023e-04, 8.54385507e-06, 3.14206118e-05,\n",
              "        4.62866119e-08, 4.04018901e-05, 2.73997307e-06, 1.71427993e-04,\n",
              "        1.20814973e-02, 6.24362110e-06],\n",
              "       [2.08201564e-05, 9.10265662e-04, 5.10031387e-05, 4.64248706e-06,\n",
              "        6.48217901e-05, 5.55141423e-06, 1.39220365e-05, 1.46429380e-02,\n",
              "        2.70857271e-07, 1.87742745e-03, 3.96305695e-04, 1.58141447e-05,\n",
              "        1.35853712e-04, 1.07070060e-04, 4.70631197e-03, 8.24959585e-08,\n",
              "        9.37697828e-01, 4.07924708e-05, 1.06450825e-04, 1.56449514e-05,\n",
              "        3.39636654e-02, 2.86002527e-04, 4.87966649e-03, 2.92500476e-06,\n",
              "        2.57013326e-08, 5.38023305e-05],\n",
              "       [1.05713319e-03, 4.56259695e-05, 8.78898019e-04, 3.00099600e-06,\n",
              "        1.10243236e-05, 2.16966737e-06, 5.39646817e-06, 5.10344766e-02,\n",
              "        1.75969049e-07, 2.82467052e-04, 9.60993420e-05, 1.45291371e-04,\n",
              "        1.09839416e-03, 1.86935547e-04, 1.16009630e-01, 2.00811110e-06,\n",
              "        2.84461211e-02, 4.78114316e-06, 1.59949886e-05, 2.58893760e-06,\n",
              "        7.10637987e-01, 8.29282217e-03, 8.16873312e-02, 1.81006162e-05,\n",
              "        1.01972671e-06, 3.46535489e-05],\n",
              "       [3.44805759e-12, 3.43098616e-09, 4.39407941e-08, 6.97424479e-11,\n",
              "        1.43676787e-03, 3.90004041e-03, 1.56874282e-08, 2.13700990e-09,\n",
              "        9.49324608e-01, 1.04394474e-07, 6.11862294e-09, 3.47368655e-06,\n",
              "        1.03108544e-08, 7.28019742e-08, 3.53993812e-09, 1.31046576e-07,\n",
              "        3.27469607e-08, 7.31116279e-11, 3.00447294e-03, 4.15879972e-02,\n",
              "        1.24508693e-11, 1.62951305e-10, 3.34687444e-10, 9.87431115e-09,\n",
              "        1.23535836e-04, 6.18703256e-04],\n",
              "       [4.48792824e-04, 1.51935755e-03, 8.90186727e-01, 4.48972889e-04,\n",
              "        2.96038366e-03, 4.27868217e-04, 6.63421825e-02, 1.37358302e-05,\n",
              "        2.31712038e-05, 4.85070123e-05, 1.04379506e-04, 6.83357287e-03,\n",
              "        2.28463014e-05, 1.42808794e-03, 1.48797326e-03, 1.33331725e-03,\n",
              "        2.31315043e-05, 6.05900959e-06, 9.35374002e-04, 3.06020927e-04,\n",
              "        4.46337690e-05, 6.00373028e-07, 2.41109170e-02, 1.23805876e-05,\n",
              "        1.44563353e-06, 9.29516042e-04],\n",
              "       [4.16529275e-07, 8.26340448e-03, 2.35746207e-04, 1.91456711e-05,\n",
              "        1.00442218e-02, 8.20004425e-05, 3.14391975e-04, 8.28129705e-05,\n",
              "        1.29415378e-06, 8.11295104e-06, 9.54982162e-01, 3.26996524e-05,\n",
              "        1.07756560e-03, 1.54092710e-03, 6.56356104e-04, 3.85727733e-03,\n",
              "        3.37495585e-05, 1.16043165e-02, 3.82956678e-05, 2.31109312e-04,\n",
              "        5.24109828e-06, 1.89462633e-04, 6.00940984e-05, 2.73503899e-03,\n",
              "        1.32792644e-04, 3.77127761e-03],\n",
              "       [1.68819403e-10, 3.26031409e-06, 5.99830310e-05, 2.04194424e-04,\n",
              "        8.77279136e-03, 3.85567953e-04, 6.82094553e-03, 4.36039356e-12,\n",
              "        1.60708332e-05, 1.37352288e-06, 4.17787277e-07, 9.83657420e-01,\n",
              "        6.75758546e-11, 1.67868344e-10, 6.09228607e-07, 7.31484988e-07,\n",
              "        4.99674755e-08, 2.96312214e-06, 2.09539894e-06, 3.40215621e-07,\n",
              "        5.63164937e-09, 7.05306473e-12, 1.59613691e-08, 9.44144589e-12,\n",
              "        4.38457326e-11, 7.11821704e-05],\n",
              "       [4.46763444e-08, 8.58900293e-08, 2.52316656e-08, 1.38921400e-11,\n",
              "        4.32657471e-05, 1.80510324e-04, 2.99148617e-08, 4.45675937e-07,\n",
              "        2.52261816e-04, 3.39128746e-07, 7.93756226e-06, 2.83291435e-08,\n",
              "        1.23326299e-05, 1.24215047e-04, 9.36787288e-08, 6.34953845e-03,\n",
              "        2.90729485e-08, 8.45748787e-08, 1.86987265e-04, 5.13815228e-03,\n",
              "        5.02329234e-10, 6.43502281e-05, 5.81471404e-08, 1.59537736e-02,\n",
              "        9.71666098e-01, 1.93661235e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_list = \"\"\n",
        "for num in range(len(predicted_message)):\n",
        "    max_val = 0\n",
        "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    message_list += predict\n",
        "print(message_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3jvVHJvSclF",
        "outputId": "c84f6376-afce-4821-9558-abeb56401620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZBEFIVIBONINGWIIAFLSJUMPQUICKLY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
        "list_mismatch = []\n",
        "num_total, num_correct = len(actual_message), len(message_list)\n",
        "for i in range(len(actual_message)):\n",
        "    if message_list[i] != actual_message[i]:\n",
        "        num_correct -= 1\n",
        "        list_mismatch.append([actual_message[i], message_list[i]])\n",
        "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_mismatch) == 0:\n",
        "    print(\"The message appears to be decoded correctly.\")\n",
        "else:\n",
        "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_mismatch)):\n",
        "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv11X09hSb3X",
        "outputId": "f843e0b0-5a3d-47b7-af42-7c01601944df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy between predicted and actual message: 77.41935483870968%\n",
            "Here is the list of letters that are a mismatch between predicted and actual message:\n",
            "Actual    Predicted\n",
            "T         Z\n",
            "H         B\n",
            "E         I\n",
            "X         N\n",
            "Z         I\n",
            "R         F\n",
            "D         L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Does the performance improve?"
      ],
      "metadata": {
        "id": "QZPpiD5BrI6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A: No, the performance does not improve simply by adding more hidden layers."
      ],
      "metadata": {
        "id": "r-GVfQg0rJyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Repeat experiment (7), adding additional layers of the same size until the message is decoded correctly. What results do you observe?"
      ],
      "metadata": {
        "id": "aFxJNZe6gfw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = tf.keras.models.Sequential()\n",
        "model3.add(tf.keras.Input(shape=(256,)))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer1\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer2\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer3\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer4\"))\n",
        "model3.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name=\"Hidden_layer5\"))\n",
        "model3.add(tf.keras.layers.Dense(26, activation=tf.keras.activations.softmax, name=\"Output_layer\"))\n",
        "\n",
        "model3.summary()\n",
        "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "             metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"categorical_accuracy\", patience=50, restore_best_weights=False)\n",
        "model3.fit(x=TRAINING_SET_2D, y=unicode_cat_train, epochs=9999, verbose=2, callbacks=[callback])\n",
        "model3.evaluate(x=TEST_SET_2D, y=unicode_cat_test, verbose=1)\n"
      ],
      "metadata": {
        "id": "dKLc7F4KpJ2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f7a744-760b-4864-9f41-4dfdde21b1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Hidden_layer1 (Dense)       (None, 32)                8224      \n",
            "                                                                 \n",
            " Hidden_layer2 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer3 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer4 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Hidden_layer5 (Dense)       (None, 32)                1056      \n",
            "                                                                 \n",
            " Output_layer (Dense)        (None, 26)                858       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,306\n",
            "Trainable params: 13,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/9999\n",
            "2/2 - 1s - loss: 3.2656 - categorical_accuracy: 0.0385 - 536ms/epoch - 268ms/step\n",
            "Epoch 2/9999\n",
            "2/2 - 0s - loss: 3.2324 - categorical_accuracy: 0.0577 - 12ms/epoch - 6ms/step\n",
            "Epoch 3/9999\n",
            "2/2 - 0s - loss: 3.2112 - categorical_accuracy: 0.0962 - 8ms/epoch - 4ms/step\n",
            "Epoch 4/9999\n",
            "2/2 - 0s - loss: 3.1946 - categorical_accuracy: 0.0962 - 8ms/epoch - 4ms/step\n",
            "Epoch 5/9999\n",
            "2/2 - 0s - loss: 3.1782 - categorical_accuracy: 0.0769 - 8ms/epoch - 4ms/step\n",
            "Epoch 6/9999\n",
            "2/2 - 0s - loss: 3.1598 - categorical_accuracy: 0.0962 - 7ms/epoch - 4ms/step\n",
            "Epoch 7/9999\n",
            "2/2 - 0s - loss: 3.1421 - categorical_accuracy: 0.1154 - 9ms/epoch - 4ms/step\n",
            "Epoch 8/9999\n",
            "2/2 - 0s - loss: 3.1209 - categorical_accuracy: 0.1346 - 9ms/epoch - 4ms/step\n",
            "Epoch 9/9999\n",
            "2/2 - 0s - loss: 3.0965 - categorical_accuracy: 0.1346 - 9ms/epoch - 4ms/step\n",
            "Epoch 10/9999\n",
            "2/2 - 0s - loss: 3.0703 - categorical_accuracy: 0.1154 - 8ms/epoch - 4ms/step\n",
            "Epoch 11/9999\n",
            "2/2 - 0s - loss: 3.0437 - categorical_accuracy: 0.1154 - 9ms/epoch - 4ms/step\n",
            "Epoch 12/9999\n",
            "2/2 - 0s - loss: 3.0193 - categorical_accuracy: 0.1154 - 11ms/epoch - 5ms/step\n",
            "Epoch 13/9999\n",
            "2/2 - 0s - loss: 2.9879 - categorical_accuracy: 0.1346 - 9ms/epoch - 5ms/step\n",
            "Epoch 14/9999\n",
            "2/2 - 0s - loss: 2.9580 - categorical_accuracy: 0.1346 - 8ms/epoch - 4ms/step\n",
            "Epoch 15/9999\n",
            "2/2 - 0s - loss: 2.9283 - categorical_accuracy: 0.1346 - 8ms/epoch - 4ms/step\n",
            "Epoch 16/9999\n",
            "2/2 - 0s - loss: 2.8980 - categorical_accuracy: 0.1346 - 8ms/epoch - 4ms/step\n",
            "Epoch 17/9999\n",
            "2/2 - 0s - loss: 2.8659 - categorical_accuracy: 0.1346 - 10ms/epoch - 5ms/step\n",
            "Epoch 18/9999\n",
            "2/2 - 0s - loss: 2.8297 - categorical_accuracy: 0.1346 - 9ms/epoch - 4ms/step\n",
            "Epoch 19/9999\n",
            "2/2 - 0s - loss: 2.7965 - categorical_accuracy: 0.1538 - 8ms/epoch - 4ms/step\n",
            "Epoch 20/9999\n",
            "2/2 - 0s - loss: 2.7563 - categorical_accuracy: 0.1731 - 8ms/epoch - 4ms/step\n",
            "Epoch 21/9999\n",
            "2/2 - 0s - loss: 2.7175 - categorical_accuracy: 0.1731 - 9ms/epoch - 5ms/step\n",
            "Epoch 22/9999\n",
            "2/2 - 0s - loss: 2.6756 - categorical_accuracy: 0.1538 - 8ms/epoch - 4ms/step\n",
            "Epoch 23/9999\n",
            "2/2 - 0s - loss: 2.6320 - categorical_accuracy: 0.2115 - 8ms/epoch - 4ms/step\n",
            "Epoch 24/9999\n",
            "2/2 - 0s - loss: 2.5873 - categorical_accuracy: 0.2308 - 8ms/epoch - 4ms/step\n",
            "Epoch 25/9999\n",
            "2/2 - 0s - loss: 2.5397 - categorical_accuracy: 0.2308 - 7ms/epoch - 4ms/step\n",
            "Epoch 26/9999\n",
            "2/2 - 0s - loss: 2.4882 - categorical_accuracy: 0.2692 - 10ms/epoch - 5ms/step\n",
            "Epoch 27/9999\n",
            "2/2 - 0s - loss: 2.4379 - categorical_accuracy: 0.3077 - 10ms/epoch - 5ms/step\n",
            "Epoch 28/9999\n",
            "2/2 - 0s - loss: 2.3848 - categorical_accuracy: 0.3462 - 9ms/epoch - 5ms/step\n",
            "Epoch 29/9999\n",
            "2/2 - 0s - loss: 2.3282 - categorical_accuracy: 0.3654 - 9ms/epoch - 5ms/step\n",
            "Epoch 30/9999\n",
            "2/2 - 0s - loss: 2.2682 - categorical_accuracy: 0.3654 - 9ms/epoch - 4ms/step\n",
            "Epoch 31/9999\n",
            "2/2 - 0s - loss: 2.2053 - categorical_accuracy: 0.3462 - 9ms/epoch - 5ms/step\n",
            "Epoch 32/9999\n",
            "2/2 - 0s - loss: 2.1376 - categorical_accuracy: 0.4038 - 10ms/epoch - 5ms/step\n",
            "Epoch 33/9999\n",
            "2/2 - 0s - loss: 2.0740 - categorical_accuracy: 0.4423 - 11ms/epoch - 5ms/step\n",
            "Epoch 34/9999\n",
            "2/2 - 0s - loss: 2.0003 - categorical_accuracy: 0.4808 - 8ms/epoch - 4ms/step\n",
            "Epoch 35/9999\n",
            "2/2 - 0s - loss: 1.9379 - categorical_accuracy: 0.5385 - 8ms/epoch - 4ms/step\n",
            "Epoch 36/9999\n",
            "2/2 - 0s - loss: 1.8667 - categorical_accuracy: 0.5192 - 8ms/epoch - 4ms/step\n",
            "Epoch 37/9999\n",
            "2/2 - 0s - loss: 1.7954 - categorical_accuracy: 0.5385 - 7ms/epoch - 4ms/step\n",
            "Epoch 38/9999\n",
            "2/2 - 0s - loss: 1.7191 - categorical_accuracy: 0.5962 - 10ms/epoch - 5ms/step\n",
            "Epoch 39/9999\n",
            "2/2 - 0s - loss: 1.6514 - categorical_accuracy: 0.5769 - 11ms/epoch - 6ms/step\n",
            "Epoch 40/9999\n",
            "2/2 - 0s - loss: 1.5853 - categorical_accuracy: 0.5962 - 11ms/epoch - 5ms/step\n",
            "Epoch 41/9999\n",
            "2/2 - 0s - loss: 1.5127 - categorical_accuracy: 0.6538 - 8ms/epoch - 4ms/step\n",
            "Epoch 42/9999\n",
            "2/2 - 0s - loss: 1.4506 - categorical_accuracy: 0.6538 - 8ms/epoch - 4ms/step\n",
            "Epoch 43/9999\n",
            "2/2 - 0s - loss: 1.3800 - categorical_accuracy: 0.6731 - 7ms/epoch - 4ms/step\n",
            "Epoch 44/9999\n",
            "2/2 - 0s - loss: 1.3137 - categorical_accuracy: 0.6538 - 7ms/epoch - 4ms/step\n",
            "Epoch 45/9999\n",
            "2/2 - 0s - loss: 1.2565 - categorical_accuracy: 0.6538 - 9ms/epoch - 5ms/step\n",
            "Epoch 46/9999\n",
            "2/2 - 0s - loss: 1.1900 - categorical_accuracy: 0.6731 - 8ms/epoch - 4ms/step\n",
            "Epoch 47/9999\n",
            "2/2 - 0s - loss: 1.1365 - categorical_accuracy: 0.6731 - 8ms/epoch - 4ms/step\n",
            "Epoch 48/9999\n",
            "2/2 - 0s - loss: 1.0849 - categorical_accuracy: 0.7308 - 8ms/epoch - 4ms/step\n",
            "Epoch 49/9999\n",
            "2/2 - 0s - loss: 1.0276 - categorical_accuracy: 0.7115 - 10ms/epoch - 5ms/step\n",
            "Epoch 50/9999\n",
            "2/2 - 0s - loss: 0.9787 - categorical_accuracy: 0.7115 - 7ms/epoch - 4ms/step\n",
            "Epoch 51/9999\n",
            "2/2 - 0s - loss: 0.9278 - categorical_accuracy: 0.6923 - 8ms/epoch - 4ms/step\n",
            "Epoch 52/9999\n",
            "2/2 - 0s - loss: 0.8783 - categorical_accuracy: 0.7500 - 8ms/epoch - 4ms/step\n",
            "Epoch 53/9999\n",
            "2/2 - 0s - loss: 0.8350 - categorical_accuracy: 0.7885 - 7ms/epoch - 4ms/step\n",
            "Epoch 54/9999\n",
            "2/2 - 0s - loss: 0.7896 - categorical_accuracy: 0.8077 - 9ms/epoch - 4ms/step\n",
            "Epoch 55/9999\n",
            "2/2 - 0s - loss: 0.7473 - categorical_accuracy: 0.7885 - 10ms/epoch - 5ms/step\n",
            "Epoch 56/9999\n",
            "2/2 - 0s - loss: 0.7001 - categorical_accuracy: 0.8462 - 11ms/epoch - 5ms/step\n",
            "Epoch 57/9999\n",
            "2/2 - 0s - loss: 0.6668 - categorical_accuracy: 0.8654 - 9ms/epoch - 5ms/step\n",
            "Epoch 58/9999\n",
            "2/2 - 0s - loss: 0.6198 - categorical_accuracy: 0.9038 - 8ms/epoch - 4ms/step\n",
            "Epoch 59/9999\n",
            "2/2 - 0s - loss: 0.5890 - categorical_accuracy: 0.8846 - 10ms/epoch - 5ms/step\n",
            "Epoch 60/9999\n",
            "2/2 - 0s - loss: 0.5493 - categorical_accuracy: 0.9038 - 9ms/epoch - 5ms/step\n",
            "Epoch 61/9999\n",
            "2/2 - 0s - loss: 0.5130 - categorical_accuracy: 0.9423 - 8ms/epoch - 4ms/step\n",
            "Epoch 62/9999\n",
            "2/2 - 0s - loss: 0.4845 - categorical_accuracy: 0.9615 - 8ms/epoch - 4ms/step\n",
            "Epoch 63/9999\n",
            "2/2 - 0s - loss: 0.4495 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 64/9999\n",
            "2/2 - 0s - loss: 0.4145 - categorical_accuracy: 0.9808 - 9ms/epoch - 4ms/step\n",
            "Epoch 65/9999\n",
            "2/2 - 0s - loss: 0.3842 - categorical_accuracy: 0.9808 - 8ms/epoch - 4ms/step\n",
            "Epoch 66/9999\n",
            "2/2 - 0s - loss: 0.3577 - categorical_accuracy: 0.9615 - 10ms/epoch - 5ms/step\n",
            "Epoch 67/9999\n",
            "2/2 - 0s - loss: 0.3303 - categorical_accuracy: 0.9615 - 7ms/epoch - 4ms/step\n",
            "Epoch 68/9999\n",
            "2/2 - 0s - loss: 0.3050 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 69/9999\n",
            "2/2 - 0s - loss: 0.2871 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 70/9999\n",
            "2/2 - 0s - loss: 0.2591 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 71/9999\n",
            "2/2 - 0s - loss: 0.2395 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 72/9999\n",
            "2/2 - 0s - loss: 0.2218 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 73/9999\n",
            "2/2 - 0s - loss: 0.2029 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 74/9999\n",
            "2/2 - 0s - loss: 0.1878 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 75/9999\n",
            "2/2 - 0s - loss: 0.1717 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 76/9999\n",
            "2/2 - 0s - loss: 0.1572 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 77/9999\n",
            "2/2 - 0s - loss: 0.1464 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 78/9999\n",
            "2/2 - 0s - loss: 0.1356 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 79/9999\n",
            "2/2 - 0s - loss: 0.1234 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 80/9999\n",
            "2/2 - 0s - loss: 0.1171 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 81/9999\n",
            "2/2 - 0s - loss: 0.1095 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 82/9999\n",
            "2/2 - 0s - loss: 0.1000 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 83/9999\n",
            "2/2 - 0s - loss: 0.0906 - categorical_accuracy: 1.0000 - 11ms/epoch - 6ms/step\n",
            "Epoch 84/9999\n",
            "2/2 - 0s - loss: 0.0839 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 85/9999\n",
            "2/2 - 0s - loss: 0.0793 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 86/9999\n",
            "2/2 - 0s - loss: 0.0751 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 87/9999\n",
            "2/2 - 0s - loss: 0.0697 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 88/9999\n",
            "2/2 - 0s - loss: 0.0642 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 89/9999\n",
            "2/2 - 0s - loss: 0.0607 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 90/9999\n",
            "2/2 - 0s - loss: 0.0570 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 91/9999\n",
            "2/2 - 0s - loss: 0.0536 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 92/9999\n",
            "2/2 - 0s - loss: 0.0517 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 93/9999\n",
            "2/2 - 0s - loss: 0.0475 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 94/9999\n",
            "2/2 - 0s - loss: 0.0448 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 95/9999\n",
            "2/2 - 0s - loss: 0.0430 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 96/9999\n",
            "2/2 - 0s - loss: 0.0406 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 97/9999\n",
            "2/2 - 0s - loss: 0.0386 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 98/9999\n",
            "2/2 - 0s - loss: 0.0366 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 99/9999\n",
            "2/2 - 0s - loss: 0.0347 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 100/9999\n",
            "2/2 - 0s - loss: 0.0331 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 101/9999\n",
            "2/2 - 0s - loss: 0.0318 - categorical_accuracy: 1.0000 - 9ms/epoch - 4ms/step\n",
            "Epoch 102/9999\n",
            "2/2 - 0s - loss: 0.0300 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 103/9999\n",
            "2/2 - 0s - loss: 0.0289 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 104/9999\n",
            "2/2 - 0s - loss: 0.0276 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 105/9999\n",
            "2/2 - 0s - loss: 0.0266 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 106/9999\n",
            "2/2 - 0s - loss: 0.0252 - categorical_accuracy: 1.0000 - 12ms/epoch - 6ms/step\n",
            "Epoch 107/9999\n",
            "2/2 - 0s - loss: 0.0242 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 108/9999\n",
            "2/2 - 0s - loss: 0.0233 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 109/9999\n",
            "2/2 - 0s - loss: 0.0224 - categorical_accuracy: 1.0000 - 10ms/epoch - 5ms/step\n",
            "Epoch 110/9999\n",
            "2/2 - 0s - loss: 0.0217 - categorical_accuracy: 1.0000 - 9ms/epoch - 5ms/step\n",
            "Epoch 111/9999\n",
            "2/2 - 0s - loss: 0.0209 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 112/9999\n",
            "2/2 - 0s - loss: 0.0201 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 113/9999\n",
            "2/2 - 0s - loss: 0.0194 - categorical_accuracy: 1.0000 - 14ms/epoch - 7ms/step\n",
            "Epoch 114/9999\n",
            "2/2 - 0s - loss: 0.0187 - categorical_accuracy: 1.0000 - 11ms/epoch - 5ms/step\n",
            "Epoch 115/9999\n",
            "2/2 - 0s - loss: 0.0181 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 116/9999\n",
            "2/2 - 0s - loss: 0.0175 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 117/9999\n",
            "2/2 - 0s - loss: 0.0170 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "Epoch 118/9999\n",
            "2/2 - 0s - loss: 0.0165 - categorical_accuracy: 1.0000 - 8ms/epoch - 4ms/step\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 4.9917 - categorical_accuracy: 0.2308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.991666793823242, 0.23076923191547394]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model3.predict(TEST_SET_2D)\n",
        "predicted"
      ],
      "metadata": {
        "id": "lSGEDvpk6hBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54385980-1009-4e0f-fa77-8d9f09f0bf14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.83356033e-04, 6.29789656e-06, 7.54535165e-07, 6.50549948e-01,\n",
              "        3.55053060e-02, 1.12760980e-02, 8.30469802e-02, 1.42852896e-05,\n",
              "        8.26780160e-04, 3.34025035e-03, 7.81121112e-10, 1.61918387e-01,\n",
              "        2.98311619e-07, 2.45565388e-07, 5.18582240e-02, 7.20756725e-05,\n",
              "        4.90012189e-06, 6.79516117e-04, 6.82090395e-05, 5.60402391e-10,\n",
              "        3.48909525e-04, 9.90122426e-05, 4.75693085e-08, 3.07783066e-09,\n",
              "        1.50718247e-08, 7.12550746e-14],\n",
              "       [6.92760915e-09, 7.52363801e-01, 1.18341404e-06, 1.21215987e-03,\n",
              "        2.40880981e-05, 2.81484146e-03, 5.39906296e-07, 6.40443832e-05,\n",
              "        1.58851430e-01, 5.96940797e-03, 4.90980634e-09, 5.07970910e-09,\n",
              "        1.89853978e-07, 2.56560861e-06, 3.15735597e-05, 1.19906547e-03,\n",
              "        2.63157181e-05, 5.77459900e-07, 1.18013064e-03, 3.10174306e-03,\n",
              "        3.92921123e-04, 4.67201881e-03, 1.60064250e-02, 5.37300576e-03,\n",
              "        4.67117652e-02, 1.62052373e-07],\n",
              "       [6.01035572e-05, 2.55904979e-06, 4.70994979e-01, 1.78747103e-02,\n",
              "        6.67135604e-03, 7.97590762e-02, 1.40277371e-01, 2.77132116e-04,\n",
              "        1.82420120e-03, 3.87903280e-03, 4.08013676e-08, 1.33452378e-03,\n",
              "        3.96174255e-06, 7.72723695e-07, 5.17597143e-03, 1.19984113e-01,\n",
              "        5.72895892e-02, 7.28414379e-05, 3.84203595e-04, 9.02443730e-07,\n",
              "        5.24786301e-05, 9.06167924e-02, 1.04347804e-04, 1.21676507e-07,\n",
              "        3.35879088e-03, 5.82115689e-10],\n",
              "       [1.06312527e-07, 5.41207986e-03, 1.10266726e-07, 7.41087377e-01,\n",
              "        2.23395036e-05, 2.82574594e-02, 1.68455028e-06, 2.84388790e-07,\n",
              "        1.95234954e-01, 1.74122527e-02, 7.88664923e-13, 1.35784580e-06,\n",
              "        3.63076319e-10, 3.24227933e-09, 7.62018329e-03, 8.66892788e-06,\n",
              "        2.18510813e-05, 8.01899915e-08, 1.22442725e-03, 2.63490006e-06,\n",
              "        1.88655740e-05, 6.70687645e-04, 1.20322807e-06, 1.50393959e-07,\n",
              "        3.00120632e-03, 1.13123202e-13],\n",
              "       [6.78963952e-09, 3.34658742e-01, 9.95146024e-07, 1.82091643e-03,\n",
              "        4.18931432e-03, 5.84951825e-02, 6.99710085e-07, 1.33394115e-05,\n",
              "        1.03417579e-02, 4.71151382e-01, 2.40414462e-08, 1.56918162e-07,\n",
              "        4.16008517e-09, 1.39086035e-06, 1.11747850e-05, 2.18881411e-03,\n",
              "        5.75039703e-05, 2.26425618e-06, 1.21521147e-03, 1.80149818e-05,\n",
              "        8.84961537e-06, 3.44007313e-02, 4.03520884e-03, 9.10922477e-04,\n",
              "        7.64775127e-02, 8.52915338e-09],\n",
              "       [4.60359290e-14, 5.06831682e-04, 1.21851201e-07, 4.00498757e-05,\n",
              "        5.03317317e-08, 3.49725038e-02, 4.58635463e-10, 2.74743076e-11,\n",
              "        8.42541631e-04, 1.00276922e-03, 5.75888595e-15, 1.75489120e-11,\n",
              "        1.12984725e-14, 2.10251616e-13, 9.44404306e-08, 1.02724101e-04,\n",
              "        5.89077763e-06, 3.07545547e-12, 1.50328461e-07, 5.92487126e-09,\n",
              "        2.04775169e-10, 2.25271750e-03, 1.42775468e-06, 1.16144772e-09,\n",
              "        9.60272133e-01, 2.64403187e-16],\n",
              "       [6.53624826e-04, 1.74196088e-04, 7.73829088e-05, 1.71800808e-03,\n",
              "        6.44791245e-01, 5.06509701e-03, 1.56746671e-01, 1.11358566e-03,\n",
              "        3.70952439e-05, 1.05517572e-02, 3.94696690e-04, 1.22822791e-01,\n",
              "        1.09920060e-04, 4.10450128e-04, 5.54654340e-04, 7.22806435e-03,\n",
              "        2.67511077e-05, 4.63741682e-02, 7.03838014e-05, 2.04128536e-08,\n",
              "        5.40116220e-04, 4.88528691e-04, 3.84539126e-05, 1.18010212e-05,\n",
              "        3.70617187e-07, 8.09285794e-08],\n",
              "       [1.89418813e-07, 1.50820743e-02, 2.09831924e-07, 4.00168892e-06,\n",
              "        6.61638379e-02, 8.25989046e-05, 3.42483108e-04, 9.89726046e-04,\n",
              "        1.05480785e-06, 5.84021336e-06, 3.90287377e-02, 4.48015169e-04,\n",
              "        1.66297346e-01, 5.90429641e-02, 6.91234590e-08, 2.26537138e-03,\n",
              "        3.73615539e-09, 6.20489001e-01, 2.41208909e-07, 7.28789828e-08,\n",
              "        4.81828675e-03, 2.73213227e-05, 5.01466602e-05, 2.48557646e-02,\n",
              "        2.54955381e-08, 4.57685428e-06],\n",
              "       [2.20190759e-06, 3.03441405e-09, 4.61105174e-06, 4.78354032e-08,\n",
              "        1.38039742e-12, 5.32977973e-09, 3.08822329e-10, 7.19952737e-08,\n",
              "        8.82642865e-01, 5.58289939e-05, 3.39291651e-20, 9.29128863e-16,\n",
              "        1.63358110e-13, 9.78278292e-11, 2.18668778e-04, 4.93640476e-11,\n",
              "        1.88027062e-02, 9.59468101e-15, 8.41886774e-02, 1.11356564e-02,\n",
              "        9.28232335e-09, 2.50142557e-06, 1.06598301e-04, 3.21970339e-09,\n",
              "        2.83958158e-03, 1.45918322e-09],\n",
              "       [2.53664562e-04, 5.54825291e-02, 6.25209650e-05, 7.12251358e-06,\n",
              "        4.46354039e-04, 2.12912026e-04, 1.31276408e-06, 4.98751615e-05,\n",
              "        1.51236192e-04, 9.26233947e-01, 1.39584779e-04, 5.48413072e-06,\n",
              "        1.98640507e-07, 1.35227840e-03, 2.95046775e-04, 9.67627275e-05,\n",
              "        4.92766158e-05, 4.71171894e-04, 7.88642745e-03, 1.56783187e-06,\n",
              "        7.83394033e-04, 8.53658094e-06, 1.27956009e-04, 3.00263520e-03,\n",
              "        2.39603833e-04, 2.63863127e-03],\n",
              "       [7.46084375e-11, 8.96425009e-01, 1.83358296e-07, 1.43420731e-03,\n",
              "        7.17480609e-04, 9.16147605e-02, 1.05136746e-06, 1.38475301e-07,\n",
              "        1.27930485e-04, 6.77964417e-04, 1.38548285e-06, 4.99647695e-06,\n",
              "        8.23524005e-09, 2.43138828e-07, 5.15717375e-07, 7.28124240e-03,\n",
              "        3.40295934e-08, 4.83159019e-05, 8.77918524e-07, 1.01252873e-08,\n",
              "        3.66186487e-06, 8.75736077e-05, 2.89767991e-06, 7.09399364e-06,\n",
              "        1.56240980e-03, 3.30115761e-11],\n",
              "       [1.05367471e-05, 2.17465870e-03, 1.36715414e-07, 4.84761626e-01,\n",
              "        1.09738074e-01, 6.61740303e-02, 2.58679222e-03, 9.01264411e-06,\n",
              "        2.49703735e-04, 5.33576822e-03, 3.63365075e-07, 3.22047114e-01,\n",
              "        8.34500838e-07, 6.07316451e-06, 2.39873258e-03, 9.97882380e-05,\n",
              "        5.46016906e-07, 4.10400238e-03, 3.74879164e-05, 3.57537533e-09,\n",
              "        1.72443761e-04, 9.07839931e-05, 7.45512381e-08, 3.92143392e-07,\n",
              "        9.14717532e-07, 4.44926301e-12],\n",
              "       [1.45678478e-05, 1.51496213e-02, 1.45864833e-04, 2.08749017e-03,\n",
              "        3.21968824e-01, 1.03726402e-01, 6.00337498e-02, 3.15545534e-04,\n",
              "        1.58084324e-04, 1.47508299e-02, 6.99156336e-03, 1.49637699e-01,\n",
              "        3.88180015e-05, 5.53785416e-04, 1.79035225e-04, 1.21209227e-01,\n",
              "        5.55718543e-06, 1.97677210e-01, 7.15766000e-05, 2.36378437e-08,\n",
              "        4.82265837e-03, 2.92003417e-04, 5.23604176e-05, 9.82138954e-05,\n",
              "        1.93405340e-05, 3.01457916e-08],\n",
              "       [4.25657873e-07, 1.05809129e-03, 2.62310351e-08, 4.04757657e-07,\n",
              "        7.31752813e-02, 4.15293471e-05, 1.26048180e-04, 6.53067327e-05,\n",
              "        1.01978630e-08, 2.78535772e-05, 1.44556304e-02, 7.99500477e-03,\n",
              "        1.62382831e-03, 1.79300308e-02, 3.78106790e-08, 1.70972256e-04,\n",
              "        1.46558071e-10, 8.81952643e-01, 1.80683450e-08, 1.51093478e-11,\n",
              "        1.27803371e-03, 7.05088439e-07, 2.42561470e-07, 9.78915559e-05,\n",
              "        1.63377006e-10, 4.11229522e-08],\n",
              "       [9.55108626e-05, 1.06608297e-06, 1.17216371e-02, 1.58263311e-01,\n",
              "        5.87076138e-05, 5.44120036e-02, 3.58655639e-02, 1.17542993e-06,\n",
              "        3.59095111e-02, 4.43397323e-03, 3.79575624e-12, 3.56158504e-04,\n",
              "        8.21928658e-09, 4.77528506e-09, 6.80810571e-01, 1.83649408e-03,\n",
              "        1.22417463e-02, 1.22341987e-06, 3.40428640e-04, 5.64294602e-08,\n",
              "        2.55887368e-04, 3.18550691e-03, 5.56317161e-07, 3.19139160e-10,\n",
              "        2.08787053e-04, 1.24662718e-13],\n",
              "       [4.31727516e-13, 1.39933657e-02, 3.62794481e-06, 4.50999429e-03,\n",
              "        5.96789448e-07, 1.73477098e-01, 1.37922768e-07, 1.10741807e-08,\n",
              "        5.37667759e-02, 2.02084877e-04, 7.98743463e-14, 1.26987046e-10,\n",
              "        6.91827082e-11, 1.08644855e-11, 6.42808936e-06, 1.78294890e-02,\n",
              "        2.65269609e-05, 2.89680002e-10, 3.88000331e-07, 3.75353557e-06,\n",
              "        4.70943633e-07, 1.29591689e-01, 1.43689234e-04, 6.72334437e-08,\n",
              "        6.06443763e-01, 8.86491977e-15],\n",
              "       [1.85958947e-06, 1.72533898e-08, 1.22081826e-03, 1.69281155e-01,\n",
              "        1.80820562e-03, 7.77914748e-02, 5.15699796e-02, 6.00020121e-06,\n",
              "        1.00809417e-03, 6.88337663e-04, 3.17814920e-13, 8.65881229e-05,\n",
              "        6.51128351e-09, 1.31287536e-10, 7.86621496e-03, 4.62323576e-02,\n",
              "        3.98122482e-02, 3.87571006e-08, 8.99990664e-06, 5.39708678e-09,\n",
              "        5.79845982e-06, 6.02586091e-01, 4.12309146e-06, 4.40386859e-11,\n",
              "        2.15239725e-05, 4.20602469e-15],\n",
              "       [4.66938337e-12, 7.04831332e-02, 6.79586378e-08, 4.35114771e-01,\n",
              "        1.42234174e-04, 4.63038594e-01, 3.57755039e-06, 7.76057263e-08,\n",
              "        1.21248439e-02, 1.17478514e-04, 8.35375432e-12, 3.19032409e-07,\n",
              "        2.15870366e-09, 2.94624408e-10, 1.96856145e-05, 1.02308281e-02,\n",
              "        1.65881602e-07, 4.19056704e-07, 3.84633523e-07, 8.04334164e-08,\n",
              "        1.22231222e-05, 7.76452292e-03, 2.79869960e-06, 8.90551988e-08,\n",
              "        9.43821506e-04, 1.25235685e-15],\n",
              "       [1.03099388e-03, 1.46642458e-02, 4.55989240e-04, 1.40320088e-04,\n",
              "        3.30452854e-03, 4.57305752e-04, 2.12475352e-04, 7.81890228e-02,\n",
              "        3.31299081e-02, 9.50814262e-02, 4.96604298e-06, 2.66223833e-06,\n",
              "        8.66106930e-05, 4.59793396e-03, 3.98678618e-04, 9.54689458e-04,\n",
              "        3.17884772e-03, 3.97527503e-04, 9.24402699e-02, 1.12330019e-02,\n",
              "        2.54426803e-03, 1.45344706e-02, 5.47515213e-01, 7.44632334e-02,\n",
              "        3.29002109e-03, 1.76912881e-02],\n",
              "       [2.79724245e-05, 3.83259979e-10, 5.30902071e-06, 2.92683808e-06,\n",
              "        1.57654900e-09, 2.75441352e-07, 1.24883570e-09, 5.48117853e-08,\n",
              "        8.14977735e-02, 2.42651515e-02, 2.91037993e-19, 2.20240642e-13,\n",
              "        1.60719487e-14, 1.73661849e-11, 4.09579283e-04, 2.47418225e-10,\n",
              "        4.29829210e-01, 1.66369976e-14, 4.57157403e-01, 2.07874720e-04,\n",
              "        6.99568015e-10, 2.77605955e-04, 3.90230198e-05, 4.93282803e-10,\n",
              "        6.27978751e-03, 4.60707202e-11],\n",
              "       [1.26841696e-04, 2.79893429e-05, 3.83016514e-03, 1.35388253e-02,\n",
              "        7.52516231e-03, 2.06957571e-02, 7.05645204e-01, 1.40786709e-04,\n",
              "        1.67515682e-04, 3.14345438e-04, 1.47621804e-05, 1.82780385e-01,\n",
              "        1.79733433e-05, 4.42107512e-06, 2.97036697e-03, 5.14451675e-02,\n",
              "        7.30853790e-05, 9.21558216e-03, 2.28104273e-05, 1.06114291e-08,\n",
              "        1.11443538e-03, 3.19383544e-04, 2.69106545e-06, 1.91671504e-07,\n",
              "        6.14762121e-06, 6.72893075e-10],\n",
              "       [1.56083028e-04, 1.91999752e-05, 1.53814167e-01, 6.70118779e-02,\n",
              "        8.71850457e-03, 3.81490648e-01, 1.92799956e-01, 1.60516764e-04,\n",
              "        5.27870515e-03, 1.31021291e-02, 1.49698337e-07, 2.29491964e-02,\n",
              "        2.03423906e-06, 1.86091040e-06, 2.12575048e-02, 8.08437318e-02,\n",
              "        2.25575436e-02, 4.63879231e-04, 1.07156090e-03, 3.62507251e-07,\n",
              "        2.22489354e-04, 2.10034531e-02, 2.05985671e-05, 2.44810309e-07,\n",
              "        7.05360714e-03, 3.82487098e-10],\n",
              "       [4.55993927e-07, 2.66252368e-06, 1.39760552e-04, 2.95172900e-01,\n",
              "        9.21604130e-03, 3.22124422e-01, 2.39220396e-01, 2.15566342e-06,\n",
              "        1.72513115e-04, 7.07293220e-05, 3.81756582e-09, 4.76874411e-02,\n",
              "        4.80486904e-07, 6.24659835e-09, 2.23538955e-03, 7.71324858e-02,\n",
              "        3.54856966e-05, 9.51615584e-05, 4.22050562e-07, 4.66123029e-10,\n",
              "        1.45457147e-04, 6.54372573e-03, 2.59986308e-07, 9.12657672e-10,\n",
              "        1.64930577e-06, 1.53769108e-14],\n",
              "       [7.08917412e-07, 6.08943738e-02, 4.63949000e-05, 5.31289443e-05,\n",
              "        1.94614108e-06, 1.01904938e-04, 1.02995887e-06, 1.65520178e-03,\n",
              "        1.14584513e-01, 1.47804501e-04, 1.42819474e-07, 2.99146530e-09,\n",
              "        7.29141393e-05, 3.06893664e-04, 2.75735547e-05, 2.38632769e-04,\n",
              "        2.84372916e-04, 9.68587074e-07, 3.01853777e-03, 5.56859910e-01,\n",
              "        6.23100623e-03, 1.20251393e-03, 1.15545511e-01, 9.43240896e-02,\n",
              "        3.74580398e-02, 6.94185542e-03],\n",
              "       [4.65044941e-05, 4.16013077e-02, 3.31408664e-04, 1.12550322e-03,\n",
              "        1.16987118e-04, 1.44080422e-03, 1.49558391e-05, 2.63167452e-03,\n",
              "        3.01691622e-01, 4.37512100e-02, 3.98747460e-07, 2.85884084e-07,\n",
              "        1.70301064e-05, 2.41172762e-04, 1.69533142e-03, 6.79391902e-04,\n",
              "        9.35403351e-03, 6.52769359e-06, 6.80867508e-02, 2.21667349e-01,\n",
              "        3.37534281e-03, 1.77256446e-02, 1.35918826e-01, 2.34749150e-02,\n",
              "        1.24180071e-01, 8.24904419e-04],\n",
              "       [8.33395120e-07, 4.01425920e-03, 1.69085197e-05, 1.54757563e-05,\n",
              "        1.22988531e-05, 2.10225029e-04, 4.45591652e-08, 6.11116266e-05,\n",
              "        5.31385802e-02, 3.92022967e-01, 6.41865561e-10, 4.85069596e-10,\n",
              "        3.35905770e-09, 3.54114036e-06, 3.90564201e-05, 1.79362087e-05,\n",
              "        8.45150743e-03, 1.31327562e-08, 7.12691844e-02, 1.35195814e-02,\n",
              "        4.73167393e-06, 8.64573754e-03, 7.56138712e-02, 2.16374779e-03,\n",
              "        3.70751470e-01, 2.68277308e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_list = []\n",
        "for num in range(len(predicted)):\n",
        "    max_val = 0\n",
        "    actual = chr(num+ord('A'))\n",
        "    max_val = predicted[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    result_list.append([actual, predict])\n",
        "result_list"
      ],
      "metadata": {
        "id": "N0JsYeXi6h2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353dab35-a4d1-4407-a0f8-f1b991e6cf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A', 'D'],\n",
              " ['B', 'B'],\n",
              " ['C', 'C'],\n",
              " ['D', 'D'],\n",
              " ['E', 'J'],\n",
              " ['F', 'Y'],\n",
              " ['G', 'E'],\n",
              " ['H', 'R'],\n",
              " ['I', 'I'],\n",
              " ['J', 'J'],\n",
              " ['K', 'B'],\n",
              " ['L', 'D'],\n",
              " ['M', 'E'],\n",
              " ['N', 'R'],\n",
              " ['O', 'O'],\n",
              " ['P', 'Y'],\n",
              " ['Q', 'V'],\n",
              " ['R', 'F'],\n",
              " ['S', 'W'],\n",
              " ['T', 'S'],\n",
              " ['U', 'G'],\n",
              " ['V', 'F'],\n",
              " ['W', 'F'],\n",
              " ['X', 'T'],\n",
              " ['Y', 'I'],\n",
              " ['Z', 'J']]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_misclassified = []\n",
        "num_total, num_correct = 26, 26\n",
        "for i in range(len(result_list)):\n",
        "    if result_list[i][0] != result_list[i][1]:\n",
        "        num_correct -= 1\n",
        "        list_misclassified.append([result_list[i][0], result_list[i][1]])\n",
        "print(\"Number of images correctly classified: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_misclassified) == 0:\n",
        "    print(\"All test images are classified correctly.\")\n",
        "else:\n",
        "    print(\"Here are the list of test images that are misclassified and how they appear:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_misclassified)):\n",
        "        print(\"{}         {}\".format(list_misclassified[i][0], list_misclassified[i][1]))"
      ],
      "metadata": {
        "id": "E3qg5Hv66nmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b3105a-8175-4a6c-899c-b77fad54fc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images correctly classified: 23.076923076923077%\n",
            "Here are the list of test images that are misclassified and how they appear:\n",
            "Actual    Predicted\n",
            "A         D\n",
            "E         J\n",
            "F         Y\n",
            "G         E\n",
            "H         R\n",
            "K         B\n",
            "L         D\n",
            "M         E\n",
            "N         R\n",
            "P         Y\n",
            "Q         V\n",
            "R         F\n",
            "S         W\n",
            "T         S\n",
            "U         G\n",
            "V         F\n",
            "W         F\n",
            "X         T\n",
            "Y         I\n",
            "Z         J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_message = model3.predict(MESSAGE_2D)\n",
        "predicted_message"
      ],
      "metadata": {
        "id": "bU1DX5QZ6oyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9f61ac-a0d5-4f0f-e325-258b13955b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.90222379e-05, 5.11857778e-10, 3.75518539e-06, 2.21910773e-06,\n",
              "        2.05344786e-09, 2.29102284e-07, 8.51112236e-10, 5.75873287e-08,\n",
              "        6.21967278e-02, 3.36261354e-02, 5.30048082e-19, 2.22339589e-13,\n",
              "        1.46316370e-14, 2.50904818e-11, 3.00672080e-04, 1.96998404e-10,\n",
              "        3.19715708e-01, 2.24049108e-14, 5.78434765e-01, 1.70874875e-04,\n",
              "        6.36518172e-10, 1.93342785e-04, 3.58161560e-05, 7.83851095e-10,\n",
              "        5.29072108e-03, 8.31121630e-11],\n",
              "       [4.93466246e-07, 5.99277802e-02, 8.99254428e-06, 9.02079046e-05,\n",
              "        1.91506699e-01, 1.30478886e-03, 3.94799933e-03, 7.17996480e-03,\n",
              "        4.16016483e-05, 9.81694102e-05, 3.48835811e-02, 5.52817073e-04,\n",
              "        1.34084880e-01, 2.39669215e-02, 1.62101298e-06, 9.09384862e-02,\n",
              "        3.05164576e-07, 3.62033069e-01, 3.80506913e-06, 4.00500403e-06,\n",
              "        1.70419980e-02, 1.56137475e-03, 2.60812859e-03, 6.81977868e-02,\n",
              "        1.84786131e-06, 1.27690419e-05],\n",
              "       [5.64197933e-09, 2.24086922e-03, 6.71060585e-09, 1.67314848e-03,\n",
              "        9.54858124e-01, 1.50298579e-02, 4.92640189e-04, 5.01611203e-06,\n",
              "        1.19230970e-06, 7.66714977e-04, 9.62177728e-06, 7.92364217e-03,\n",
              "        1.79139960e-07, 2.70582700e-06, 7.02526847e-07, 2.08320539e-03,\n",
              "        1.88802485e-09, 1.48395691e-02, 3.62568045e-07, 1.59224474e-11,\n",
              "        3.03391189e-05, 4.02877049e-05, 1.86734454e-07, 1.37469260e-06,\n",
              "        1.47067096e-08, 6.91136547e-13],\n",
              "       [3.14193550e-12, 5.95219790e-05, 1.80401436e-08, 2.34491974e-02,\n",
              "        3.11091635e-02, 9.08505976e-01, 4.70064348e-04, 6.63510402e-09,\n",
              "        9.19363231e-07, 1.27151625e-05, 3.66316955e-09, 3.30566685e-03,\n",
              "        6.85848989e-10, 5.29540196e-11, 1.17522582e-07, 3.21187675e-02,\n",
              "        3.61805541e-09, 2.67317100e-05, 4.26878838e-10, 1.69209725e-13,\n",
              "        3.62584309e-08, 9.40476253e-04, 8.73030181e-09, 1.39640924e-10,\n",
              "        6.34337312e-07, 2.44280269e-18],\n",
              "       [2.76942330e-07, 2.82083960e-07, 9.84236834e-08, 2.25387339e-07,\n",
              "        2.55262356e-11, 8.45069881e-09, 4.65975647e-10, 1.52544862e-05,\n",
              "        8.31843138e-01, 1.59762374e-06, 2.44259797e-18, 1.11058428e-15,\n",
              "        3.00026726e-10, 4.65896788e-09, 4.71404246e-06, 3.18115256e-10,\n",
              "        1.07742773e-04, 7.58001192e-13, 1.51058026e-02, 1.50179312e-01,\n",
              "        3.95411507e-06, 5.18878051e-06, 2.57266336e-03, 2.39862379e-06,\n",
              "        1.57164497e-04, 1.14570177e-07],\n",
              "       [8.70341830e-13, 8.60361382e-09, 4.14669239e-06, 9.55432000e-08,\n",
              "        3.67822537e-08, 4.58367140e-06, 1.27243407e-08, 4.56260594e-08,\n",
              "        2.27250912e-05, 2.78993666e-05, 2.81046061e-17, 2.03626771e-14,\n",
              "        9.30003036e-12, 5.50113611e-13, 4.39344518e-07, 1.24547270e-03,\n",
              "        1.05709350e-03, 7.03845890e-14, 1.30056090e-08, 1.16639023e-06,\n",
              "        1.14318837e-07, 9.96566415e-01, 1.05675333e-03, 1.01603126e-09,\n",
              "        1.28678948e-05, 1.45291765e-13],\n",
              "       [8.93540797e-09, 7.25466132e-01, 6.71021212e-07, 1.81852165e-03,\n",
              "        5.67103457e-03, 2.85736099e-02, 1.22183110e-06, 5.56852428e-05,\n",
              "        1.02737611e-02, 1.71411440e-01, 1.06751415e-07, 1.97889264e-07,\n",
              "        2.86685555e-08, 6.15396539e-06, 9.38851008e-06, 2.92843510e-03,\n",
              "        1.96974033e-05, 9.70945166e-06, 1.21482275e-03, 4.68746912e-05,\n",
              "        4.73003493e-05, 1.99495759e-02, 7.31918169e-03, 3.78526957e-03,\n",
              "        2.13909671e-02, 4.74044128e-08],\n",
              "       [1.24326287e-12, 9.90994871e-01, 1.03579867e-10, 3.17990470e-08,\n",
              "        4.38755924e-05, 1.99334113e-06, 3.61211185e-08, 7.82222855e-07,\n",
              "        2.29860191e-07, 2.45647982e-07, 6.54501782e-05, 1.32471198e-08,\n",
              "        1.03993225e-04, 2.11707375e-04, 1.52552970e-10, 2.24620220e-04,\n",
              "        6.01264589e-13, 2.59280135e-03, 1.29716282e-09, 3.90632054e-10,\n",
              "        1.33255919e-04, 3.39868009e-08, 6.65694245e-07, 5.62538998e-03,\n",
              "        3.39959905e-09, 5.88991123e-09],\n",
              "       [5.37242100e-04, 4.37748753e-08, 3.35493032e-03, 1.33361864e-05,\n",
              "        1.63738338e-08, 9.28750410e-07, 4.78564689e-05, 3.52673339e-07,\n",
              "        7.20757060e-03, 1.90138549e-03, 9.45861363e-15, 2.34175457e-09,\n",
              "        1.02726945e-10, 4.91479746e-09, 9.80886936e-01, 2.94413007e-06,\n",
              "        4.69392631e-03, 3.31753580e-09, 1.14583550e-03, 4.18577514e-07,\n",
              "        1.96235458e-04, 8.98045528e-06, 3.09970943e-07, 1.21549507e-10,\n",
              "        7.17237356e-07, 4.43574344e-11],\n",
              "       [4.00550152e-06, 6.90978020e-02, 2.90121316e-05, 4.48418832e-05,\n",
              "        7.89211936e-06, 5.56124978e-05, 3.11777148e-06, 9.51294135e-03,\n",
              "        5.32586798e-02, 5.85484631e-05, 6.58392992e-07, 1.07818430e-08,\n",
              "        1.06342568e-03, 3.12160677e-03, 1.64280846e-05, 2.03502001e-04,\n",
              "        9.51669499e-05, 9.96516246e-06, 2.58603645e-03, 2.83840358e-01,\n",
              "        1.99324507e-02, 6.52945135e-04, 1.35688424e-01, 3.36271375e-01,\n",
              "        3.86390043e-03, 8.05813074e-02],\n",
              "       [8.64546108e-12, 1.61238029e-06, 7.63998820e-09, 1.32269852e-05,\n",
              "        4.19621803e-12, 3.76650092e-07, 4.90751675e-11, 1.74674053e-08,\n",
              "        9.92788732e-01, 1.18444268e-06, 4.82643449e-21, 1.43541300e-16,\n",
              "        3.93934234e-13, 6.33710640e-13, 6.89258695e-06, 7.69416886e-09,\n",
              "        1.91496365e-05, 2.94513144e-15, 1.22679936e-04, 5.38566895e-03,\n",
              "        1.76721173e-07, 5.88201074e-05, 7.24990896e-05, 9.54014467e-09,\n",
              "        1.52906706e-03, 7.33626484e-14],\n",
              "       [1.12272405e-08, 1.26751070e-06, 4.92262030e-11, 1.43859764e-13,\n",
              "        1.11517593e-07, 3.32983328e-11, 1.73890911e-08, 1.00626959e-03,\n",
              "        4.87030472e-10, 3.70032824e-11, 2.62616275e-08, 5.44817837e-11,\n",
              "        8.67985040e-02, 9.03701544e-01, 9.90122155e-13, 3.95548616e-09,\n",
              "        4.88757797e-13, 1.98954775e-04, 3.62478852e-10, 1.06405176e-10,\n",
              "        1.11309031e-03, 1.15891168e-11, 2.99255078e-07, 5.44884568e-03,\n",
              "        4.30767143e-14, 1.73085602e-03],\n",
              "       [1.20719662e-02, 6.03802960e-07, 3.54320407e-02, 5.89863397e-04,\n",
              "        2.72924546e-04, 2.46980286e-04, 9.19721723e-01, 5.06935699e-04,\n",
              "        9.70975088e-04, 3.41833278e-04, 3.37984325e-08, 4.55806585e-04,\n",
              "        3.89874049e-06, 2.71848307e-06, 2.52376590e-02, 1.61703059e-03,\n",
              "        6.80671015e-04, 4.78054921e-04, 3.65224550e-04, 1.64180598e-08,\n",
              "        9.74817667e-04, 2.55899231e-05, 2.21513119e-06, 1.12472547e-08,\n",
              "        5.76947855e-07, 6.63321342e-10],\n",
              "       [1.19460450e-08, 4.72149877e-05, 4.27541318e-06, 9.52858343e-08,\n",
              "        3.94458948e-06, 4.22840230e-07, 5.49816377e-06, 1.67734530e-02,\n",
              "        4.50009858e-04, 3.10030987e-06, 1.17526318e-08, 3.83037768e-11,\n",
              "        3.67299945e-05, 1.03492484e-05, 4.74733568e-08, 1.02085865e-03,\n",
              "        7.12936207e-06, 2.85639373e-07, 9.79269862e-06, 4.35808161e-03,\n",
              "        3.86715634e-04, 2.87308823e-03, 9.70062733e-01, 3.88044305e-03,\n",
              "        1.02694930e-05, 5.54182880e-05],\n",
              "       [4.30581804e-06, 5.27109911e-09, 4.55654316e-07, 1.33856645e-07,\n",
              "        1.38646343e-11, 4.47056392e-09, 1.21895993e-09, 1.50500432e-06,\n",
              "        9.25569355e-01, 8.30838417e-06, 1.03277357e-19, 1.74219505e-15,\n",
              "        7.42172730e-12, 5.60519187e-10, 4.26531369e-05, 8.57720145e-11,\n",
              "        1.77959958e-03, 1.11092369e-13, 5.84521629e-02, 1.36006419e-02,\n",
              "        2.09110311e-07, 3.46558090e-06, 4.21082979e-04, 2.60447699e-08,\n",
              "        1.16017902e-04, 3.75613629e-09],\n",
              "       [8.30428348e-07, 4.32912447e-03, 1.35696291e-05, 1.06345477e-04,\n",
              "        1.08684602e-04, 1.93560484e-03, 3.65290731e-07, 2.12400082e-05,\n",
              "        2.16619521e-02, 7.87711680e-01, 2.44142218e-09, 1.91494696e-08,\n",
              "        3.78902332e-09, 1.56427927e-06, 9.59853860e-05, 8.78091523e-05,\n",
              "        4.29274421e-03, 7.96384754e-08, 1.57343838e-02, 4.69221181e-04,\n",
              "        5.25078167e-06, 1.44134555e-02, 1.19386679e-02, 4.71437612e-04,\n",
              "        1.36599287e-01, 7.73280760e-07],\n",
              "       [9.71536636e-01, 7.35895966e-09, 1.52723274e-07, 9.12419127e-05,\n",
              "        3.40227329e-04, 2.82150069e-07, 1.04054073e-02, 1.46760460e-04,\n",
              "        8.24868985e-05, 6.45602825e-07, 6.86783269e-11, 3.80775338e-04,\n",
              "        6.88061118e-05, 7.87876488e-05, 8.57293780e-04, 4.79127005e-09,\n",
              "        2.93423099e-08, 4.65566339e-03, 6.34016687e-05, 4.71724715e-10,\n",
              "        1.12913391e-02, 1.56026545e-07, 2.66577227e-09, 2.48526111e-09,\n",
              "        2.39888144e-12, 3.65201029e-11],\n",
              "       [5.42237262e-12, 1.36553459e-02, 1.74937131e-08, 7.55328715e-01,\n",
              "        8.78743813e-05, 2.07491174e-01, 2.44447733e-06, 6.19561646e-08,\n",
              "        1.29942279e-02, 7.78413960e-05, 5.40762097e-13, 1.85639877e-07,\n",
              "        9.75489356e-10, 7.97118899e-11, 2.85135102e-05, 2.16883700e-03,\n",
              "        1.55484102e-07, 1.07382355e-07, 4.16483715e-07, 7.54053531e-08,\n",
              "        6.89052467e-06, 7.85906054e-03, 1.36044616e-06, 2.67806648e-08,\n",
              "        2.96634185e-04, 1.83391262e-16],\n",
              "       [1.85170549e-07, 2.91688107e-02, 3.72730113e-07, 5.93660116e-01,\n",
              "        5.55291081e-05, 6.36722147e-02, 2.54685892e-06, 5.63113815e-07,\n",
              "        2.40026161e-01, 5.25272340e-02, 1.15414093e-11, 3.08244034e-06,\n",
              "        8.93345009e-10, 1.67282934e-08, 8.17063823e-03, 3.15748148e-05,\n",
              "        3.43081156e-05, 3.64945095e-07, 2.24788161e-03, 4.22384937e-06,\n",
              "        4.05711398e-05, 8.50324053e-04, 3.35738764e-06, 7.18842671e-07,\n",
              "        9.49913636e-03, 1.23994174e-12],\n",
              "       [1.62696495e-04, 4.85403632e-07, 1.11170198e-06, 1.85461502e-07,\n",
              "        1.00558739e-09, 4.19423500e-08, 4.45142007e-10, 6.80327219e-07,\n",
              "        6.22076504e-02, 7.34744314e-03, 2.57700324e-16, 5.42408620e-13,\n",
              "        1.66386089e-12, 2.44736231e-08, 2.65533075e-04, 1.79407170e-10,\n",
              "        9.49451525e-04, 2.45578471e-11, 9.28630769e-01, 1.34499918e-04,\n",
              "        7.69330427e-07, 5.78638037e-07, 2.10207963e-05, 3.44581736e-07,\n",
              "        2.76496488e-04, 1.23917815e-07],\n",
              "       [3.51999915e-06, 6.42039549e-06, 4.67398253e-09, 2.37298305e-08,\n",
              "        1.57334881e-07, 8.35054266e-08, 2.87010086e-11, 3.36556072e-09,\n",
              "        8.39473978e-06, 9.97577727e-01, 6.18931555e-13, 3.98635222e-11,\n",
              "        5.20627039e-14, 1.98757846e-08, 3.73845178e-05, 7.34336369e-10,\n",
              "        2.55331634e-06, 1.02938280e-09, 2.36295909e-03, 1.50061696e-09,\n",
              "        7.16757285e-08, 2.67402314e-08, 6.20189340e-08, 1.09807694e-07,\n",
              "        4.46097062e-07, 4.64004257e-09],\n",
              "       [1.20098796e-03, 7.35176855e-07, 2.79640972e-06, 1.48758099e-05,\n",
              "        1.41478095e-05, 2.59897348e-07, 3.58343823e-03, 6.27551740e-03,\n",
              "        1.26454153e-03, 6.70255588e-07, 2.59623816e-11, 1.03559408e-07,\n",
              "        3.53414565e-04, 2.85378537e-05, 1.66613015e-03, 3.63257413e-06,\n",
              "        4.75478231e-07, 1.03429884e-04, 2.82611672e-05, 1.23341908e-06,\n",
              "        9.85441387e-01, 5.30767556e-06, 9.48871184e-06, 6.45190482e-07,\n",
              "        3.94980021e-11, 8.97408459e-09],\n",
              "       [9.10258522e-13, 1.14814214e-09, 2.95958479e-15, 1.05808502e-17,\n",
              "        1.54579422e-11, 5.77068989e-17, 8.09721561e-12, 1.62868784e-03,\n",
              "        3.79367761e-12, 2.61657858e-17, 1.09660375e-13, 7.26592472e-17,\n",
              "        9.74170983e-01, 2.15142984e-02, 1.99624573e-17, 5.69251569e-13,\n",
              "        5.50607150e-18, 1.66448683e-06, 8.26504053e-14, 1.98774122e-11,\n",
              "        7.27629449e-05, 2.28726153e-14, 2.18240199e-08, 2.48194183e-03,\n",
              "        8.60862890e-20, 1.29553140e-04],\n",
              "       [3.48291478e-14, 9.62311635e-04, 1.25218918e-07, 7.45489524e-05,\n",
              "        1.44560554e-03, 6.20744787e-02, 6.10440838e-05, 2.44195881e-08,\n",
              "        3.26881974e-07, 1.46639513e-06, 1.50084105e-07, 5.64162110e-06,\n",
              "        9.37295042e-09, 2.50486298e-10, 2.58588106e-09, 9.34667408e-01,\n",
              "        1.00550512e-09, 1.93824908e-05, 2.58540498e-11, 3.04992555e-12,\n",
              "        9.66534017e-08, 6.85003877e-04, 7.18387298e-07, 1.29134836e-08,\n",
              "        1.64996584e-06, 1.76356018e-15],\n",
              "       [1.66156880e-07, 5.56825697e-11, 4.96149121e-04, 2.09430997e-08,\n",
              "        9.76178600e-12, 2.34326212e-08, 1.22241306e-09, 1.16165069e-08,\n",
              "        4.40165959e-03, 1.18561741e-03, 3.48650287e-21, 8.38354354e-16,\n",
              "        7.19203224e-15, 4.17398483e-13, 9.92637477e-04, 2.50168419e-08,\n",
              "        9.90475476e-01, 4.04399231e-16, 5.53863472e-04, 2.99537387e-05,\n",
              "        1.49751755e-09, 1.02854450e-03, 2.41150319e-05, 5.59031467e-12,\n",
              "        8.11811071e-04, 2.36633434e-12],\n",
              "       [4.97466535e-04, 9.95581853e-04, 3.55271460e-03, 8.36953132e-06,\n",
              "        1.03257553e-04, 6.19434577e-05, 4.41971095e-03, 1.38293328e-02,\n",
              "        2.99987523e-03, 4.81057854e-04, 3.99309101e-06, 2.12491750e-06,\n",
              "        3.99785436e-04, 2.44086608e-03, 1.58375374e-03, 6.71279198e-03,\n",
              "        1.19980927e-04, 9.39710822e-04, 5.17132401e-04, 6.73827744e-05,\n",
              "        9.56068039e-01, 1.22303012e-04, 3.12241842e-03, 6.49815600e-04,\n",
              "        3.87025102e-06, 2.96705111e-04],\n",
              "       [2.72215479e-11, 2.74534409e-06, 1.17363212e-08, 4.44352336e-05,\n",
              "        1.51040708e-11, 1.65139738e-06, 1.26161998e-10, 1.19256214e-08,\n",
              "        9.95169699e-01, 4.65101357e-06, 1.67368235e-20, 1.57071238e-15,\n",
              "        3.15143365e-13, 8.98030565e-13, 1.92130701e-05, 1.22433379e-08,\n",
              "        2.92187269e-05, 9.23660037e-15, 1.87633748e-04, 1.61273847e-03,\n",
              "        1.92354918e-07, 7.39753232e-05, 3.16768783e-05, 6.16460927e-09,\n",
              "        2.82201031e-03, 3.34438321e-14],\n",
              "       [3.22780329e-06, 1.28931610e-09, 9.96673822e-01, 1.86596280e-05,\n",
              "        8.94754990e-08, 7.67329220e-06, 6.56205055e-04, 1.73797605e-06,\n",
              "        4.44728212e-05, 6.74423688e-07, 1.79666785e-12, 6.05840853e-08,\n",
              "        1.20251586e-08, 3.77027565e-09, 1.00074313e-03, 1.81214346e-04,\n",
              "        1.36427348e-03, 8.27901445e-08, 1.26362165e-05, 5.57573436e-08,\n",
              "        5.14829935e-06, 2.08223373e-05, 4.92657826e-08, 3.04436754e-11,\n",
              "        8.42663485e-06, 1.81958858e-11],\n",
              "       [9.38047451e-14, 4.10677940e-02, 1.03006444e-10, 9.64585911e-11,\n",
              "        9.55460127e-04, 2.63235563e-07, 1.35456384e-08, 2.89331243e-08,\n",
              "        1.80017980e-11, 4.05518001e-08, 9.01505053e-01, 2.03473974e-07,\n",
              "        3.78085897e-05, 9.79023403e-04, 1.71907248e-13, 3.33315809e-04,\n",
              "        8.59645268e-15, 5.44780008e-02, 1.86223294e-11, 9.47092204e-14,\n",
              "        1.33518586e-06, 7.12288117e-10, 6.99560943e-09, 6.41688879e-04,\n",
              "        4.82462958e-11, 4.98711694e-09],\n",
              "       [7.16756321e-10, 1.30546844e-08, 1.12275389e-11, 1.23162390e-04,\n",
              "        3.37343663e-03, 4.69762999e-05, 1.29117660e-04, 1.69425321e-10,\n",
              "        5.68512390e-11, 2.04176627e-06, 1.15856313e-09, 9.95739222e-01,\n",
              "        3.08175881e-11, 3.89673398e-11, 2.20452492e-07, 3.61452976e-07,\n",
              "        3.94981384e-13, 5.85410919e-04, 2.08368375e-11, 4.15614443e-19,\n",
              "        7.14625781e-09, 2.92655988e-09, 9.14959399e-15, 4.13769361e-14,\n",
              "        2.68710293e-14, 2.27688168e-21],\n",
              "       [9.92791664e-14, 1.05543565e-02, 2.59029576e-07, 5.65434366e-09,\n",
              "        7.60175478e-10, 7.41043050e-06, 4.14957001e-12, 3.43897995e-08,\n",
              "        3.21901956e-04, 5.37578817e-05, 3.84295720e-11, 5.99809526e-15,\n",
              "        1.76407777e-11, 3.08884829e-09, 5.42558443e-10, 1.88096656e-05,\n",
              "        2.04278558e-06, 7.22585446e-12, 1.61585001e-06, 6.78357959e-04,\n",
              "        4.15102726e-08, 1.50991153e-04, 6.44795364e-03, 1.65406789e-04,\n",
              "        9.81596768e-01, 3.38727034e-07]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_list = \"\"\n",
        "for num in range(len(predicted_message)):\n",
        "    max_val = 0\n",
        "    max_val = predicted_message[num].argmax() # Return the index that contains the largest value.\n",
        "    predict = chr(int(max_val)+ord('A'))\n",
        "    message_list += predict\n",
        "print(message_list)"
      ],
      "metadata": {
        "id": "nG-Us71Z6pha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687caeca-8ab0-41bf-d9e8-92de66fd5d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SREFIVBBOXINGWIJADDSJUMPQUICKLY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_message = \"THEFIVEBOXINGWIZARDSJUMPQUICKLY\"\n",
        "list_mismatch = []\n",
        "num_total, num_correct = len(actual_message), len(message_list)\n",
        "for i in range(len(actual_message)):\n",
        "    if message_list[i] != actual_message[i]:\n",
        "        num_correct -= 1\n",
        "        list_mismatch.append([actual_message[i], message_list[i]])\n",
        "print(\"Accuracy between predicted and actual message: {}%\".format((num_correct/num_total)*100))\n",
        "if len(list_mismatch) == 0:\n",
        "    print(\"The message appears to be decoded correctly.\")\n",
        "else:\n",
        "    print(\"Here is the list of letters that are a mismatch between predicted and actual message:\")\n",
        "    print(\"Actual    Predicted\")\n",
        "    for i in range(len(list_mismatch)):\n",
        "        print(\"{}         {}\".format(list_mismatch[i][0], list_mismatch[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKPnYAN1Smjt",
        "outputId": "54d5f13b-3a18-4058-e33c-8bfcf1bb7658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy between predicted and actual message: 83.87096774193549%\n",
            "Here is the list of letters that are a mismatch between predicted and actual message:\n",
            "Actual    Predicted\n",
            "T         S\n",
            "H         R\n",
            "E         B\n",
            "Z         J\n",
            "R         D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A: As more hidden layers are added, the categorical accuracy obtained will gradually worsen due to the vanishing gradient."
      ],
      "metadata": {
        "id": "L8Fgx3s06qmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Load the EMNIST Letters dataset, and use plt.imshow() to verify that the image data has been loaded correctly and that the corresponding labels are correct."
      ],
      "metadata": {
        "id": "HsXHxW4a0kl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hRZJslkA89H7"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt # Needed to do matplotlib operations\n",
        "emnist_data = np.load('emnist_letters.npz')\n",
        "\n",
        "train_img = emnist_data['train_images']\n",
        "train_label = emnist_data['train_labels']\n",
        "\n",
        "test_img = emnist_data['test_images']\n",
        "test_label = emnist_data['test_labels']\n",
        "\n",
        "validate_img = emnist_data['validate_images']\n",
        "validate_label = emnist_data['validate_labels']\n",
        "\n",
        "#test\n",
        "train_img = train_img.reshape((104000, 28*28))\n",
        "train_img = train_img.astype(\"float32\") / 255\n",
        "test_img = test_img.reshape((20800, 28*28))\n",
        "test_img = test_img.astype(\"float32\") / 255\n",
        "validate_img = validate_img.reshape((20800, 28*28))\n",
        "validate_img = validate_img.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(emnist_data)\n",
        "print(\"Train_img shape: \", train_img.shape)\n",
        "print(\"Train_label shape: \", train_label.shape)\n",
        "print(\"Test_img shape: \", test_img.shape)\n",
        "print(\"Test_label shape: \", test_label.shape)\n",
        "print(\"Validate_img shape: \", validate_img.shape)\n",
        "print(\"Validate_label shape: \", validate_label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toUOFe1X8_yS",
        "outputId": "31a58c64-c6f0-49f6-adb3-148d0076d280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<numpy.lib.npyio.NpzFile object at 0x7fc644c0e050>\n",
            "Train_img shape:  (104000, 784)\n",
            "Train_label shape:  (104000, 27)\n",
            "Test_img shape:  (20800, 784)\n",
            "Test_label shape:  (20800, 27)\n",
            "Validate_img shape:  (20800, 784)\n",
            "Validate_label shape:  (20800, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the first 3 letters.\n",
        "plt.imshow(train_img[0].reshape(28, 28))\n",
        "print(\"Label of the image: \",train_label[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "lBrBavugSwy9",
        "outputId": "21f79898-cd73-4da2-a797-c125fc23ec48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label of the image:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_img[1].reshape(28, 28))\n",
        "print(\"Label of the image: \",train_label[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "KKtfxZWXSxuK",
        "outputId": "ed5adfe6-acfb-4383-d30f-720f9057bd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label of the image:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_img[2].reshape(28, 28))\n",
        "print(\"Label of the image: \",train_label[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "etH3aPdVSyli",
        "outputId": "4bab8ef1-f772-4484-e097-d79b1520566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label of the image:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.\tApply the network architecture from Cholletâ€™s MNIST notebook to the EMNIST Letters data. (You will need to modify the numbers of inputs and outputs, but should leave the dense layer intact.) What accuracy do you achieve? How does this compare with the accuracy for MNIST?"
      ],
      "metadata": {
        "id": "BN-IAng3ZLBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(27, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "Rmq9wPE2ZFi3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "sxYW7FbAZUr_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_img, train_label, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBI8lFbQZWzh",
        "outputId": "2833fe0e-77eb-496e-a825-a3966244baaf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "813/813 [==============================] - 11s 12ms/step - loss: 2.3125 - accuracy: 0.4260\n",
            "Epoch 2/5\n",
            "813/813 [==============================] - 9s 11ms/step - loss: 1.3952 - accuracy: 0.6000\n",
            "Epoch 3/5\n",
            "813/813 [==============================] - 9s 11ms/step - loss: 1.2003 - accuracy: 0.6497\n",
            "Epoch 4/5\n",
            "813/813 [==============================] - 9s 11ms/step - loss: 1.1162 - accuracy: 0.6739\n",
            "Epoch 5/5\n",
            "813/813 [==============================] - 9s 11ms/step - loss: 1.0599 - accuracy: 0.6905\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f335f5c10>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A: The accuracy obtained ended being 69.1%, and this compares to the accuracy for MNIST since said accuracy for EMNIST ended up being lower (say, less than 98.89% for MNIST)."
      ],
      "metadata": {
        "id": "oahb36Taqp13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST accuracy from Chollet's notebook\n",
        "\n",
        "Epoch 1/5\n",
        "469/469 [==============================] - 5s 10ms/step - loss: 0.2580 - accuracy: 0.9250\n",
        "\n",
        "Epoch 2/5\n",
        "469/469 [==============================] - 4s 9ms/step - loss: 0.1030 - accuracy: 0.9697\n",
        "\n",
        "Epoch 3/5\n",
        "469/469 [==============================] - 4s 10ms/step - loss: 0.0673 - accuracy: 0.9805\n",
        "\n",
        "Epoch 4/5\n",
        "469/469 [==============================] - 5s 10ms/step - loss: 0.0487 - accuracy: 0.9857\n",
        "\n",
        "Epoch 5/5\n",
        "469/469 [==============================] - 5s 10ms/step - loss: 0.0373 - accuracy: 0.9889\n",
        "\n",
        "<keras.callbacks.History at 0x7f0e79b5ec90>"
      ],
      "metadata": {
        "id": "pm7htN1Tc4r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.\tKeeping the same number of layers in the network (i.e. an MLP with a single hidden layer), modify the architecture to improve the accuracy. You will need to decide on an appropriate number of neurons in the hidden layer. Keep in mind that:\n",
        "\n",
        ">a.\tThere are 27 classes rather than 10, so you will need a larger hidden layer than the MNIST network.\n",
        "\n",
        ">b.\tIn addition to having more classes, EMNIST Letters mixes upper- and lowercase letters within each class, so even with enough neurons in the hidden layer, your accuracy is likely to be lower.  See the details in the EMNIST paper for the kind of performance you might reasonably expect.\n",
        "\n",
        ">c.\tThe Keras fit() method can take a validation_data parameter in order to evaluate metrics on the validation set.\n"
      ],
      "metadata": {
        "id": "1bIgW6jorVct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = keras.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1024, activation=\"relu\"),\n",
        "    layers.Dense(27, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "6Rm2rLsJroSP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "zbMhL5Vkrvd9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(train_img, train_label, validation_data=(validate_img, validate_label), epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc5bB4TftF05",
        "outputId": "30a38797-354c-4378-d9cc-ffde68c23938"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "813/813 [==============================] - 16s 20ms/step - loss: 1.1262 - accuracy: 0.6711 - val_loss: 1.0841 - val_accuracy: 0.6863\n",
            "Epoch 2/5\n",
            "813/813 [==============================] - 16s 20ms/step - loss: 1.0449 - accuracy: 0.6955 - val_loss: 1.0110 - val_accuracy: 0.7076\n",
            "Epoch 3/5\n",
            "813/813 [==============================] - 16s 20ms/step - loss: 0.9739 - accuracy: 0.7178 - val_loss: 0.9428 - val_accuracy: 0.7266\n",
            "Epoch 4/5\n",
            "813/813 [==============================] - 16s 20ms/step - loss: 0.9025 - accuracy: 0.7386 - val_loss: 0.8725 - val_accuracy: 0.7475\n",
            "Epoch 5/5\n",
            "813/813 [==============================] - 16s 20ms/step - loss: 0.8308 - accuracy: 0.7610 - val_loss: 0.8061 - val_accuracy: 0.7691\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f33187050>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}